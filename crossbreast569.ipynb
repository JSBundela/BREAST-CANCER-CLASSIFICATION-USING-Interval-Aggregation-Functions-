{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HDJ-qUqdi5sY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV dataset\n",
        "data = pd.read_csv('wdbc.csv')\n",
        "\n",
        "X = data.iloc[:, 2:].values  # Feature columns\n",
        "y = data.iloc[:, 1].values.reshape(-1, 1)  # Label column\n",
        "\n",
        "# Normalize the feature columns\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the training data into training and validation sets\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the ANN model parameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 5\n",
        "output_size = y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(y_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(569, 32)\n"
          ]
        }
      ],
      "source": [
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "FIbm7oi-uVdk",
        "outputId": "54b92bfc-3e17-4033-ad57-50a50a3168a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>...</th>\n",
              "      <th>Unnamed: 22</th>\n",
              "      <th>Unnamed: 23</th>\n",
              "      <th>Unnamed: 24</th>\n",
              "      <th>Unnamed: 25</th>\n",
              "      <th>Unnamed: 26</th>\n",
              "      <th>Unnamed: 27</th>\n",
              "      <th>Unnamed: 28</th>\n",
              "      <th>Unnamed: 29</th>\n",
              "      <th>Unnamed: 30</th>\n",
              "      <th>Unnamed: 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 Unnamed: 1  Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5  \\\n",
              "0      842302          M       17.99       10.38      122.80      1001.0   \n",
              "1      842517          M       20.57       17.77      132.90      1326.0   \n",
              "2    84300903          M       19.69       21.25      130.00      1203.0   \n",
              "3    84348301          M       11.42       20.38       77.58       386.1   \n",
              "4    84358402          M       20.29       14.34      135.10      1297.0   \n",
              "\n",
              "   Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  ...  Unnamed: 22  \\\n",
              "0     0.11840     0.27760      0.3001     0.14710  ...        25.38   \n",
              "1     0.08474     0.07864      0.0869     0.07017  ...        24.99   \n",
              "2     0.10960     0.15990      0.1974     0.12790  ...        23.57   \n",
              "3     0.14250     0.28390      0.2414     0.10520  ...        14.91   \n",
              "4     0.10030     0.13280      0.1980     0.10430  ...        22.54   \n",
              "\n",
              "   Unnamed: 23  Unnamed: 24  Unnamed: 25  Unnamed: 26  Unnamed: 27  \\\n",
              "0        17.33       184.60       2019.0       0.1622       0.6656   \n",
              "1        23.41       158.80       1956.0       0.1238       0.1866   \n",
              "2        25.53       152.50       1709.0       0.1444       0.4245   \n",
              "3        26.50        98.87        567.7       0.2098       0.8663   \n",
              "4        16.67       152.20       1575.0       0.1374       0.2050   \n",
              "\n",
              "   Unnamed: 28  Unnamed: 29  Unnamed: 30  Unnamed: 31  \n",
              "0       0.7119       0.2654       0.4601      0.11890  \n",
              "1       0.2416       0.1860       0.2750      0.08902  \n",
              "2       0.4504       0.2430       0.3613      0.08758  \n",
              "3       0.6869       0.2575       0.6638      0.17300  \n",
              "4       0.4000       0.1625       0.2364      0.07678  \n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln88Nj5956sK",
        "outputId": "86df8300-e743-43dc-a95b-53c7620aedab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(455, 30)\n",
            "(455, 2)\n",
            "(114, 30)\n",
            "(114, 2)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "#print(X_val.shape)\n",
        "#print(y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lbl6Z5pSjDxd"
      },
      "outputs": [],
      "source": [
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = softmax(z2)\n",
        "    return a1, a2\n",
        "\n",
        "# Backward propagation\n",
        "def backward_propagation(X, y, a1, a2, W1, W2):\n",
        "    m = X.shape[0]\n",
        "    dz2 = a2 - y\n",
        "    dW2 = np.dot(a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "    dz1 = np.dot(dz2, W2.T) * sigmoid_derivative(a1)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "    return dW1, db1, dW2, db2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em-WfLr3AR5r",
        "outputId": "716ca056-493d-4a4e-cd0f-4c4d77d7da2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000 - Train Loss: 0.4296, Train Accuracy: 0.3714, Val Loss: 0.4203, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.3927, Train Accuracy: 0.3714, Val Loss: 0.3868, Val Accuracy: 0.3956\n",
            "Epoch 201/1000 - Train Loss: 0.3572, Train Accuracy: 0.3714, Val Loss: 0.3547, Val Accuracy: 0.3956\n",
            "Epoch 301/1000 - Train Loss: 0.3246, Train Accuracy: 0.3824, Val Loss: 0.3248, Val Accuracy: 0.4176\n",
            "Epoch 401/1000 - Train Loss: 0.2954, Train Accuracy: 0.4088, Val Loss: 0.2979, Val Accuracy: 0.4396\n",
            "Epoch 501/1000 - Train Loss: 0.2699, Train Accuracy: 0.4462, Val Loss: 0.2742, Val Accuracy: 0.4396\n",
            "Epoch 601/1000 - Train Loss: 0.2479, Train Accuracy: 0.5253, Val Loss: 0.2536, Val Accuracy: 0.4945\n",
            "Epoch 701/1000 - Train Loss: 0.2291, Train Accuracy: 0.6000, Val Loss: 0.2360, Val Accuracy: 0.5604\n",
            "Epoch 801/1000 - Train Loss: 0.2132, Train Accuracy: 0.6659, Val Loss: 0.2209, Val Accuracy: 0.6044\n",
            "Epoch 901/1000 - Train Loss: 0.1998, Train Accuracy: 0.7231, Val Loss: 0.2081, Val Accuracy: 0.6813\n",
            "Epoch 1/1000 - Train Loss: 0.2539, Train Accuracy: 0.6308, Val Loss: 0.2742, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.2396, Train Accuracy: 0.6330, Val Loss: 0.2588, Val Accuracy: 0.6044\n",
            "Epoch 201/1000 - Train Loss: 0.2257, Train Accuracy: 0.6352, Val Loss: 0.2435, Val Accuracy: 0.6044\n",
            "Epoch 301/1000 - Train Loss: 0.2128, Train Accuracy: 0.6396, Val Loss: 0.2290, Val Accuracy: 0.6044\n",
            "Epoch 401/1000 - Train Loss: 0.2011, Train Accuracy: 0.6527, Val Loss: 0.2155, Val Accuracy: 0.6154\n",
            "Epoch 501/1000 - Train Loss: 0.1907, Train Accuracy: 0.6637, Val Loss: 0.2034, Val Accuracy: 0.6154\n",
            "Epoch 601/1000 - Train Loss: 0.1816, Train Accuracy: 0.6725, Val Loss: 0.1927, Val Accuracy: 0.6154\n",
            "Epoch 701/1000 - Train Loss: 0.1735, Train Accuracy: 0.6901, Val Loss: 0.1832, Val Accuracy: 0.6374\n",
            "Epoch 801/1000 - Train Loss: 0.1665, Train Accuracy: 0.7011, Val Loss: 0.1749, Val Accuracy: 0.6374\n",
            "Epoch 901/1000 - Train Loss: 0.1602, Train Accuracy: 0.7121, Val Loss: 0.1675, Val Accuracy: 0.6484\n",
            "Epoch 1/1000 - Train Loss: 0.2150, Train Accuracy: 0.6286, Val Loss: 0.2279, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.2103, Train Accuracy: 0.6286, Val Loss: 0.2227, Val Accuracy: 0.6044\n",
            "Epoch 201/1000 - Train Loss: 0.2058, Train Accuracy: 0.6286, Val Loss: 0.2178, Val Accuracy: 0.6044\n",
            "Epoch 301/1000 - Train Loss: 0.2015, Train Accuracy: 0.6286, Val Loss: 0.2132, Val Accuracy: 0.6044\n",
            "Epoch 401/1000 - Train Loss: 0.1975, Train Accuracy: 0.6286, Val Loss: 0.2088, Val Accuracy: 0.6044\n",
            "Epoch 501/1000 - Train Loss: 0.1936, Train Accuracy: 0.6286, Val Loss: 0.2047, Val Accuracy: 0.6044\n",
            "Epoch 601/1000 - Train Loss: 0.1900, Train Accuracy: 0.6308, Val Loss: 0.2007, Val Accuracy: 0.6044\n",
            "Epoch 701/1000 - Train Loss: 0.1864, Train Accuracy: 0.6308, Val Loss: 0.1969, Val Accuracy: 0.6044\n",
            "Epoch 801/1000 - Train Loss: 0.1830, Train Accuracy: 0.6308, Val Loss: 0.1933, Val Accuracy: 0.6044\n",
            "Epoch 901/1000 - Train Loss: 0.1797, Train Accuracy: 0.6308, Val Loss: 0.1898, Val Accuracy: 0.6044\n",
            "Epoch 1/1000 - Train Loss: 0.4266, Train Accuracy: 0.3714, Val Loss: 0.4160, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.3852, Train Accuracy: 0.3714, Val Loss: 0.3785, Val Accuracy: 0.3956\n",
            "Epoch 201/1000 - Train Loss: 0.3454, Train Accuracy: 0.3714, Val Loss: 0.3423, Val Accuracy: 0.3956\n",
            "Epoch 301/1000 - Train Loss: 0.3090, Train Accuracy: 0.3736, Val Loss: 0.3091, Val Accuracy: 0.3956\n",
            "Epoch 401/1000 - Train Loss: 0.2771, Train Accuracy: 0.4044, Val Loss: 0.2797, Val Accuracy: 0.4176\n",
            "Epoch 501/1000 - Train Loss: 0.2499, Train Accuracy: 0.4857, Val Loss: 0.2545, Val Accuracy: 0.5165\n",
            "Epoch 601/1000 - Train Loss: 0.2271, Train Accuracy: 0.5978, Val Loss: 0.2332, Val Accuracy: 0.6044\n",
            "Epoch 701/1000 - Train Loss: 0.2082, Train Accuracy: 0.7033, Val Loss: 0.2155, Val Accuracy: 0.6813\n",
            "Epoch 801/1000 - Train Loss: 0.1927, Train Accuracy: 0.7648, Val Loss: 0.2006, Val Accuracy: 0.6923\n",
            "Epoch 901/1000 - Train Loss: 0.1798, Train Accuracy: 0.8088, Val Loss: 0.1883, Val Accuracy: 0.7363\n",
            "Epoch 1/1000 - Train Loss: 0.2000, Train Accuracy: 0.6462, Val Loss: 0.2164, Val Accuracy: 0.6154\n",
            "Epoch 101/1000 - Train Loss: 0.1880, Train Accuracy: 0.6593, Val Loss: 0.2033, Val Accuracy: 0.6154\n",
            "Epoch 201/1000 - Train Loss: 0.1771, Train Accuracy: 0.6747, Val Loss: 0.1915, Val Accuracy: 0.6484\n",
            "Epoch 301/1000 - Train Loss: 0.1672, Train Accuracy: 0.7121, Val Loss: 0.1808, Val Accuracy: 0.6703\n",
            "Epoch 401/1000 - Train Loss: 0.1583, Train Accuracy: 0.7473, Val Loss: 0.1712, Val Accuracy: 0.7033\n",
            "Epoch 501/1000 - Train Loss: 0.1503, Train Accuracy: 0.7736, Val Loss: 0.1625, Val Accuracy: 0.7363\n",
            "Epoch 601/1000 - Train Loss: 0.1430, Train Accuracy: 0.8000, Val Loss: 0.1547, Val Accuracy: 0.7912\n",
            "Epoch 701/1000 - Train Loss: 0.1365, Train Accuracy: 0.8264, Val Loss: 0.1476, Val Accuracy: 0.8132\n",
            "Epoch 801/1000 - Train Loss: 0.1306, Train Accuracy: 0.8549, Val Loss: 0.1413, Val Accuracy: 0.8571\n",
            "Epoch 901/1000 - Train Loss: 0.1252, Train Accuracy: 0.8593, Val Loss: 0.1356, Val Accuracy: 0.8681\n",
            "Epoch 1/1000 - Train Loss: 0.2408, Train Accuracy: 0.6198, Val Loss: 0.2438, Val Accuracy: 0.6154\n",
            "Epoch 101/1000 - Train Loss: 0.2247, Train Accuracy: 0.6813, Val Loss: 0.2285, Val Accuracy: 0.6484\n",
            "Epoch 201/1000 - Train Loss: 0.2103, Train Accuracy: 0.7077, Val Loss: 0.2146, Val Accuracy: 0.6923\n",
            "Epoch 301/1000 - Train Loss: 0.1973, Train Accuracy: 0.7407, Val Loss: 0.2021, Val Accuracy: 0.7033\n",
            "Epoch 401/1000 - Train Loss: 0.1858, Train Accuracy: 0.7802, Val Loss: 0.1909, Val Accuracy: 0.7802\n",
            "Epoch 501/1000 - Train Loss: 0.1755, Train Accuracy: 0.7912, Val Loss: 0.1808, Val Accuracy: 0.7802\n",
            "Epoch 601/1000 - Train Loss: 0.1663, Train Accuracy: 0.8066, Val Loss: 0.1717, Val Accuracy: 0.7912\n",
            "Epoch 701/1000 - Train Loss: 0.1582, Train Accuracy: 0.8154, Val Loss: 0.1636, Val Accuracy: 0.8132\n",
            "Epoch 801/1000 - Train Loss: 0.1510, Train Accuracy: 0.8286, Val Loss: 0.1563, Val Accuracy: 0.8462\n",
            "Epoch 901/1000 - Train Loss: 0.1446, Train Accuracy: 0.8374, Val Loss: 0.1497, Val Accuracy: 0.8571\n",
            "Epoch 1/1000 - Train Loss: 0.3820, Train Accuracy: 0.3714, Val Loss: 0.3722, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.3469, Train Accuracy: 0.3714, Val Loss: 0.3385, Val Accuracy: 0.3956\n",
            "Epoch 201/1000 - Train Loss: 0.3145, Train Accuracy: 0.3714, Val Loss: 0.3072, Val Accuracy: 0.3956\n",
            "Epoch 301/1000 - Train Loss: 0.2855, Train Accuracy: 0.3714, Val Loss: 0.2791, Val Accuracy: 0.3956\n",
            "Epoch 401/1000 - Train Loss: 0.2600, Train Accuracy: 0.3714, Val Loss: 0.2544, Val Accuracy: 0.3956\n",
            "Epoch 501/1000 - Train Loss: 0.2381, Train Accuracy: 0.3956, Val Loss: 0.2332, Val Accuracy: 0.4286\n",
            "Epoch 601/1000 - Train Loss: 0.2196, Train Accuracy: 0.4835, Val Loss: 0.2151, Val Accuracy: 0.4725\n",
            "Epoch 701/1000 - Train Loss: 0.2039, Train Accuracy: 0.6066, Val Loss: 0.1999, Val Accuracy: 0.5824\n",
            "Epoch 801/1000 - Train Loss: 0.1908, Train Accuracy: 0.7253, Val Loss: 0.1873, Val Accuracy: 0.7253\n",
            "Epoch 901/1000 - Train Loss: 0.1799, Train Accuracy: 0.7868, Val Loss: 0.1767, Val Accuracy: 0.7582\n",
            "Epoch 1/1000 - Train Loss: 0.3498, Train Accuracy: 0.3714, Val Loss: 0.3334, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.3234, Train Accuracy: 0.3736, Val Loss: 0.3088, Val Accuracy: 0.3956\n",
            "Epoch 201/1000 - Train Loss: 0.2999, Train Accuracy: 0.3714, Val Loss: 0.2870, Val Accuracy: 0.3956\n",
            "Epoch 301/1000 - Train Loss: 0.2793, Train Accuracy: 0.3714, Val Loss: 0.2680, Val Accuracy: 0.3956\n",
            "Epoch 401/1000 - Train Loss: 0.2614, Train Accuracy: 0.3956, Val Loss: 0.2515, Val Accuracy: 0.4286\n",
            "Epoch 501/1000 - Train Loss: 0.2460, Train Accuracy: 0.4242, Val Loss: 0.2373, Val Accuracy: 0.4505\n",
            "Epoch 601/1000 - Train Loss: 0.2327, Train Accuracy: 0.5011, Val Loss: 0.2252, Val Accuracy: 0.5165\n",
            "Epoch 701/1000 - Train Loss: 0.2212, Train Accuracy: 0.5780, Val Loss: 0.2147, Val Accuracy: 0.5604\n",
            "Epoch 801/1000 - Train Loss: 0.2113, Train Accuracy: 0.6659, Val Loss: 0.2056, Val Accuracy: 0.6703\n",
            "Epoch 901/1000 - Train Loss: 0.2026, Train Accuracy: 0.7253, Val Loss: 0.1977, Val Accuracy: 0.7473\n",
            "Epoch 1/1000 - Train Loss: 0.3971, Train Accuracy: 0.3385, Val Loss: 0.3935, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.3696, Train Accuracy: 0.3033, Val Loss: 0.3686, Val Accuracy: 0.3187\n",
            "Epoch 201/1000 - Train Loss: 0.3444, Train Accuracy: 0.2615, Val Loss: 0.3458, Val Accuracy: 0.2637\n",
            "Epoch 301/1000 - Train Loss: 0.3219, Train Accuracy: 0.2220, Val Loss: 0.3253, Val Accuracy: 0.2527\n",
            "Epoch 401/1000 - Train Loss: 0.3021, Train Accuracy: 0.2088, Val Loss: 0.3073, Val Accuracy: 0.2088\n",
            "Epoch 501/1000 - Train Loss: 0.2848, Train Accuracy: 0.2418, Val Loss: 0.2915, Val Accuracy: 0.2088\n",
            "Epoch 601/1000 - Train Loss: 0.2698, Train Accuracy: 0.3275, Val Loss: 0.2778, Val Accuracy: 0.2637\n",
            "Epoch 701/1000 - Train Loss: 0.2569, Train Accuracy: 0.4286, Val Loss: 0.2659, Val Accuracy: 0.3736\n",
            "Epoch 801/1000 - Train Loss: 0.2457, Train Accuracy: 0.5758, Val Loss: 0.2556, Val Accuracy: 0.5275\n",
            "Epoch 901/1000 - Train Loss: 0.2359, Train Accuracy: 0.6264, Val Loss: 0.2466, Val Accuracy: 0.5824\n",
            "Epoch 1/1000 - Train Loss: 0.4353, Train Accuracy: 0.3714, Val Loss: 0.4223, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.4017, Train Accuracy: 0.3714, Val Loss: 0.3914, Val Accuracy: 0.3956\n",
            "Epoch 201/1000 - Train Loss: 0.3692, Train Accuracy: 0.3714, Val Loss: 0.3613, Val Accuracy: 0.3956\n",
            "Epoch 301/1000 - Train Loss: 0.3388, Train Accuracy: 0.3714, Val Loss: 0.3332, Val Accuracy: 0.3956\n",
            "Epoch 401/1000 - Train Loss: 0.3112, Train Accuracy: 0.3714, Val Loss: 0.3076, Val Accuracy: 0.3956\n",
            "Epoch 501/1000 - Train Loss: 0.2867, Train Accuracy: 0.3714, Val Loss: 0.2849, Val Accuracy: 0.3956\n",
            "Epoch 601/1000 - Train Loss: 0.2654, Train Accuracy: 0.3802, Val Loss: 0.2650, Val Accuracy: 0.3956\n",
            "Epoch 701/1000 - Train Loss: 0.2470, Train Accuracy: 0.4110, Val Loss: 0.2478, Val Accuracy: 0.3956\n",
            "Epoch 801/1000 - Train Loss: 0.2312, Train Accuracy: 0.5011, Val Loss: 0.2329, Val Accuracy: 0.4945\n",
            "Epoch 901/1000 - Train Loss: 0.2177, Train Accuracy: 0.6110, Val Loss: 0.2202, Val Accuracy: 0.5714\n",
            "Epoch 1/1000 - Train Loss: 0.2481, Train Accuracy: 0.4242, Val Loss: 0.2390, Val Accuracy: 0.4615\n",
            "Epoch 101/1000 - Train Loss: 0.1657, Train Accuracy: 0.9099, Val Loss: 0.1633, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.1297, Train Accuracy: 0.9253, Val Loss: 0.1298, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.1058, Train Accuracy: 0.9341, Val Loss: 0.1073, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0885, Train Accuracy: 0.9385, Val Loss: 0.0910, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0758, Train Accuracy: 0.9495, Val Loss: 0.0792, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0664, Train Accuracy: 0.9516, Val Loss: 0.0704, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0593, Train Accuracy: 0.9560, Val Loss: 0.0639, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0537, Train Accuracy: 0.9560, Val Loss: 0.0589, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0493, Train Accuracy: 0.9604, Val Loss: 0.0550, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.2925, Train Accuracy: 0.3714, Val Loss: 0.2941, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.1972, Train Accuracy: 0.7780, Val Loss: 0.2077, Val Accuracy: 0.7582\n",
            "Epoch 201/1000 - Train Loss: 0.1672, Train Accuracy: 0.8000, Val Loss: 0.1784, Val Accuracy: 0.7912\n",
            "Epoch 301/1000 - Train Loss: 0.1447, Train Accuracy: 0.8637, Val Loss: 0.1550, Val Accuracy: 0.8462\n",
            "Epoch 401/1000 - Train Loss: 0.1253, Train Accuracy: 0.8857, Val Loss: 0.1347, Val Accuracy: 0.8901\n",
            "Epoch 501/1000 - Train Loss: 0.1091, Train Accuracy: 0.9055, Val Loss: 0.1182, Val Accuracy: 0.9011\n",
            "Epoch 601/1000 - Train Loss: 0.0959, Train Accuracy: 0.9143, Val Loss: 0.1053, Val Accuracy: 0.9011\n",
            "Epoch 701/1000 - Train Loss: 0.0853, Train Accuracy: 0.9165, Val Loss: 0.0953, Val Accuracy: 0.9121\n",
            "Epoch 801/1000 - Train Loss: 0.0768, Train Accuracy: 0.9209, Val Loss: 0.0875, Val Accuracy: 0.9121\n",
            "Epoch 901/1000 - Train Loss: 0.0698, Train Accuracy: 0.9187, Val Loss: 0.0813, Val Accuracy: 0.9121\n",
            "Epoch 1/1000 - Train Loss: 0.2467, Train Accuracy: 0.6066, Val Loss: 0.2526, Val Accuracy: 0.5495\n",
            "Epoch 101/1000 - Train Loss: 0.1868, Train Accuracy: 0.8286, Val Loss: 0.1954, Val Accuracy: 0.8132\n",
            "Epoch 201/1000 - Train Loss: 0.1482, Train Accuracy: 0.8857, Val Loss: 0.1571, Val Accuracy: 0.8681\n",
            "Epoch 301/1000 - Train Loss: 0.1201, Train Accuracy: 0.9011, Val Loss: 0.1289, Val Accuracy: 0.8901\n",
            "Epoch 401/1000 - Train Loss: 0.0993, Train Accuracy: 0.9121, Val Loss: 0.1082, Val Accuracy: 0.9011\n",
            "Epoch 501/1000 - Train Loss: 0.0839, Train Accuracy: 0.9209, Val Loss: 0.0931, Val Accuracy: 0.9121\n",
            "Epoch 601/1000 - Train Loss: 0.0723, Train Accuracy: 0.9297, Val Loss: 0.0819, Val Accuracy: 0.9231\n",
            "Epoch 701/1000 - Train Loss: 0.0634, Train Accuracy: 0.9385, Val Loss: 0.0734, Val Accuracy: 0.9231\n",
            "Epoch 801/1000 - Train Loss: 0.0563, Train Accuracy: 0.9473, Val Loss: 0.0668, Val Accuracy: 0.9231\n",
            "Epoch 901/1000 - Train Loss: 0.0506, Train Accuracy: 0.9516, Val Loss: 0.0614, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.2226, Train Accuracy: 0.6286, Val Loss: 0.2399, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.1418, Train Accuracy: 0.8088, Val Loss: 0.1471, Val Accuracy: 0.7692\n",
            "Epoch 201/1000 - Train Loss: 0.1064, Train Accuracy: 0.9253, Val Loss: 0.1074, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0868, Train Accuracy: 0.9363, Val Loss: 0.0867, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0742, Train Accuracy: 0.9407, Val Loss: 0.0738, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0653, Train Accuracy: 0.9429, Val Loss: 0.0652, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0588, Train Accuracy: 0.9451, Val Loss: 0.0591, Val Accuracy: 0.9560\n",
            "Epoch 701/1000 - Train Loss: 0.0536, Train Accuracy: 0.9473, Val Loss: 0.0546, Val Accuracy: 0.9560\n",
            "Epoch 801/1000 - Train Loss: 0.0495, Train Accuracy: 0.9473, Val Loss: 0.0512, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0461, Train Accuracy: 0.9516, Val Loss: 0.0485, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.2489, Train Accuracy: 0.6198, Val Loss: 0.2619, Val Accuracy: 0.5714\n",
            "Epoch 101/1000 - Train Loss: 0.1656, Train Accuracy: 0.6857, Val Loss: 0.1779, Val Accuracy: 0.6374\n",
            "Epoch 201/1000 - Train Loss: 0.1278, Train Accuracy: 0.8264, Val Loss: 0.1388, Val Accuracy: 0.8242\n",
            "Epoch 301/1000 - Train Loss: 0.1036, Train Accuracy: 0.8967, Val Loss: 0.1142, Val Accuracy: 0.9011\n",
            "Epoch 401/1000 - Train Loss: 0.0863, Train Accuracy: 0.9099, Val Loss: 0.0971, Val Accuracy: 0.9121\n",
            "Epoch 501/1000 - Train Loss: 0.0739, Train Accuracy: 0.9231, Val Loss: 0.0854, Val Accuracy: 0.9121\n",
            "Epoch 601/1000 - Train Loss: 0.0650, Train Accuracy: 0.9341, Val Loss: 0.0769, Val Accuracy: 0.9231\n",
            "Epoch 701/1000 - Train Loss: 0.0581, Train Accuracy: 0.9451, Val Loss: 0.0706, Val Accuracy: 0.9231\n",
            "Epoch 801/1000 - Train Loss: 0.0527, Train Accuracy: 0.9473, Val Loss: 0.0654, Val Accuracy: 0.9231\n",
            "Epoch 901/1000 - Train Loss: 0.0482, Train Accuracy: 0.9560, Val Loss: 0.0611, Val Accuracy: 0.9231\n",
            "Epoch 1/1000 - Train Loss: 0.3086, Train Accuracy: 0.3868, Val Loss: 0.2850, Val Accuracy: 0.4835\n",
            "Epoch 101/1000 - Train Loss: 0.1299, Train Accuracy: 0.9077, Val Loss: 0.1281, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.0851, Train Accuracy: 0.9407, Val Loss: 0.0878, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0687, Train Accuracy: 0.9473, Val Loss: 0.0729, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0595, Train Accuracy: 0.9495, Val Loss: 0.0647, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0533, Train Accuracy: 0.9516, Val Loss: 0.0593, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0486, Train Accuracy: 0.9538, Val Loss: 0.0553, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0450, Train Accuracy: 0.9582, Val Loss: 0.0522, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0421, Train Accuracy: 0.9582, Val Loss: 0.0497, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0396, Train Accuracy: 0.9560, Val Loss: 0.0475, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.3073, Train Accuracy: 0.4571, Val Loss: 0.2984, Val Accuracy: 0.4615\n",
            "Epoch 101/1000 - Train Loss: 0.1146, Train Accuracy: 0.8967, Val Loss: 0.1171, Val Accuracy: 0.8901\n",
            "Epoch 201/1000 - Train Loss: 0.0787, Train Accuracy: 0.9363, Val Loss: 0.0835, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0657, Train Accuracy: 0.9560, Val Loss: 0.0713, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0579, Train Accuracy: 0.9516, Val Loss: 0.0642, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0524, Train Accuracy: 0.9538, Val Loss: 0.0592, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0482, Train Accuracy: 0.9582, Val Loss: 0.0554, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0448, Train Accuracy: 0.9582, Val Loss: 0.0524, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0419, Train Accuracy: 0.9604, Val Loss: 0.0499, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0395, Train Accuracy: 0.9604, Val Loss: 0.0478, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.2968, Train Accuracy: 0.3231, Val Loss: 0.2828, Val Accuracy: 0.4066\n",
            "Epoch 101/1000 - Train Loss: 0.1906, Train Accuracy: 0.8044, Val Loss: 0.1867, Val Accuracy: 0.8352\n",
            "Epoch 201/1000 - Train Loss: 0.1516, Train Accuracy: 0.8374, Val Loss: 0.1507, Val Accuracy: 0.8571\n",
            "Epoch 301/1000 - Train Loss: 0.1260, Train Accuracy: 0.8725, Val Loss: 0.1266, Val Accuracy: 0.8901\n",
            "Epoch 401/1000 - Train Loss: 0.1068, Train Accuracy: 0.9033, Val Loss: 0.1083, Val Accuracy: 0.9121\n",
            "Epoch 501/1000 - Train Loss: 0.0919, Train Accuracy: 0.9253, Val Loss: 0.0943, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0804, Train Accuracy: 0.9341, Val Loss: 0.0834, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0712, Train Accuracy: 0.9407, Val Loss: 0.0749, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0638, Train Accuracy: 0.9451, Val Loss: 0.0681, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0579, Train Accuracy: 0.9516, Val Loss: 0.0626, Val Accuracy: 0.9560\n",
            "Epoch 1/1000 - Train Loss: 0.4659, Train Accuracy: 0.3714, Val Loss: 0.4487, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.2123, Train Accuracy: 0.7253, Val Loss: 0.2132, Val Accuracy: 0.7363\n",
            "Epoch 201/1000 - Train Loss: 0.1560, Train Accuracy: 0.9121, Val Loss: 0.1600, Val Accuracy: 0.9121\n",
            "Epoch 301/1000 - Train Loss: 0.1284, Train Accuracy: 0.9209, Val Loss: 0.1323, Val Accuracy: 0.9341\n",
            "Epoch 401/1000 - Train Loss: 0.1065, Train Accuracy: 0.9385, Val Loss: 0.1099, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0891, Train Accuracy: 0.9407, Val Loss: 0.0922, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0757, Train Accuracy: 0.9473, Val Loss: 0.0787, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0655, Train Accuracy: 0.9495, Val Loss: 0.0687, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0577, Train Accuracy: 0.9560, Val Loss: 0.0612, Val Accuracy: 0.9560\n",
            "Epoch 901/1000 - Train Loss: 0.0517, Train Accuracy: 0.9582, Val Loss: 0.0555, Val Accuracy: 0.9560\n",
            "Epoch 1/1000 - Train Loss: 0.4143, Train Accuracy: 0.3714, Val Loss: 0.4008, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.2062, Train Accuracy: 0.7099, Val Loss: 0.2057, Val Accuracy: 0.7363\n",
            "Epoch 201/1000 - Train Loss: 0.1369, Train Accuracy: 0.8549, Val Loss: 0.1393, Val Accuracy: 0.8462\n",
            "Epoch 301/1000 - Train Loss: 0.1059, Train Accuracy: 0.9055, Val Loss: 0.1088, Val Accuracy: 0.9011\n",
            "Epoch 401/1000 - Train Loss: 0.0870, Train Accuracy: 0.9165, Val Loss: 0.0905, Val Accuracy: 0.9011\n",
            "Epoch 501/1000 - Train Loss: 0.0742, Train Accuracy: 0.9297, Val Loss: 0.0784, Val Accuracy: 0.9121\n",
            "Epoch 601/1000 - Train Loss: 0.0649, Train Accuracy: 0.9319, Val Loss: 0.0700, Val Accuracy: 0.9121\n",
            "Epoch 701/1000 - Train Loss: 0.0578, Train Accuracy: 0.9407, Val Loss: 0.0639, Val Accuracy: 0.9121\n",
            "Epoch 801/1000 - Train Loss: 0.0522, Train Accuracy: 0.9451, Val Loss: 0.0592, Val Accuracy: 0.9121\n",
            "Epoch 901/1000 - Train Loss: 0.0476, Train Accuracy: 0.9473, Val Loss: 0.0554, Val Accuracy: 0.9121\n",
            "Epoch 1/1000 - Train Loss: 0.3636, Train Accuracy: 0.3714, Val Loss: 0.3396, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0766, Train Accuracy: 0.9495, Val Loss: 0.0775, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0410, Train Accuracy: 0.9736, Val Loss: 0.0462, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0292, Train Accuracy: 0.9802, Val Loss: 0.0363, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0238, Train Accuracy: 0.9824, Val Loss: 0.0324, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0209, Train Accuracy: 0.9824, Val Loss: 0.0306, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0191, Train Accuracy: 0.9802, Val Loss: 0.0297, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0179, Train Accuracy: 0.9802, Val Loss: 0.0291, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0170, Train Accuracy: 0.9802, Val Loss: 0.0288, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0163, Train Accuracy: 0.9824, Val Loss: 0.0286, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3417, Train Accuracy: 0.3780, Val Loss: 0.3395, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0458, Train Accuracy: 0.9495, Val Loss: 0.0553, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0313, Train Accuracy: 0.9648, Val Loss: 0.0401, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0253, Train Accuracy: 0.9692, Val Loss: 0.0346, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0221, Train Accuracy: 0.9736, Val Loss: 0.0321, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0201, Train Accuracy: 0.9780, Val Loss: 0.0307, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0187, Train Accuracy: 0.9824, Val Loss: 0.0299, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0177, Train Accuracy: 0.9824, Val Loss: 0.0293, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0169, Train Accuracy: 0.9824, Val Loss: 0.0289, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0162, Train Accuracy: 0.9824, Val Loss: 0.0285, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.4089, Train Accuracy: 0.3670, Val Loss: 0.3975, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0606, Train Accuracy: 0.9451, Val Loss: 0.0675, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0374, Train Accuracy: 0.9626, Val Loss: 0.0460, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0279, Train Accuracy: 0.9780, Val Loss: 0.0373, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0235, Train Accuracy: 0.9802, Val Loss: 0.0336, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0210, Train Accuracy: 0.9824, Val Loss: 0.0318, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0194, Train Accuracy: 0.9824, Val Loss: 0.0308, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0182, Train Accuracy: 0.9824, Val Loss: 0.0302, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0173, Train Accuracy: 0.9824, Val Loss: 0.0297, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0165, Train Accuracy: 0.9824, Val Loss: 0.0292, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3137, Train Accuracy: 0.3538, Val Loss: 0.3203, Val Accuracy: 0.3626\n",
            "Epoch 101/1000 - Train Loss: 0.0770, Train Accuracy: 0.9429, Val Loss: 0.0814, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0454, Train Accuracy: 0.9538, Val Loss: 0.0521, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0322, Train Accuracy: 0.9670, Val Loss: 0.0408, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0261, Train Accuracy: 0.9736, Val Loss: 0.0352, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0228, Train Accuracy: 0.9736, Val Loss: 0.0326, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0208, Train Accuracy: 0.9802, Val Loss: 0.0313, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0194, Train Accuracy: 0.9802, Val Loss: 0.0306, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0183, Train Accuracy: 0.9802, Val Loss: 0.0301, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0175, Train Accuracy: 0.9802, Val Loss: 0.0298, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3600, Train Accuracy: 0.3714, Val Loss: 0.3496, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0883, Train Accuracy: 0.9363, Val Loss: 0.0907, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0501, Train Accuracy: 0.9538, Val Loss: 0.0539, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0353, Train Accuracy: 0.9648, Val Loss: 0.0411, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0281, Train Accuracy: 0.9758, Val Loss: 0.0357, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0242, Train Accuracy: 0.9802, Val Loss: 0.0333, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0218, Train Accuracy: 0.9802, Val Loss: 0.0321, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0202, Train Accuracy: 0.9780, Val Loss: 0.0314, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0191, Train Accuracy: 0.9780, Val Loss: 0.0309, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0182, Train Accuracy: 0.9780, Val Loss: 0.0305, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.1885, Train Accuracy: 0.7912, Val Loss: 0.1844, Val Accuracy: 0.8352\n",
            "Epoch 101/1000 - Train Loss: 0.0648, Train Accuracy: 0.9473, Val Loss: 0.0711, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0397, Train Accuracy: 0.9648, Val Loss: 0.0495, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0291, Train Accuracy: 0.9802, Val Loss: 0.0401, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0235, Train Accuracy: 0.9846, Val Loss: 0.0349, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0203, Train Accuracy: 0.9824, Val Loss: 0.0321, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0185, Train Accuracy: 0.9824, Val Loss: 0.0307, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0173, Train Accuracy: 0.9824, Val Loss: 0.0300, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0165, Train Accuracy: 0.9824, Val Loss: 0.0295, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0158, Train Accuracy: 0.9824, Val Loss: 0.0291, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3106, Train Accuracy: 0.3758, Val Loss: 0.3004, Val Accuracy: 0.4066\n",
            "Epoch 101/1000 - Train Loss: 0.0716, Train Accuracy: 0.9407, Val Loss: 0.0752, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0417, Train Accuracy: 0.9626, Val Loss: 0.0474, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0311, Train Accuracy: 0.9758, Val Loss: 0.0385, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0259, Train Accuracy: 0.9802, Val Loss: 0.0348, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0229, Train Accuracy: 0.9802, Val Loss: 0.0330, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0210, Train Accuracy: 0.9802, Val Loss: 0.0319, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0196, Train Accuracy: 0.9802, Val Loss: 0.0312, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0186, Train Accuracy: 0.9802, Val Loss: 0.0307, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0178, Train Accuracy: 0.9802, Val Loss: 0.0303, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2506, Train Accuracy: 0.6066, Val Loss: 0.2445, Val Accuracy: 0.5824\n",
            "Epoch 101/1000 - Train Loss: 0.0427, Train Accuracy: 0.9626, Val Loss: 0.0527, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0303, Train Accuracy: 0.9692, Val Loss: 0.0426, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0249, Train Accuracy: 0.9758, Val Loss: 0.0372, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0219, Train Accuracy: 0.9780, Val Loss: 0.0341, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0199, Train Accuracy: 0.9802, Val Loss: 0.0323, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0185, Train Accuracy: 0.9824, Val Loss: 0.0311, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0175, Train Accuracy: 0.9824, Val Loss: 0.0304, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0166, Train Accuracy: 0.9846, Val Loss: 0.0298, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0160, Train Accuracy: 0.9846, Val Loss: 0.0294, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3083, Train Accuracy: 0.5934, Val Loss: 0.3304, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.0853, Train Accuracy: 0.9099, Val Loss: 0.0980, Val Accuracy: 0.8791\n",
            "Epoch 201/1000 - Train Loss: 0.0468, Train Accuracy: 0.9516, Val Loss: 0.0596, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0326, Train Accuracy: 0.9648, Val Loss: 0.0449, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0262, Train Accuracy: 0.9736, Val Loss: 0.0383, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0227, Train Accuracy: 0.9802, Val Loss: 0.0350, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0206, Train Accuracy: 0.9802, Val Loss: 0.0332, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0191, Train Accuracy: 0.9802, Val Loss: 0.0321, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0180, Train Accuracy: 0.9802, Val Loss: 0.0313, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0171, Train Accuracy: 0.9824, Val Loss: 0.0307, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2858, Train Accuracy: 0.3736, Val Loss: 0.2794, Val Accuracy: 0.4286\n",
            "Epoch 101/1000 - Train Loss: 0.0653, Train Accuracy: 0.9538, Val Loss: 0.0667, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0366, Train Accuracy: 0.9758, Val Loss: 0.0422, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0267, Train Accuracy: 0.9780, Val Loss: 0.0346, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0221, Train Accuracy: 0.9780, Val Loss: 0.0313, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0196, Train Accuracy: 0.9780, Val Loss: 0.0298, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0181, Train Accuracy: 0.9846, Val Loss: 0.0290, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0170, Train Accuracy: 0.9846, Val Loss: 0.0286, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0162, Train Accuracy: 0.9824, Val Loss: 0.0283, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0156, Train Accuracy: 0.9846, Val Loss: 0.0281, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2358, Train Accuracy: 0.6308, Val Loss: 0.2566, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.0436, Train Accuracy: 0.9648, Val Loss: 0.0488, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0283, Train Accuracy: 0.9670, Val Loss: 0.0367, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0231, Train Accuracy: 0.9736, Val Loss: 0.0338, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0203, Train Accuracy: 0.9780, Val Loss: 0.0327, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0184, Train Accuracy: 0.9780, Val Loss: 0.0321, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0168, Train Accuracy: 0.9802, Val Loss: 0.0315, Val Accuracy: 0.9560\n",
            "Epoch 701/1000 - Train Loss: 0.0155, Train Accuracy: 0.9846, Val Loss: 0.0309, Val Accuracy: 0.9560\n",
            "Epoch 801/1000 - Train Loss: 0.0146, Train Accuracy: 0.9846, Val Loss: 0.0303, Val Accuracy: 0.9560\n",
            "Epoch 901/1000 - Train Loss: 0.0139, Train Accuracy: 0.9868, Val Loss: 0.0297, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3098, Train Accuracy: 0.4418, Val Loss: 0.2891, Val Accuracy: 0.4725\n",
            "Epoch 101/1000 - Train Loss: 0.0341, Train Accuracy: 0.9648, Val Loss: 0.0432, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0239, Train Accuracy: 0.9758, Val Loss: 0.0355, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0198, Train Accuracy: 0.9802, Val Loss: 0.0325, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0177, Train Accuracy: 0.9846, Val Loss: 0.0310, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0163, Train Accuracy: 0.9846, Val Loss: 0.0301, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0153, Train Accuracy: 0.9846, Val Loss: 0.0295, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0145, Train Accuracy: 0.9846, Val Loss: 0.0289, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0139, Train Accuracy: 0.9868, Val Loss: 0.0284, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0134, Train Accuracy: 0.9868, Val Loss: 0.0280, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3065, Train Accuracy: 0.3758, Val Loss: 0.3118, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0509, Train Accuracy: 0.9319, Val Loss: 0.0615, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.0310, Train Accuracy: 0.9648, Val Loss: 0.0401, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0237, Train Accuracy: 0.9714, Val Loss: 0.0339, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0203, Train Accuracy: 0.9780, Val Loss: 0.0319, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0183, Train Accuracy: 0.9780, Val Loss: 0.0309, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0169, Train Accuracy: 0.9802, Val Loss: 0.0303, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0159, Train Accuracy: 0.9824, Val Loss: 0.0297, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0151, Train Accuracy: 0.9846, Val Loss: 0.0291, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0144, Train Accuracy: 0.9846, Val Loss: 0.0286, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.4063, Train Accuracy: 0.3714, Val Loss: 0.3935, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0404, Train Accuracy: 0.9626, Val Loss: 0.0480, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0243, Train Accuracy: 0.9780, Val Loss: 0.0339, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0193, Train Accuracy: 0.9824, Val Loss: 0.0309, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0170, Train Accuracy: 0.9846, Val Loss: 0.0299, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0157, Train Accuracy: 0.9846, Val Loss: 0.0294, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0148, Train Accuracy: 0.9868, Val Loss: 0.0289, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0141, Train Accuracy: 0.9868, Val Loss: 0.0285, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0136, Train Accuracy: 0.9868, Val Loss: 0.0281, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0131, Train Accuracy: 0.9868, Val Loss: 0.0277, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.4728, Train Accuracy: 0.3714, Val Loss: 0.4532, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0360, Train Accuracy: 0.9692, Val Loss: 0.0441, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0230, Train Accuracy: 0.9802, Val Loss: 0.0329, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0191, Train Accuracy: 0.9802, Val Loss: 0.0308, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0173, Train Accuracy: 0.9824, Val Loss: 0.0302, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0162, Train Accuracy: 0.9824, Val Loss: 0.0298, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0153, Train Accuracy: 0.9824, Val Loss: 0.0295, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0147, Train Accuracy: 0.9824, Val Loss: 0.0291, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0141, Train Accuracy: 0.9824, Val Loss: 0.0288, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0136, Train Accuracy: 0.9824, Val Loss: 0.0284, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.5639, Train Accuracy: 0.3714, Val Loss: 0.5323, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0421, Train Accuracy: 0.9604, Val Loss: 0.0453, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0258, Train Accuracy: 0.9736, Val Loss: 0.0339, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0205, Train Accuracy: 0.9802, Val Loss: 0.0314, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0179, Train Accuracy: 0.9824, Val Loss: 0.0304, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0164, Train Accuracy: 0.9846, Val Loss: 0.0297, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0153, Train Accuracy: 0.9868, Val Loss: 0.0292, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0145, Train Accuracy: 0.9868, Val Loss: 0.0287, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0139, Train Accuracy: 0.9868, Val Loss: 0.0283, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0134, Train Accuracy: 0.9868, Val Loss: 0.0279, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2957, Train Accuracy: 0.6286, Val Loss: 0.3164, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.0484, Train Accuracy: 0.9516, Val Loss: 0.0541, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0272, Train Accuracy: 0.9714, Val Loss: 0.0360, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0212, Train Accuracy: 0.9802, Val Loss: 0.0326, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0185, Train Accuracy: 0.9824, Val Loss: 0.0317, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0169, Train Accuracy: 0.9824, Val Loss: 0.0312, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0158, Train Accuracy: 0.9846, Val Loss: 0.0308, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0150, Train Accuracy: 0.9846, Val Loss: 0.0304, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0143, Train Accuracy: 0.9846, Val Loss: 0.0299, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0138, Train Accuracy: 0.9846, Val Loss: 0.0294, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3414, Train Accuracy: 0.3187, Val Loss: 0.3270, Val Accuracy: 0.3407\n",
            "Epoch 101/1000 - Train Loss: 0.0512, Train Accuracy: 0.9560, Val Loss: 0.0644, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0293, Train Accuracy: 0.9758, Val Loss: 0.0425, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0223, Train Accuracy: 0.9802, Val Loss: 0.0343, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0193, Train Accuracy: 0.9780, Val Loss: 0.0317, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0175, Train Accuracy: 0.9824, Val Loss: 0.0304, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0163, Train Accuracy: 0.9824, Val Loss: 0.0295, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0154, Train Accuracy: 0.9824, Val Loss: 0.0288, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0282, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0140, Train Accuracy: 0.9824, Val Loss: 0.0277, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2495, Train Accuracy: 0.6286, Val Loss: 0.2574, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.0494, Train Accuracy: 0.9516, Val Loss: 0.0559, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0258, Train Accuracy: 0.9780, Val Loss: 0.0359, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0197, Train Accuracy: 0.9802, Val Loss: 0.0314, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0171, Train Accuracy: 0.9868, Val Loss: 0.0299, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0156, Train Accuracy: 0.9868, Val Loss: 0.0292, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0146, Train Accuracy: 0.9868, Val Loss: 0.0287, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0139, Train Accuracy: 0.9868, Val Loss: 0.0283, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0133, Train Accuracy: 0.9868, Val Loss: 0.0279, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0129, Train Accuracy: 0.9868, Val Loss: 0.0275, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.4811, Train Accuracy: 0.3714, Val Loss: 0.4558, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0346, Train Accuracy: 0.9758, Val Loss: 0.0406, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0225, Train Accuracy: 0.9824, Val Loss: 0.0323, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0185, Train Accuracy: 0.9846, Val Loss: 0.0301, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0165, Train Accuracy: 0.9846, Val Loss: 0.0293, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0153, Train Accuracy: 0.9846, Val Loss: 0.0288, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0144, Train Accuracy: 0.9868, Val Loss: 0.0285, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0137, Train Accuracy: 0.9868, Val Loss: 0.0281, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0132, Train Accuracy: 0.9868, Val Loss: 0.0278, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0128, Train Accuracy: 0.9868, Val Loss: 0.0275, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2351, Train Accuracy: 0.6374, Val Loss: 0.2376, Val Accuracy: 0.6264\n",
            "Epoch 101/1000 - Train Loss: 0.0256, Train Accuracy: 0.9802, Val Loss: 0.0370, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0184, Train Accuracy: 0.9802, Val Loss: 0.0308, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0162, Train Accuracy: 0.9824, Val Loss: 0.0294, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0150, Train Accuracy: 0.9824, Val Loss: 0.0284, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0140, Train Accuracy: 0.9824, Val Loss: 0.0276, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0131, Train Accuracy: 0.9824, Val Loss: 0.0268, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0124, Train Accuracy: 0.9824, Val Loss: 0.0261, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0118, Train Accuracy: 0.9846, Val Loss: 0.0255, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0113, Train Accuracy: 0.9846, Val Loss: 0.0250, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3280, Train Accuracy: 0.6286, Val Loss: 0.3494, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.0237, Train Accuracy: 0.9736, Val Loss: 0.0350, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0185, Train Accuracy: 0.9780, Val Loss: 0.0312, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0161, Train Accuracy: 0.9802, Val Loss: 0.0296, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0284, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0135, Train Accuracy: 0.9846, Val Loss: 0.0275, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0126, Train Accuracy: 0.9846, Val Loss: 0.0266, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0259, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0114, Train Accuracy: 0.9868, Val Loss: 0.0253, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0109, Train Accuracy: 0.9868, Val Loss: 0.0247, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.4930, Train Accuracy: 0.3714, Val Loss: 0.4735, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0268, Train Accuracy: 0.9736, Val Loss: 0.0410, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0186, Train Accuracy: 0.9802, Val Loss: 0.0335, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0160, Train Accuracy: 0.9824, Val Loss: 0.0310, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0146, Train Accuracy: 0.9846, Val Loss: 0.0296, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0137, Train Accuracy: 0.9846, Val Loss: 0.0286, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0130, Train Accuracy: 0.9846, Val Loss: 0.0278, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0124, Train Accuracy: 0.9846, Val Loss: 0.0271, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0266, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0261, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2169, Train Accuracy: 0.7495, Val Loss: 0.2123, Val Accuracy: 0.7802\n",
            "Epoch 101/1000 - Train Loss: 0.0282, Train Accuracy: 0.9670, Val Loss: 0.0349, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0194, Train Accuracy: 0.9758, Val Loss: 0.0306, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0163, Train Accuracy: 0.9824, Val Loss: 0.0292, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0145, Train Accuracy: 0.9846, Val Loss: 0.0282, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0134, Train Accuracy: 0.9846, Val Loss: 0.0273, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0126, Train Accuracy: 0.9846, Val Loss: 0.0266, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0259, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0114, Train Accuracy: 0.9846, Val Loss: 0.0254, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0110, Train Accuracy: 0.9868, Val Loss: 0.0249, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2265, Train Accuracy: 0.6286, Val Loss: 0.2348, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.0295, Train Accuracy: 0.9736, Val Loss: 0.0415, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0200, Train Accuracy: 0.9802, Val Loss: 0.0327, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0171, Train Accuracy: 0.9824, Val Loss: 0.0308, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0155, Train Accuracy: 0.9824, Val Loss: 0.0296, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0143, Train Accuracy: 0.9824, Val Loss: 0.0285, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0133, Train Accuracy: 0.9824, Val Loss: 0.0276, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0125, Train Accuracy: 0.9824, Val Loss: 0.0268, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0262, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0114, Train Accuracy: 0.9868, Val Loss: 0.0256, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2925, Train Accuracy: 0.6286, Val Loss: 0.3080, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.0278, Train Accuracy: 0.9758, Val Loss: 0.0376, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0183, Train Accuracy: 0.9824, Val Loss: 0.0312, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0156, Train Accuracy: 0.9824, Val Loss: 0.0298, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0141, Train Accuracy: 0.9846, Val Loss: 0.0288, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0131, Train Accuracy: 0.9868, Val Loss: 0.0278, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0124, Train Accuracy: 0.9868, Val Loss: 0.0270, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0117, Train Accuracy: 0.9868, Val Loss: 0.0262, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0112, Train Accuracy: 0.9868, Val Loss: 0.0255, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0108, Train Accuracy: 0.9868, Val Loss: 0.0248, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.4479, Train Accuracy: 0.3253, Val Loss: 0.4440, Val Accuracy: 0.3626\n",
            "Epoch 101/1000 - Train Loss: 0.0240, Train Accuracy: 0.9802, Val Loss: 0.0338, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0183, Train Accuracy: 0.9802, Val Loss: 0.0305, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0162, Train Accuracy: 0.9824, Val Loss: 0.0294, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0147, Train Accuracy: 0.9824, Val Loss: 0.0285, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0136, Train Accuracy: 0.9824, Val Loss: 0.0276, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0127, Train Accuracy: 0.9846, Val Loss: 0.0267, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0119, Train Accuracy: 0.9868, Val Loss: 0.0257, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0113, Train Accuracy: 0.9890, Val Loss: 0.0248, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0107, Train Accuracy: 0.9890, Val Loss: 0.0240, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.1854, Train Accuracy: 0.7275, Val Loss: 0.1865, Val Accuracy: 0.7253\n",
            "Epoch 101/1000 - Train Loss: 0.0210, Train Accuracy: 0.9824, Val Loss: 0.0311, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0163, Train Accuracy: 0.9824, Val Loss: 0.0293, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0143, Train Accuracy: 0.9846, Val Loss: 0.0284, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0131, Train Accuracy: 0.9868, Val Loss: 0.0276, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0123, Train Accuracy: 0.9868, Val Loss: 0.0270, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0116, Train Accuracy: 0.9868, Val Loss: 0.0264, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0111, Train Accuracy: 0.9868, Val Loss: 0.0259, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0107, Train Accuracy: 0.9890, Val Loss: 0.0254, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0104, Train Accuracy: 0.9890, Val Loss: 0.0250, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2916, Train Accuracy: 0.3912, Val Loss: 0.2950, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.0292, Train Accuracy: 0.9714, Val Loss: 0.0417, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0198, Train Accuracy: 0.9824, Val Loss: 0.0337, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0169, Train Accuracy: 0.9824, Val Loss: 0.0318, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0152, Train Accuracy: 0.9824, Val Loss: 0.0305, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0140, Train Accuracy: 0.9846, Val Loss: 0.0294, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0131, Train Accuracy: 0.9846, Val Loss: 0.0285, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0125, Train Accuracy: 0.9846, Val Loss: 0.0278, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0271, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0265, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2756, Train Accuracy: 0.5516, Val Loss: 0.2784, Val Accuracy: 0.5714\n",
            "Epoch 101/1000 - Train Loss: 0.0243, Train Accuracy: 0.9780, Val Loss: 0.0342, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0183, Train Accuracy: 0.9802, Val Loss: 0.0310, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0159, Train Accuracy: 0.9824, Val Loss: 0.0298, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0144, Train Accuracy: 0.9846, Val Loss: 0.0288, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0134, Train Accuracy: 0.9846, Val Loss: 0.0278, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0125, Train Accuracy: 0.9846, Val Loss: 0.0269, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0118, Train Accuracy: 0.9846, Val Loss: 0.0261, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0112, Train Accuracy: 0.9846, Val Loss: 0.0255, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0108, Train Accuracy: 0.9868, Val Loss: 0.0249, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3054, Train Accuracy: 0.3648, Val Loss: 0.3026, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.2830, Train Accuracy: 0.3780, Val Loss: 0.2787, Val Accuracy: 0.3516\n",
            "Epoch 201/1000 - Train Loss: 0.2632, Train Accuracy: 0.4242, Val Loss: 0.2577, Val Accuracy: 0.3846\n",
            "Epoch 301/1000 - Train Loss: 0.2460, Train Accuracy: 0.5077, Val Loss: 0.2395, Val Accuracy: 0.5385\n",
            "Epoch 401/1000 - Train Loss: 0.2314, Train Accuracy: 0.6088, Val Loss: 0.2240, Val Accuracy: 0.6593\n",
            "Epoch 501/1000 - Train Loss: 0.2188, Train Accuracy: 0.7033, Val Loss: 0.2109, Val Accuracy: 0.7692\n",
            "Epoch 601/1000 - Train Loss: 0.2081, Train Accuracy: 0.7802, Val Loss: 0.1996, Val Accuracy: 0.8462\n",
            "Epoch 701/1000 - Train Loss: 0.1988, Train Accuracy: 0.8352, Val Loss: 0.1900, Val Accuracy: 0.8571\n",
            "Epoch 801/1000 - Train Loss: 0.1907, Train Accuracy: 0.8791, Val Loss: 0.1816, Val Accuracy: 0.9011\n",
            "Epoch 901/1000 - Train Loss: 0.1836, Train Accuracy: 0.8857, Val Loss: 0.1742, Val Accuracy: 0.9121\n",
            "Epoch 1/1000 - Train Loss: 0.3529, Train Accuracy: 0.3714, Val Loss: 0.3493, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.3300, Train Accuracy: 0.3692, Val Loss: 0.3249, Val Accuracy: 0.3516\n",
            "Epoch 201/1000 - Train Loss: 0.3097, Train Accuracy: 0.3780, Val Loss: 0.3034, Val Accuracy: 0.3516\n",
            "Epoch 301/1000 - Train Loss: 0.2920, Train Accuracy: 0.3912, Val Loss: 0.2845, Val Accuracy: 0.3846\n",
            "Epoch 401/1000 - Train Loss: 0.2764, Train Accuracy: 0.4308, Val Loss: 0.2680, Val Accuracy: 0.4066\n",
            "Epoch 501/1000 - Train Loss: 0.2629, Train Accuracy: 0.4747, Val Loss: 0.2537, Val Accuracy: 0.4725\n",
            "Epoch 601/1000 - Train Loss: 0.2511, Train Accuracy: 0.5582, Val Loss: 0.2412, Val Accuracy: 0.6154\n",
            "Epoch 701/1000 - Train Loss: 0.2408, Train Accuracy: 0.6220, Val Loss: 0.2303, Val Accuracy: 0.7143\n",
            "Epoch 801/1000 - Train Loss: 0.2317, Train Accuracy: 0.6747, Val Loss: 0.2207, Val Accuracy: 0.7692\n",
            "Epoch 901/1000 - Train Loss: 0.2236, Train Accuracy: 0.7253, Val Loss: 0.2121, Val Accuracy: 0.8132\n",
            "Epoch 1/1000 - Train Loss: 0.2482, Train Accuracy: 0.6264, Val Loss: 0.2347, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.2245, Train Accuracy: 0.6330, Val Loss: 0.2122, Val Accuracy: 0.6703\n",
            "Epoch 201/1000 - Train Loss: 0.2029, Train Accuracy: 0.6593, Val Loss: 0.1915, Val Accuracy: 0.6923\n",
            "Epoch 301/1000 - Train Loss: 0.1840, Train Accuracy: 0.6813, Val Loss: 0.1732, Val Accuracy: 0.7143\n",
            "Epoch 401/1000 - Train Loss: 0.1677, Train Accuracy: 0.7011, Val Loss: 0.1572, Val Accuracy: 0.7253\n",
            "Epoch 501/1000 - Train Loss: 0.1538, Train Accuracy: 0.7538, Val Loss: 0.1434, Val Accuracy: 0.7582\n",
            "Epoch 601/1000 - Train Loss: 0.1421, Train Accuracy: 0.7868, Val Loss: 0.1316, Val Accuracy: 0.7692\n",
            "Epoch 701/1000 - Train Loss: 0.1323, Train Accuracy: 0.8198, Val Loss: 0.1216, Val Accuracy: 0.8132\n",
            "Epoch 801/1000 - Train Loss: 0.1239, Train Accuracy: 0.8374, Val Loss: 0.1130, Val Accuracy: 0.8462\n",
            "Epoch 901/1000 - Train Loss: 0.1168, Train Accuracy: 0.8681, Val Loss: 0.1056, Val Accuracy: 0.8791\n",
            "Epoch 1/1000 - Train Loss: 0.2666, Train Accuracy: 0.6286, Val Loss: 0.2623, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.2485, Train Accuracy: 0.6286, Val Loss: 0.2448, Val Accuracy: 0.6484\n",
            "Epoch 201/1000 - Train Loss: 0.2308, Train Accuracy: 0.6286, Val Loss: 0.2271, Val Accuracy: 0.6484\n",
            "Epoch 301/1000 - Train Loss: 0.2141, Train Accuracy: 0.6286, Val Loss: 0.2102, Val Accuracy: 0.6484\n",
            "Epoch 401/1000 - Train Loss: 0.1987, Train Accuracy: 0.6396, Val Loss: 0.1943, Val Accuracy: 0.6593\n",
            "Epoch 501/1000 - Train Loss: 0.1847, Train Accuracy: 0.6527, Val Loss: 0.1797, Val Accuracy: 0.6593\n",
            "Epoch 601/1000 - Train Loss: 0.1721, Train Accuracy: 0.6791, Val Loss: 0.1666, Val Accuracy: 0.6593\n",
            "Epoch 701/1000 - Train Loss: 0.1609, Train Accuracy: 0.7209, Val Loss: 0.1548, Val Accuracy: 0.7143\n",
            "Epoch 801/1000 - Train Loss: 0.1509, Train Accuracy: 0.7407, Val Loss: 0.1445, Val Accuracy: 0.7363\n",
            "Epoch 901/1000 - Train Loss: 0.1421, Train Accuracy: 0.7626, Val Loss: 0.1354, Val Accuracy: 0.7582\n",
            "Epoch 1/1000 - Train Loss: 0.3487, Train Accuracy: 0.3736, Val Loss: 0.3563, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.3162, Train Accuracy: 0.3648, Val Loss: 0.3217, Val Accuracy: 0.3407\n",
            "Epoch 201/1000 - Train Loss: 0.2887, Train Accuracy: 0.3385, Val Loss: 0.2925, Val Accuracy: 0.3077\n",
            "Epoch 301/1000 - Train Loss: 0.2660, Train Accuracy: 0.3055, Val Loss: 0.2682, Val Accuracy: 0.2747\n",
            "Epoch 401/1000 - Train Loss: 0.2477, Train Accuracy: 0.5363, Val Loss: 0.2485, Val Accuracy: 0.5165\n",
            "Epoch 501/1000 - Train Loss: 0.2329, Train Accuracy: 0.6857, Val Loss: 0.2326, Val Accuracy: 0.6593\n",
            "Epoch 601/1000 - Train Loss: 0.2210, Train Accuracy: 0.6813, Val Loss: 0.2196, Val Accuracy: 0.6923\n",
            "Epoch 701/1000 - Train Loss: 0.2112, Train Accuracy: 0.6703, Val Loss: 0.2090, Val Accuracy: 0.6923\n",
            "Epoch 801/1000 - Train Loss: 0.2032, Train Accuracy: 0.6659, Val Loss: 0.2002, Val Accuracy: 0.6923\n",
            "Epoch 901/1000 - Train Loss: 0.1964, Train Accuracy: 0.6615, Val Loss: 0.1929, Val Accuracy: 0.6923\n",
            "Epoch 1/1000 - Train Loss: 0.4018, Train Accuracy: 0.3670, Val Loss: 0.4049, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.3663, Train Accuracy: 0.3626, Val Loss: 0.3672, Val Accuracy: 0.3297\n",
            "Epoch 201/1000 - Train Loss: 0.3332, Train Accuracy: 0.3648, Val Loss: 0.3325, Val Accuracy: 0.3297\n",
            "Epoch 301/1000 - Train Loss: 0.3035, Train Accuracy: 0.4022, Val Loss: 0.3015, Val Accuracy: 0.3736\n",
            "Epoch 401/1000 - Train Loss: 0.2774, Train Accuracy: 0.4703, Val Loss: 0.2745, Val Accuracy: 0.4725\n",
            "Epoch 501/1000 - Train Loss: 0.2548, Train Accuracy: 0.5451, Val Loss: 0.2513, Val Accuracy: 0.5714\n",
            "Epoch 601/1000 - Train Loss: 0.2354, Train Accuracy: 0.6022, Val Loss: 0.2317, Val Accuracy: 0.6264\n",
            "Epoch 701/1000 - Train Loss: 0.2189, Train Accuracy: 0.6593, Val Loss: 0.2150, Val Accuracy: 0.7143\n",
            "Epoch 801/1000 - Train Loss: 0.2048, Train Accuracy: 0.7231, Val Loss: 0.2009, Val Accuracy: 0.7363\n",
            "Epoch 901/1000 - Train Loss: 0.1928, Train Accuracy: 0.7473, Val Loss: 0.1889, Val Accuracy: 0.7363\n",
            "Epoch 1/1000 - Train Loss: 0.2351, Train Accuracy: 0.6286, Val Loss: 0.2126, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.2216, Train Accuracy: 0.6286, Val Loss: 0.1992, Val Accuracy: 0.6484\n",
            "Epoch 201/1000 - Train Loss: 0.2088, Train Accuracy: 0.6308, Val Loss: 0.1867, Val Accuracy: 0.6593\n",
            "Epoch 301/1000 - Train Loss: 0.1968, Train Accuracy: 0.6308, Val Loss: 0.1751, Val Accuracy: 0.6593\n",
            "Epoch 401/1000 - Train Loss: 0.1857, Train Accuracy: 0.6352, Val Loss: 0.1645, Val Accuracy: 0.6703\n",
            "Epoch 501/1000 - Train Loss: 0.1756, Train Accuracy: 0.6549, Val Loss: 0.1548, Val Accuracy: 0.6923\n",
            "Epoch 601/1000 - Train Loss: 0.1664, Train Accuracy: 0.6637, Val Loss: 0.1461, Val Accuracy: 0.7253\n",
            "Epoch 701/1000 - Train Loss: 0.1582, Train Accuracy: 0.6857, Val Loss: 0.1383, Val Accuracy: 0.7363\n",
            "Epoch 801/1000 - Train Loss: 0.1507, Train Accuracy: 0.7143, Val Loss: 0.1313, Val Accuracy: 0.7582\n",
            "Epoch 901/1000 - Train Loss: 0.1440, Train Accuracy: 0.7385, Val Loss: 0.1249, Val Accuracy: 0.7582\n",
            "Epoch 1/1000 - Train Loss: 0.3221, Train Accuracy: 0.1209, Val Loss: 0.3258, Val Accuracy: 0.1209\n",
            "Epoch 101/1000 - Train Loss: 0.3106, Train Accuracy: 0.1297, Val Loss: 0.3136, Val Accuracy: 0.1538\n",
            "Epoch 201/1000 - Train Loss: 0.3002, Train Accuracy: 0.1473, Val Loss: 0.3024, Val Accuracy: 0.1648\n",
            "Epoch 301/1000 - Train Loss: 0.2907, Train Accuracy: 0.1692, Val Loss: 0.2921, Val Accuracy: 0.1758\n",
            "Epoch 401/1000 - Train Loss: 0.2820, Train Accuracy: 0.2264, Val Loss: 0.2828, Val Accuracy: 0.2857\n",
            "Epoch 501/1000 - Train Loss: 0.2741, Train Accuracy: 0.2769, Val Loss: 0.2742, Val Accuracy: 0.3407\n",
            "Epoch 601/1000 - Train Loss: 0.2667, Train Accuracy: 0.3385, Val Loss: 0.2662, Val Accuracy: 0.3846\n",
            "Epoch 701/1000 - Train Loss: 0.2599, Train Accuracy: 0.4220, Val Loss: 0.2589, Val Accuracy: 0.4945\n",
            "Epoch 801/1000 - Train Loss: 0.2535, Train Accuracy: 0.4747, Val Loss: 0.2520, Val Accuracy: 0.5604\n",
            "Epoch 901/1000 - Train Loss: 0.2476, Train Accuracy: 0.5363, Val Loss: 0.2455, Val Accuracy: 0.5824\n",
            "Epoch 1/1000 - Train Loss: 0.2959, Train Accuracy: 0.3868, Val Loss: 0.3082, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.2654, Train Accuracy: 0.4396, Val Loss: 0.2753, Val Accuracy: 0.3846\n",
            "Epoch 201/1000 - Train Loss: 0.2392, Train Accuracy: 0.5143, Val Loss: 0.2468, Val Accuracy: 0.4725\n",
            "Epoch 301/1000 - Train Loss: 0.2170, Train Accuracy: 0.6527, Val Loss: 0.2225, Val Accuracy: 0.6154\n",
            "Epoch 401/1000 - Train Loss: 0.1983, Train Accuracy: 0.7670, Val Loss: 0.2018, Val Accuracy: 0.7033\n",
            "Epoch 501/1000 - Train Loss: 0.1826, Train Accuracy: 0.8505, Val Loss: 0.1844, Val Accuracy: 0.7912\n",
            "Epoch 601/1000 - Train Loss: 0.1693, Train Accuracy: 0.9011, Val Loss: 0.1697, Val Accuracy: 0.8681\n",
            "Epoch 701/1000 - Train Loss: 0.1581, Train Accuracy: 0.9231, Val Loss: 0.1572, Val Accuracy: 0.9231\n",
            "Epoch 801/1000 - Train Loss: 0.1486, Train Accuracy: 0.9341, Val Loss: 0.1465, Val Accuracy: 0.9341\n",
            "Epoch 901/1000 - Train Loss: 0.1404, Train Accuracy: 0.9341, Val Loss: 0.1374, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.2798, Train Accuracy: 0.4352, Val Loss: 0.2898, Val Accuracy: 0.3736\n",
            "Epoch 101/1000 - Train Loss: 0.2548, Train Accuracy: 0.5407, Val Loss: 0.2636, Val Accuracy: 0.4945\n",
            "Epoch 201/1000 - Train Loss: 0.2328, Train Accuracy: 0.6484, Val Loss: 0.2404, Val Accuracy: 0.6154\n",
            "Epoch 301/1000 - Train Loss: 0.2136, Train Accuracy: 0.7451, Val Loss: 0.2200, Val Accuracy: 0.7363\n",
            "Epoch 401/1000 - Train Loss: 0.1971, Train Accuracy: 0.8088, Val Loss: 0.2022, Val Accuracy: 0.7802\n",
            "Epoch 501/1000 - Train Loss: 0.1829, Train Accuracy: 0.8396, Val Loss: 0.1868, Val Accuracy: 0.8022\n",
            "Epoch 601/1000 - Train Loss: 0.1709, Train Accuracy: 0.8769, Val Loss: 0.1735, Val Accuracy: 0.8571\n",
            "Epoch 701/1000 - Train Loss: 0.1606, Train Accuracy: 0.8879, Val Loss: 0.1621, Val Accuracy: 0.8901\n",
            "Epoch 801/1000 - Train Loss: 0.1518, Train Accuracy: 0.8945, Val Loss: 0.1523, Val Accuracy: 0.8791\n",
            "Epoch 901/1000 - Train Loss: 0.1441, Train Accuracy: 0.8967, Val Loss: 0.1438, Val Accuracy: 0.9011\n",
            "Epoch 1/1000 - Train Loss: 0.1712, Train Accuracy: 0.7714, Val Loss: 0.1495, Val Accuracy: 0.8352\n",
            "Epoch 101/1000 - Train Loss: 0.1107, Train Accuracy: 0.8791, Val Loss: 0.0959, Val Accuracy: 0.8681\n",
            "Epoch 201/1000 - Train Loss: 0.0866, Train Accuracy: 0.9055, Val Loss: 0.0733, Val Accuracy: 0.9011\n",
            "Epoch 301/1000 - Train Loss: 0.0729, Train Accuracy: 0.9209, Val Loss: 0.0597, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.0636, Train Accuracy: 0.9319, Val Loss: 0.0505, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0566, Train Accuracy: 0.9319, Val Loss: 0.0439, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0512, Train Accuracy: 0.9385, Val Loss: 0.0391, Val Accuracy: 0.9560\n",
            "Epoch 701/1000 - Train Loss: 0.0471, Train Accuracy: 0.9385, Val Loss: 0.0356, Val Accuracy: 0.9560\n",
            "Epoch 801/1000 - Train Loss: 0.0438, Train Accuracy: 0.9429, Val Loss: 0.0329, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0411, Train Accuracy: 0.9495, Val Loss: 0.0306, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.2694, Train Accuracy: 0.6286, Val Loss: 0.2538, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.1597, Train Accuracy: 0.6418, Val Loss: 0.1477, Val Accuracy: 0.6703\n",
            "Epoch 201/1000 - Train Loss: 0.1130, Train Accuracy: 0.8747, Val Loss: 0.1030, Val Accuracy: 0.9011\n",
            "Epoch 301/1000 - Train Loss: 0.0910, Train Accuracy: 0.9143, Val Loss: 0.0816, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.0773, Train Accuracy: 0.9385, Val Loss: 0.0679, Val Accuracy: 0.9341\n",
            "Epoch 501/1000 - Train Loss: 0.0673, Train Accuracy: 0.9495, Val Loss: 0.0578, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0596, Train Accuracy: 0.9582, Val Loss: 0.0500, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0535, Train Accuracy: 0.9670, Val Loss: 0.0438, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0486, Train Accuracy: 0.9714, Val Loss: 0.0388, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0446, Train Accuracy: 0.9714, Val Loss: 0.0348, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.1922, Train Accuracy: 0.6703, Val Loss: 0.1818, Val Accuracy: 0.7253\n",
            "Epoch 101/1000 - Train Loss: 0.1214, Train Accuracy: 0.8703, Val Loss: 0.1162, Val Accuracy: 0.9011\n",
            "Epoch 201/1000 - Train Loss: 0.0942, Train Accuracy: 0.9165, Val Loss: 0.0886, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0782, Train Accuracy: 0.9319, Val Loss: 0.0718, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0669, Train Accuracy: 0.9363, Val Loss: 0.0600, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0584, Train Accuracy: 0.9451, Val Loss: 0.0512, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0518, Train Accuracy: 0.9495, Val Loss: 0.0446, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0465, Train Accuracy: 0.9538, Val Loss: 0.0394, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0422, Train Accuracy: 0.9582, Val Loss: 0.0353, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0386, Train Accuracy: 0.9692, Val Loss: 0.0319, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2152, Train Accuracy: 0.6462, Val Loss: 0.2046, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.1586, Train Accuracy: 0.7934, Val Loss: 0.1483, Val Accuracy: 0.8352\n",
            "Epoch 201/1000 - Train Loss: 0.1242, Train Accuracy: 0.8879, Val Loss: 0.1136, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.1005, Train Accuracy: 0.9187, Val Loss: 0.0896, Val Accuracy: 0.9341\n",
            "Epoch 401/1000 - Train Loss: 0.0831, Train Accuracy: 0.9341, Val Loss: 0.0719, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0704, Train Accuracy: 0.9473, Val Loss: 0.0592, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0609, Train Accuracy: 0.9516, Val Loss: 0.0500, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0537, Train Accuracy: 0.9604, Val Loss: 0.0431, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0482, Train Accuracy: 0.9670, Val Loss: 0.0379, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0439, Train Accuracy: 0.9670, Val Loss: 0.0339, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2655, Train Accuracy: 0.3846, Val Loss: 0.2733, Val Accuracy: 0.3626\n",
            "Epoch 101/1000 - Train Loss: 0.1606, Train Accuracy: 0.8549, Val Loss: 0.1577, Val Accuracy: 0.8352\n",
            "Epoch 201/1000 - Train Loss: 0.1327, Train Accuracy: 0.8791, Val Loss: 0.1261, Val Accuracy: 0.9121\n",
            "Epoch 301/1000 - Train Loss: 0.1132, Train Accuracy: 0.9033, Val Loss: 0.1042, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0974, Train Accuracy: 0.9165, Val Loss: 0.0867, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0848, Train Accuracy: 0.9209, Val Loss: 0.0730, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0749, Train Accuracy: 0.9297, Val Loss: 0.0623, Val Accuracy: 0.9560\n",
            "Epoch 701/1000 - Train Loss: 0.0670, Train Accuracy: 0.9341, Val Loss: 0.0541, Val Accuracy: 0.9560\n",
            "Epoch 801/1000 - Train Loss: 0.0606, Train Accuracy: 0.9363, Val Loss: 0.0477, Val Accuracy: 0.9560\n",
            "Epoch 901/1000 - Train Loss: 0.0554, Train Accuracy: 0.9363, Val Loss: 0.0425, Val Accuracy: 0.9560\n",
            "Epoch 1/1000 - Train Loss: 0.3817, Train Accuracy: 0.2615, Val Loss: 0.3972, Val Accuracy: 0.2418\n",
            "Epoch 101/1000 - Train Loss: 0.1436, Train Accuracy: 0.8681, Val Loss: 0.1431, Val Accuracy: 0.8901\n",
            "Epoch 201/1000 - Train Loss: 0.0907, Train Accuracy: 0.9275, Val Loss: 0.0881, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0708, Train Accuracy: 0.9473, Val Loss: 0.0672, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0598, Train Accuracy: 0.9495, Val Loss: 0.0554, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0525, Train Accuracy: 0.9516, Val Loss: 0.0479, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0473, Train Accuracy: 0.9538, Val Loss: 0.0427, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0433, Train Accuracy: 0.9560, Val Loss: 0.0388, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0402, Train Accuracy: 0.9582, Val Loss: 0.0358, Val Accuracy: 0.9560\n",
            "Epoch 901/1000 - Train Loss: 0.0376, Train Accuracy: 0.9582, Val Loss: 0.0333, Val Accuracy: 0.9560\n",
            "Epoch 1/1000 - Train Loss: 0.3673, Train Accuracy: 0.3692, Val Loss: 0.3759, Val Accuracy: 0.3626\n",
            "Epoch 101/1000 - Train Loss: 0.1289, Train Accuracy: 0.8835, Val Loss: 0.1236, Val Accuracy: 0.8901\n",
            "Epoch 201/1000 - Train Loss: 0.0882, Train Accuracy: 0.9297, Val Loss: 0.0799, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0723, Train Accuracy: 0.9363, Val Loss: 0.0631, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0625, Train Accuracy: 0.9429, Val Loss: 0.0531, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0556, Train Accuracy: 0.9473, Val Loss: 0.0463, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0502, Train Accuracy: 0.9516, Val Loss: 0.0412, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0460, Train Accuracy: 0.9560, Val Loss: 0.0372, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0426, Train Accuracy: 0.9560, Val Loss: 0.0341, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0397, Train Accuracy: 0.9604, Val Loss: 0.0314, Val Accuracy: 0.9560\n",
            "Epoch 1/1000 - Train Loss: 0.3592, Train Accuracy: 0.6286, Val Loss: 0.3463, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.2252, Train Accuracy: 0.6330, Val Loss: 0.2146, Val Accuracy: 0.6484\n",
            "Epoch 201/1000 - Train Loss: 0.1158, Train Accuracy: 0.8747, Val Loss: 0.1113, Val Accuracy: 0.9011\n",
            "Epoch 301/1000 - Train Loss: 0.0829, Train Accuracy: 0.9341, Val Loss: 0.0748, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0680, Train Accuracy: 0.9451, Val Loss: 0.0585, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0587, Train Accuracy: 0.9495, Val Loss: 0.0485, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0522, Train Accuracy: 0.9495, Val Loss: 0.0415, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0474, Train Accuracy: 0.9538, Val Loss: 0.0362, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0436, Train Accuracy: 0.9626, Val Loss: 0.0322, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0405, Train Accuracy: 0.9670, Val Loss: 0.0290, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2653, Train Accuracy: 0.4484, Val Loss: 0.2587, Val Accuracy: 0.4505\n",
            "Epoch 101/1000 - Train Loss: 0.1210, Train Accuracy: 0.9077, Val Loss: 0.1108, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0856, Train Accuracy: 0.9385, Val Loss: 0.0758, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0694, Train Accuracy: 0.9473, Val Loss: 0.0596, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0591, Train Accuracy: 0.9560, Val Loss: 0.0490, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0518, Train Accuracy: 0.9582, Val Loss: 0.0413, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0464, Train Accuracy: 0.9582, Val Loss: 0.0355, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0422, Train Accuracy: 0.9604, Val Loss: 0.0309, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0389, Train Accuracy: 0.9626, Val Loss: 0.0274, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0362, Train Accuracy: 0.9648, Val Loss: 0.0247, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.4517, Train Accuracy: 0.3714, Val Loss: 0.4606, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.1757, Train Accuracy: 0.8176, Val Loss: 0.1699, Val Accuracy: 0.8462\n",
            "Epoch 201/1000 - Train Loss: 0.1201, Train Accuracy: 0.9165, Val Loss: 0.1112, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0982, Train Accuracy: 0.9319, Val Loss: 0.0885, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0837, Train Accuracy: 0.9341, Val Loss: 0.0736, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0728, Train Accuracy: 0.9341, Val Loss: 0.0624, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0644, Train Accuracy: 0.9407, Val Loss: 0.0538, Val Accuracy: 0.9560\n",
            "Epoch 701/1000 - Train Loss: 0.0577, Train Accuracy: 0.9451, Val Loss: 0.0472, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0523, Train Accuracy: 0.9495, Val Loss: 0.0419, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0480, Train Accuracy: 0.9538, Val Loss: 0.0377, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2462, Train Accuracy: 0.6286, Val Loss: 0.2420, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.0703, Train Accuracy: 0.9319, Val Loss: 0.0611, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0441, Train Accuracy: 0.9516, Val Loss: 0.0342, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0331, Train Accuracy: 0.9714, Val Loss: 0.0238, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0272, Train Accuracy: 0.9780, Val Loss: 0.0184, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0238, Train Accuracy: 0.9758, Val Loss: 0.0154, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0216, Train Accuracy: 0.9802, Val Loss: 0.0135, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0201, Train Accuracy: 0.9780, Val Loss: 0.0122, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0191, Train Accuracy: 0.9802, Val Loss: 0.0113, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0182, Train Accuracy: 0.9824, Val Loss: 0.0107, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.1763, Train Accuracy: 0.7604, Val Loss: 0.1670, Val Accuracy: 0.8132\n",
            "Epoch 101/1000 - Train Loss: 0.0503, Train Accuracy: 0.9363, Val Loss: 0.0427, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0358, Train Accuracy: 0.9560, Val Loss: 0.0286, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0287, Train Accuracy: 0.9692, Val Loss: 0.0211, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0248, Train Accuracy: 0.9758, Val Loss: 0.0169, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0223, Train Accuracy: 0.9780, Val Loss: 0.0143, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0206, Train Accuracy: 0.9802, Val Loss: 0.0127, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0194, Train Accuracy: 0.9802, Val Loss: 0.0116, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0184, Train Accuracy: 0.9802, Val Loss: 0.0108, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0176, Train Accuracy: 0.9824, Val Loss: 0.0102, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2197, Train Accuracy: 0.6286, Val Loss: 0.2069, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.0945, Train Accuracy: 0.9231, Val Loss: 0.0846, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0576, Train Accuracy: 0.9429, Val Loss: 0.0483, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0407, Train Accuracy: 0.9626, Val Loss: 0.0321, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0322, Train Accuracy: 0.9692, Val Loss: 0.0241, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0274, Train Accuracy: 0.9714, Val Loss: 0.0196, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0243, Train Accuracy: 0.9758, Val Loss: 0.0167, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0222, Train Accuracy: 0.9758, Val Loss: 0.0148, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0206, Train Accuracy: 0.9780, Val Loss: 0.0134, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0194, Train Accuracy: 0.9846, Val Loss: 0.0123, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2834, Train Accuracy: 0.6286, Val Loss: 0.2677, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.0617, Train Accuracy: 0.9516, Val Loss: 0.0521, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0371, Train Accuracy: 0.9648, Val Loss: 0.0277, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0282, Train Accuracy: 0.9758, Val Loss: 0.0197, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0237, Train Accuracy: 0.9780, Val Loss: 0.0160, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0211, Train Accuracy: 0.9824, Val Loss: 0.0139, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0194, Train Accuracy: 0.9824, Val Loss: 0.0126, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0182, Train Accuracy: 0.9824, Val Loss: 0.0116, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0173, Train Accuracy: 0.9846, Val Loss: 0.0109, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0166, Train Accuracy: 0.9846, Val Loss: 0.0104, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2100, Train Accuracy: 0.6747, Val Loss: 0.2074, Val Accuracy: 0.7143\n",
            "Epoch 101/1000 - Train Loss: 0.0809, Train Accuracy: 0.9143, Val Loss: 0.0699, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0480, Train Accuracy: 0.9473, Val Loss: 0.0388, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0350, Train Accuracy: 0.9604, Val Loss: 0.0282, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0282, Train Accuracy: 0.9692, Val Loss: 0.0225, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0244, Train Accuracy: 0.9714, Val Loss: 0.0190, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0221, Train Accuracy: 0.9758, Val Loss: 0.0167, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0205, Train Accuracy: 0.9780, Val Loss: 0.0151, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0194, Train Accuracy: 0.9780, Val Loss: 0.0140, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0185, Train Accuracy: 0.9824, Val Loss: 0.0131, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2540, Train Accuracy: 0.6110, Val Loss: 0.2407, Val Accuracy: 0.6264\n",
            "Epoch 101/1000 - Train Loss: 0.0825, Train Accuracy: 0.9055, Val Loss: 0.0750, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.0457, Train Accuracy: 0.9451, Val Loss: 0.0382, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0326, Train Accuracy: 0.9648, Val Loss: 0.0249, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0262, Train Accuracy: 0.9780, Val Loss: 0.0188, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0225, Train Accuracy: 0.9824, Val Loss: 0.0154, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0201, Train Accuracy: 0.9868, Val Loss: 0.0132, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0184, Train Accuracy: 0.9868, Val Loss: 0.0116, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0172, Train Accuracy: 0.9868, Val Loss: 0.0105, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0163, Train Accuracy: 0.9868, Val Loss: 0.0096, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.4596, Train Accuracy: 0.2374, Val Loss: 0.4709, Val Accuracy: 0.2527\n",
            "Epoch 101/1000 - Train Loss: 0.0810, Train Accuracy: 0.9231, Val Loss: 0.0703, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0449, Train Accuracy: 0.9495, Val Loss: 0.0355, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0323, Train Accuracy: 0.9604, Val Loss: 0.0239, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0267, Train Accuracy: 0.9714, Val Loss: 0.0187, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0235, Train Accuracy: 0.9736, Val Loss: 0.0157, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0215, Train Accuracy: 0.9758, Val Loss: 0.0139, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0201, Train Accuracy: 0.9802, Val Loss: 0.0127, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0190, Train Accuracy: 0.9780, Val Loss: 0.0118, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0182, Train Accuracy: 0.9780, Val Loss: 0.0111, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.1786, Train Accuracy: 0.8154, Val Loss: 0.1798, Val Accuracy: 0.7802\n",
            "Epoch 101/1000 - Train Loss: 0.0495, Train Accuracy: 0.9560, Val Loss: 0.0430, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0356, Train Accuracy: 0.9626, Val Loss: 0.0297, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0295, Train Accuracy: 0.9670, Val Loss: 0.0242, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0259, Train Accuracy: 0.9692, Val Loss: 0.0207, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0236, Train Accuracy: 0.9736, Val Loss: 0.0182, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0220, Train Accuracy: 0.9780, Val Loss: 0.0164, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0207, Train Accuracy: 0.9802, Val Loss: 0.0149, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0197, Train Accuracy: 0.9824, Val Loss: 0.0138, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0189, Train Accuracy: 0.9824, Val Loss: 0.0129, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3013, Train Accuracy: 0.3934, Val Loss: 0.3151, Val Accuracy: 0.3846\n",
            "Epoch 101/1000 - Train Loss: 0.0505, Train Accuracy: 0.9538, Val Loss: 0.0408, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0350, Train Accuracy: 0.9670, Val Loss: 0.0267, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0281, Train Accuracy: 0.9714, Val Loss: 0.0212, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0244, Train Accuracy: 0.9714, Val Loss: 0.0181, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0221, Train Accuracy: 0.9758, Val Loss: 0.0162, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0205, Train Accuracy: 0.9802, Val Loss: 0.0148, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0193, Train Accuracy: 0.9780, Val Loss: 0.0138, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0184, Train Accuracy: 0.9780, Val Loss: 0.0130, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0176, Train Accuracy: 0.9780, Val Loss: 0.0123, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.1704, Train Accuracy: 0.7451, Val Loss: 0.1646, Val Accuracy: 0.8022\n",
            "Epoch 101/1000 - Train Loss: 0.0440, Train Accuracy: 0.9626, Val Loss: 0.0320, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0316, Train Accuracy: 0.9714, Val Loss: 0.0213, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0258, Train Accuracy: 0.9758, Val Loss: 0.0166, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0225, Train Accuracy: 0.9758, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0204, Train Accuracy: 0.9802, Val Loss: 0.0127, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0190, Train Accuracy: 0.9802, Val Loss: 0.0117, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0180, Train Accuracy: 0.9780, Val Loss: 0.0110, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0172, Train Accuracy: 0.9780, Val Loss: 0.0104, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0166, Train Accuracy: 0.9780, Val Loss: 0.0100, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.4799, Train Accuracy: 0.3714, Val Loss: 0.4954, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.0495, Train Accuracy: 0.9429, Val Loss: 0.0431, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0290, Train Accuracy: 0.9648, Val Loss: 0.0239, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0228, Train Accuracy: 0.9736, Val Loss: 0.0177, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0198, Train Accuracy: 0.9736, Val Loss: 0.0146, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0179, Train Accuracy: 0.9758, Val Loss: 0.0127, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0167, Train Accuracy: 0.9758, Val Loss: 0.0114, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0157, Train Accuracy: 0.9802, Val Loss: 0.0104, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0149, Train Accuracy: 0.9824, Val Loss: 0.0097, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0142, Train Accuracy: 0.9846, Val Loss: 0.0091, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2533, Train Accuracy: 0.5670, Val Loss: 0.2579, Val Accuracy: 0.5495\n",
            "Epoch 101/1000 - Train Loss: 0.0707, Train Accuracy: 0.9231, Val Loss: 0.0555, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0364, Train Accuracy: 0.9670, Val Loss: 0.0266, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0244, Train Accuracy: 0.9736, Val Loss: 0.0176, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0202, Train Accuracy: 0.9802, Val Loss: 0.0134, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0181, Train Accuracy: 0.9824, Val Loss: 0.0112, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0167, Train Accuracy: 0.9824, Val Loss: 0.0100, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0157, Train Accuracy: 0.9824, Val Loss: 0.0092, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0149, Train Accuracy: 0.9846, Val Loss: 0.0087, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0142, Train Accuracy: 0.9846, Val Loss: 0.0083, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3582, Train Accuracy: 0.3385, Val Loss: 0.3652, Val Accuracy: 0.2857\n",
            "Epoch 101/1000 - Train Loss: 0.0471, Train Accuracy: 0.9516, Val Loss: 0.0373, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0269, Train Accuracy: 0.9714, Val Loss: 0.0170, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0210, Train Accuracy: 0.9802, Val Loss: 0.0128, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0183, Train Accuracy: 0.9802, Val Loss: 0.0111, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0168, Train Accuracy: 0.9824, Val Loss: 0.0102, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0157, Train Accuracy: 0.9846, Val Loss: 0.0095, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0149, Train Accuracy: 0.9846, Val Loss: 0.0090, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0142, Train Accuracy: 0.9846, Val Loss: 0.0086, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0082, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.3133, Train Accuracy: 0.6286, Val Loss: 0.2916, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.0332, Train Accuracy: 0.9670, Val Loss: 0.0262, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0231, Train Accuracy: 0.9824, Val Loss: 0.0171, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0194, Train Accuracy: 0.9802, Val Loss: 0.0138, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0174, Train Accuracy: 0.9802, Val Loss: 0.0121, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0161, Train Accuracy: 0.9846, Val Loss: 0.0110, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0152, Train Accuracy: 0.9846, Val Loss: 0.0103, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0144, Train Accuracy: 0.9846, Val Loss: 0.0098, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0138, Train Accuracy: 0.9846, Val Loss: 0.0093, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0133, Train Accuracy: 0.9846, Val Loss: 0.0089, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2945, Train Accuracy: 0.6286, Val Loss: 0.2848, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.0438, Train Accuracy: 0.9604, Val Loss: 0.0337, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0277, Train Accuracy: 0.9714, Val Loss: 0.0189, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0217, Train Accuracy: 0.9780, Val Loss: 0.0141, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0188, Train Accuracy: 0.9780, Val Loss: 0.0120, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0170, Train Accuracy: 0.9824, Val Loss: 0.0107, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0158, Train Accuracy: 0.9846, Val Loss: 0.0098, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0149, Train Accuracy: 0.9846, Val Loss: 0.0091, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0142, Train Accuracy: 0.9846, Val Loss: 0.0086, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0082, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.4773, Train Accuracy: 0.3714, Val Loss: 0.4937, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.0459, Train Accuracy: 0.9516, Val Loss: 0.0391, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0272, Train Accuracy: 0.9736, Val Loss: 0.0222, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0210, Train Accuracy: 0.9824, Val Loss: 0.0160, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0181, Train Accuracy: 0.9846, Val Loss: 0.0130, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0165, Train Accuracy: 0.9868, Val Loss: 0.0112, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0153, Train Accuracy: 0.9868, Val Loss: 0.0101, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0145, Train Accuracy: 0.9868, Val Loss: 0.0094, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0139, Train Accuracy: 0.9868, Val Loss: 0.0089, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0133, Train Accuracy: 0.9868, Val Loss: 0.0085, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2818, Train Accuracy: 0.6286, Val Loss: 0.2657, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.0479, Train Accuracy: 0.9473, Val Loss: 0.0388, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0267, Train Accuracy: 0.9780, Val Loss: 0.0195, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0206, Train Accuracy: 0.9802, Val Loss: 0.0139, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0178, Train Accuracy: 0.9824, Val Loss: 0.0115, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0162, Train Accuracy: 0.9824, Val Loss: 0.0102, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0151, Train Accuracy: 0.9824, Val Loss: 0.0093, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0142, Train Accuracy: 0.9824, Val Loss: 0.0087, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0083, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0130, Train Accuracy: 0.9846, Val Loss: 0.0079, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2301, Train Accuracy: 0.6286, Val Loss: 0.2147, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.0398, Train Accuracy: 0.9582, Val Loss: 0.0284, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0256, Train Accuracy: 0.9758, Val Loss: 0.0163, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0211, Train Accuracy: 0.9780, Val Loss: 0.0125, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0188, Train Accuracy: 0.9802, Val Loss: 0.0109, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0173, Train Accuracy: 0.9824, Val Loss: 0.0099, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0163, Train Accuracy: 0.9824, Val Loss: 0.0093, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0154, Train Accuracy: 0.9824, Val Loss: 0.0088, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0083, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0140, Train Accuracy: 0.9824, Val Loss: 0.0080, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.4924, Train Accuracy: 0.3714, Val Loss: 0.5060, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.0417, Train Accuracy: 0.9626, Val Loss: 0.0342, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0261, Train Accuracy: 0.9714, Val Loss: 0.0202, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0213, Train Accuracy: 0.9802, Val Loss: 0.0153, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0190, Train Accuracy: 0.9802, Val Loss: 0.0128, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0175, Train Accuracy: 0.9802, Val Loss: 0.0112, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0165, Train Accuracy: 0.9824, Val Loss: 0.0103, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0156, Train Accuracy: 0.9824, Val Loss: 0.0096, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0150, Train Accuracy: 0.9824, Val Loss: 0.0091, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0144, Train Accuracy: 0.9846, Val Loss: 0.0087, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.4726, Train Accuracy: 0.3714, Val Loss: 0.4817, Val Accuracy: 0.3407\n",
            "Epoch 101/1000 - Train Loss: 0.0400, Train Accuracy: 0.9473, Val Loss: 0.0267, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0260, Train Accuracy: 0.9736, Val Loss: 0.0153, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0211, Train Accuracy: 0.9780, Val Loss: 0.0126, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0186, Train Accuracy: 0.9802, Val Loss: 0.0114, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0171, Train Accuracy: 0.9802, Val Loss: 0.0107, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0160, Train Accuracy: 0.9824, Val Loss: 0.0102, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0151, Train Accuracy: 0.9824, Val Loss: 0.0098, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0144, Train Accuracy: 0.9824, Val Loss: 0.0094, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0139, Train Accuracy: 0.9846, Val Loss: 0.0090, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.3319, Train Accuracy: 0.5846, Val Loss: 0.3092, Val Accuracy: 0.5934\n",
            "Epoch 101/1000 - Train Loss: 0.0231, Train Accuracy: 0.9780, Val Loss: 0.0168, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0168, Train Accuracy: 0.9824, Val Loss: 0.0105, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0147, Train Accuracy: 0.9868, Val Loss: 0.0087, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0135, Train Accuracy: 0.9868, Val Loss: 0.0079, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0126, Train Accuracy: 0.9846, Val Loss: 0.0074, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0120, Train Accuracy: 0.9868, Val Loss: 0.0071, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0115, Train Accuracy: 0.9868, Val Loss: 0.0069, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0111, Train Accuracy: 0.9868, Val Loss: 0.0068, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0108, Train Accuracy: 0.9868, Val Loss: 0.0068, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2264, Train Accuracy: 0.6857, Val Loss: 0.2204, Val Accuracy: 0.7253\n",
            "Epoch 101/1000 - Train Loss: 0.0286, Train Accuracy: 0.9714, Val Loss: 0.0224, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0194, Train Accuracy: 0.9802, Val Loss: 0.0134, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0163, Train Accuracy: 0.9824, Val Loss: 0.0110, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0146, Train Accuracy: 0.9846, Val Loss: 0.0097, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0135, Train Accuracy: 0.9846, Val Loss: 0.0088, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0126, Train Accuracy: 0.9846, Val Loss: 0.0082, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0078, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0114, Train Accuracy: 0.9846, Val Loss: 0.0075, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0110, Train Accuracy: 0.9868, Val Loss: 0.0074, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2506, Train Accuracy: 0.5824, Val Loss: 0.2252, Val Accuracy: 0.7033\n",
            "Epoch 101/1000 - Train Loss: 0.0265, Train Accuracy: 0.9758, Val Loss: 0.0180, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0185, Train Accuracy: 0.9802, Val Loss: 0.0111, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0161, Train Accuracy: 0.9824, Val Loss: 0.0093, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0147, Train Accuracy: 0.9824, Val Loss: 0.0084, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0137, Train Accuracy: 0.9846, Val Loss: 0.0078, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0129, Train Accuracy: 0.9846, Val Loss: 0.0074, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0123, Train Accuracy: 0.9846, Val Loss: 0.0071, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0069, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0067, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2625, Train Accuracy: 0.6308, Val Loss: 0.2530, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.0253, Train Accuracy: 0.9780, Val Loss: 0.0188, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0171, Train Accuracy: 0.9868, Val Loss: 0.0106, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0147, Train Accuracy: 0.9868, Val Loss: 0.0083, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0134, Train Accuracy: 0.9868, Val Loss: 0.0074, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0125, Train Accuracy: 0.9868, Val Loss: 0.0068, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0119, Train Accuracy: 0.9868, Val Loss: 0.0066, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0113, Train Accuracy: 0.9868, Val Loss: 0.0065, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0109, Train Accuracy: 0.9868, Val Loss: 0.0064, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0106, Train Accuracy: 0.9868, Val Loss: 0.0064, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.4592, Train Accuracy: 0.2044, Val Loss: 0.4616, Val Accuracy: 0.2088\n",
            "Epoch 101/1000 - Train Loss: 0.0325, Train Accuracy: 0.9626, Val Loss: 0.0259, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0210, Train Accuracy: 0.9736, Val Loss: 0.0150, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0176, Train Accuracy: 0.9780, Val Loss: 0.0119, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0158, Train Accuracy: 0.9824, Val Loss: 0.0103, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0145, Train Accuracy: 0.9846, Val Loss: 0.0093, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0086, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0129, Train Accuracy: 0.9846, Val Loss: 0.0082, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0123, Train Accuracy: 0.9846, Val Loss: 0.0079, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0118, Train Accuracy: 0.9846, Val Loss: 0.0076, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.3118, Train Accuracy: 0.6044, Val Loss: 0.2969, Val Accuracy: 0.6264\n",
            "Epoch 101/1000 - Train Loss: 0.0244, Train Accuracy: 0.9714, Val Loss: 0.0182, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0181, Train Accuracy: 0.9824, Val Loss: 0.0123, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0157, Train Accuracy: 0.9824, Val Loss: 0.0104, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0142, Train Accuracy: 0.9824, Val Loss: 0.0092, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0132, Train Accuracy: 0.9846, Val Loss: 0.0085, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0124, Train Accuracy: 0.9846, Val Loss: 0.0079, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0117, Train Accuracy: 0.9846, Val Loss: 0.0075, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0112, Train Accuracy: 0.9846, Val Loss: 0.0072, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0107, Train Accuracy: 0.9868, Val Loss: 0.0069, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.1932, Train Accuracy: 0.7341, Val Loss: 0.1908, Val Accuracy: 0.7363\n",
            "Epoch 101/1000 - Train Loss: 0.0243, Train Accuracy: 0.9824, Val Loss: 0.0177, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0176, Train Accuracy: 0.9846, Val Loss: 0.0115, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0150, Train Accuracy: 0.9846, Val Loss: 0.0093, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0135, Train Accuracy: 0.9846, Val Loss: 0.0083, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0126, Train Accuracy: 0.9868, Val Loss: 0.0077, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0119, Train Accuracy: 0.9868, Val Loss: 0.0072, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0113, Train Accuracy: 0.9868, Val Loss: 0.0070, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0109, Train Accuracy: 0.9868, Val Loss: 0.0068, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0105, Train Accuracy: 0.9868, Val Loss: 0.0066, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2786, Train Accuracy: 0.2176, Val Loss: 0.2798, Val Accuracy: 0.2198\n",
            "Epoch 101/1000 - Train Loss: 0.0278, Train Accuracy: 0.9736, Val Loss: 0.0178, Val Accuracy: 1.0000\n",
            "Epoch 201/1000 - Train Loss: 0.0190, Train Accuracy: 0.9802, Val Loss: 0.0118, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0162, Train Accuracy: 0.9824, Val Loss: 0.0102, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0146, Train Accuracy: 0.9846, Val Loss: 0.0092, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0135, Train Accuracy: 0.9846, Val Loss: 0.0086, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0127, Train Accuracy: 0.9846, Val Loss: 0.0081, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0078, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0115, Train Accuracy: 0.9868, Val Loss: 0.0075, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0111, Train Accuracy: 0.9868, Val Loss: 0.0073, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.3130, Train Accuracy: 0.3341, Val Loss: 0.3128, Val Accuracy: 0.3187\n",
            "Epoch 101/1000 - Train Loss: 0.0293, Train Accuracy: 0.9736, Val Loss: 0.0189, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0197, Train Accuracy: 0.9758, Val Loss: 0.0124, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0167, Train Accuracy: 0.9780, Val Loss: 0.0104, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0150, Train Accuracy: 0.9802, Val Loss: 0.0092, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0138, Train Accuracy: 0.9846, Val Loss: 0.0083, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0129, Train Accuracy: 0.9846, Val Loss: 0.0077, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0122, Train Accuracy: 0.9846, Val Loss: 0.0072, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0116, Train Accuracy: 0.9846, Val Loss: 0.0069, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0111, Train Accuracy: 0.9846, Val Loss: 0.0067, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3026, Train Accuracy: 0.6286, Val Loss: 0.2906, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.0273, Train Accuracy: 0.9714, Val Loss: 0.0215, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0189, Train Accuracy: 0.9780, Val Loss: 0.0128, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0164, Train Accuracy: 0.9824, Val Loss: 0.0104, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0149, Train Accuracy: 0.9824, Val Loss: 0.0093, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0139, Train Accuracy: 0.9846, Val Loss: 0.0086, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0131, Train Accuracy: 0.9846, Val Loss: 0.0082, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0125, Train Accuracy: 0.9846, Val Loss: 0.0078, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0076, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0074, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2529, Train Accuracy: 0.6242, Val Loss: 0.2881, Val Accuracy: 0.5495\n",
            "Epoch 101/1000 - Train Loss: 0.2390, Train Accuracy: 0.6242, Val Loss: 0.2717, Val Accuracy: 0.5495\n",
            "Epoch 201/1000 - Train Loss: 0.2251, Train Accuracy: 0.6264, Val Loss: 0.2554, Val Accuracy: 0.5495\n",
            "Epoch 301/1000 - Train Loss: 0.2118, Train Accuracy: 0.6352, Val Loss: 0.2395, Val Accuracy: 0.5714\n",
            "Epoch 401/1000 - Train Loss: 0.1993, Train Accuracy: 0.6484, Val Loss: 0.2248, Val Accuracy: 0.5824\n",
            "Epoch 501/1000 - Train Loss: 0.1879, Train Accuracy: 0.6593, Val Loss: 0.2116, Val Accuracy: 0.5824\n",
            "Epoch 601/1000 - Train Loss: 0.1777, Train Accuracy: 0.6835, Val Loss: 0.2000, Val Accuracy: 0.6154\n",
            "Epoch 701/1000 - Train Loss: 0.1687, Train Accuracy: 0.7011, Val Loss: 0.1898, Val Accuracy: 0.6264\n",
            "Epoch 801/1000 - Train Loss: 0.1607, Train Accuracy: 0.7231, Val Loss: 0.1809, Val Accuracy: 0.6703\n",
            "Epoch 901/1000 - Train Loss: 0.1535, Train Accuracy: 0.7582, Val Loss: 0.1730, Val Accuracy: 0.7253\n",
            "Epoch 1/1000 - Train Loss: 0.2962, Train Accuracy: 0.6286, Val Loss: 0.3493, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.2808, Train Accuracy: 0.6286, Val Loss: 0.3305, Val Accuracy: 0.5604\n",
            "Epoch 201/1000 - Train Loss: 0.2648, Train Accuracy: 0.6286, Val Loss: 0.3109, Val Accuracy: 0.5604\n",
            "Epoch 301/1000 - Train Loss: 0.2486, Train Accuracy: 0.6286, Val Loss: 0.2910, Val Accuracy: 0.5604\n",
            "Epoch 401/1000 - Train Loss: 0.2326, Train Accuracy: 0.6308, Val Loss: 0.2713, Val Accuracy: 0.5604\n",
            "Epoch 501/1000 - Train Loss: 0.2171, Train Accuracy: 0.6396, Val Loss: 0.2521, Val Accuracy: 0.5714\n",
            "Epoch 601/1000 - Train Loss: 0.2023, Train Accuracy: 0.6549, Val Loss: 0.2338, Val Accuracy: 0.5824\n",
            "Epoch 701/1000 - Train Loss: 0.1885, Train Accuracy: 0.6659, Val Loss: 0.2164, Val Accuracy: 0.6044\n",
            "Epoch 801/1000 - Train Loss: 0.1756, Train Accuracy: 0.6857, Val Loss: 0.2002, Val Accuracy: 0.6264\n",
            "Epoch 901/1000 - Train Loss: 0.1638, Train Accuracy: 0.7099, Val Loss: 0.1853, Val Accuracy: 0.6484\n",
            "Epoch 1/1000 - Train Loss: 0.2646, Train Accuracy: 0.4176, Val Loss: 0.2562, Val Accuracy: 0.4725\n",
            "Epoch 101/1000 - Train Loss: 0.2486, Train Accuracy: 0.5077, Val Loss: 0.2430, Val Accuracy: 0.5385\n",
            "Epoch 201/1000 - Train Loss: 0.2350, Train Accuracy: 0.6088, Val Loss: 0.2318, Val Accuracy: 0.6154\n",
            "Epoch 301/1000 - Train Loss: 0.2234, Train Accuracy: 0.6989, Val Loss: 0.2221, Val Accuracy: 0.7143\n",
            "Epoch 401/1000 - Train Loss: 0.2134, Train Accuracy: 0.7516, Val Loss: 0.2138, Val Accuracy: 0.7582\n",
            "Epoch 501/1000 - Train Loss: 0.2049, Train Accuracy: 0.7912, Val Loss: 0.2066, Val Accuracy: 0.7912\n",
            "Epoch 601/1000 - Train Loss: 0.1974, Train Accuracy: 0.8220, Val Loss: 0.2003, Val Accuracy: 0.8352\n",
            "Epoch 701/1000 - Train Loss: 0.1909, Train Accuracy: 0.8352, Val Loss: 0.1947, Val Accuracy: 0.8462\n",
            "Epoch 801/1000 - Train Loss: 0.1850, Train Accuracy: 0.8440, Val Loss: 0.1897, Val Accuracy: 0.8242\n",
            "Epoch 901/1000 - Train Loss: 0.1797, Train Accuracy: 0.8505, Val Loss: 0.1850, Val Accuracy: 0.8462\n",
            "Epoch 1/1000 - Train Loss: 0.2659, Train Accuracy: 0.3956, Val Loss: 0.2517, Val Accuracy: 0.4725\n",
            "Epoch 101/1000 - Train Loss: 0.2534, Train Accuracy: 0.4242, Val Loss: 0.2420, Val Accuracy: 0.5055\n",
            "Epoch 201/1000 - Train Loss: 0.2427, Train Accuracy: 0.5033, Val Loss: 0.2339, Val Accuracy: 0.5495\n",
            "Epoch 301/1000 - Train Loss: 0.2336, Train Accuracy: 0.6000, Val Loss: 0.2271, Val Accuracy: 0.6374\n",
            "Epoch 401/1000 - Train Loss: 0.2257, Train Accuracy: 0.6879, Val Loss: 0.2211, Val Accuracy: 0.7143\n",
            "Epoch 501/1000 - Train Loss: 0.2188, Train Accuracy: 0.7473, Val Loss: 0.2160, Val Accuracy: 0.7473\n",
            "Epoch 601/1000 - Train Loss: 0.2126, Train Accuracy: 0.7736, Val Loss: 0.2114, Val Accuracy: 0.7582\n",
            "Epoch 701/1000 - Train Loss: 0.2071, Train Accuracy: 0.7824, Val Loss: 0.2072, Val Accuracy: 0.7802\n",
            "Epoch 801/1000 - Train Loss: 0.2021, Train Accuracy: 0.7934, Val Loss: 0.2033, Val Accuracy: 0.7912\n",
            "Epoch 901/1000 - Train Loss: 0.1975, Train Accuracy: 0.8198, Val Loss: 0.1997, Val Accuracy: 0.8242\n",
            "Epoch 1/1000 - Train Loss: 0.3034, Train Accuracy: 0.6286, Val Loss: 0.3534, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.2862, Train Accuracy: 0.6264, Val Loss: 0.3331, Val Accuracy: 0.5495\n",
            "Epoch 201/1000 - Train Loss: 0.2688, Train Accuracy: 0.6264, Val Loss: 0.3128, Val Accuracy: 0.5495\n",
            "Epoch 301/1000 - Train Loss: 0.2517, Train Accuracy: 0.6264, Val Loss: 0.2929, Val Accuracy: 0.5385\n",
            "Epoch 401/1000 - Train Loss: 0.2353, Train Accuracy: 0.6286, Val Loss: 0.2739, Val Accuracy: 0.5495\n",
            "Epoch 501/1000 - Train Loss: 0.2198, Train Accuracy: 0.6352, Val Loss: 0.2558, Val Accuracy: 0.5495\n",
            "Epoch 601/1000 - Train Loss: 0.2056, Train Accuracy: 0.6374, Val Loss: 0.2391, Val Accuracy: 0.5385\n",
            "Epoch 701/1000 - Train Loss: 0.1926, Train Accuracy: 0.6549, Val Loss: 0.2237, Val Accuracy: 0.5714\n",
            "Epoch 801/1000 - Train Loss: 0.1810, Train Accuracy: 0.6747, Val Loss: 0.2099, Val Accuracy: 0.6044\n",
            "Epoch 901/1000 - Train Loss: 0.1708, Train Accuracy: 0.6945, Val Loss: 0.1976, Val Accuracy: 0.6264\n",
            "Epoch 1/1000 - Train Loss: 0.2560, Train Accuracy: 0.6286, Val Loss: 0.3018, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.2442, Train Accuracy: 0.6286, Val Loss: 0.2872, Val Accuracy: 0.5604\n",
            "Epoch 201/1000 - Train Loss: 0.2331, Train Accuracy: 0.6286, Val Loss: 0.2733, Val Accuracy: 0.5604\n",
            "Epoch 301/1000 - Train Loss: 0.2228, Train Accuracy: 0.6286, Val Loss: 0.2603, Val Accuracy: 0.5604\n",
            "Epoch 401/1000 - Train Loss: 0.2134, Train Accuracy: 0.6286, Val Loss: 0.2482, Val Accuracy: 0.5604\n",
            "Epoch 501/1000 - Train Loss: 0.2047, Train Accuracy: 0.6308, Val Loss: 0.2371, Val Accuracy: 0.5604\n",
            "Epoch 601/1000 - Train Loss: 0.1969, Train Accuracy: 0.6308, Val Loss: 0.2269, Val Accuracy: 0.5604\n",
            "Epoch 701/1000 - Train Loss: 0.1897, Train Accuracy: 0.6418, Val Loss: 0.2176, Val Accuracy: 0.5714\n",
            "Epoch 801/1000 - Train Loss: 0.1832, Train Accuracy: 0.6484, Val Loss: 0.2092, Val Accuracy: 0.5604\n",
            "Epoch 901/1000 - Train Loss: 0.1773, Train Accuracy: 0.6571, Val Loss: 0.2016, Val Accuracy: 0.5604\n",
            "Epoch 1/1000 - Train Loss: 0.2510, Train Accuracy: 0.5209, Val Loss: 0.2515, Val Accuracy: 0.5165\n",
            "Epoch 101/1000 - Train Loss: 0.2427, Train Accuracy: 0.5978, Val Loss: 0.2449, Val Accuracy: 0.5604\n",
            "Epoch 201/1000 - Train Loss: 0.2355, Train Accuracy: 0.6396, Val Loss: 0.2392, Val Accuracy: 0.5824\n",
            "Epoch 301/1000 - Train Loss: 0.2291, Train Accuracy: 0.6945, Val Loss: 0.2341, Val Accuracy: 0.6703\n",
            "Epoch 401/1000 - Train Loss: 0.2235, Train Accuracy: 0.7121, Val Loss: 0.2295, Val Accuracy: 0.7143\n",
            "Epoch 501/1000 - Train Loss: 0.2183, Train Accuracy: 0.7253, Val Loss: 0.2252, Val Accuracy: 0.7253\n",
            "Epoch 601/1000 - Train Loss: 0.2136, Train Accuracy: 0.7363, Val Loss: 0.2213, Val Accuracy: 0.7253\n",
            "Epoch 701/1000 - Train Loss: 0.2092, Train Accuracy: 0.7495, Val Loss: 0.2176, Val Accuracy: 0.7253\n",
            "Epoch 801/1000 - Train Loss: 0.2051, Train Accuracy: 0.7473, Val Loss: 0.2141, Val Accuracy: 0.7033\n",
            "Epoch 901/1000 - Train Loss: 0.2012, Train Accuracy: 0.7560, Val Loss: 0.2106, Val Accuracy: 0.7143\n",
            "Epoch 1/1000 - Train Loss: 0.2815, Train Accuracy: 0.4000, Val Loss: 0.2511, Val Accuracy: 0.4945\n",
            "Epoch 101/1000 - Train Loss: 0.2595, Train Accuracy: 0.4176, Val Loss: 0.2321, Val Accuracy: 0.5275\n",
            "Epoch 201/1000 - Train Loss: 0.2401, Train Accuracy: 0.4901, Val Loss: 0.2154, Val Accuracy: 0.6154\n",
            "Epoch 301/1000 - Train Loss: 0.2229, Train Accuracy: 0.5780, Val Loss: 0.2007, Val Accuracy: 0.7033\n",
            "Epoch 401/1000 - Train Loss: 0.2077, Train Accuracy: 0.6813, Val Loss: 0.1877, Val Accuracy: 0.7802\n",
            "Epoch 501/1000 - Train Loss: 0.1943, Train Accuracy: 0.8154, Val Loss: 0.1763, Val Accuracy: 0.8901\n",
            "Epoch 601/1000 - Train Loss: 0.1825, Train Accuracy: 0.8593, Val Loss: 0.1661, Val Accuracy: 0.9011\n",
            "Epoch 701/1000 - Train Loss: 0.1719, Train Accuracy: 0.8835, Val Loss: 0.1571, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.1625, Train Accuracy: 0.8923, Val Loss: 0.1489, Val Accuracy: 0.9231\n",
            "Epoch 901/1000 - Train Loss: 0.1541, Train Accuracy: 0.8923, Val Loss: 0.1417, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.1755, Train Accuracy: 0.7033, Val Loss: 0.1914, Val Accuracy: 0.6813\n",
            "Epoch 101/1000 - Train Loss: 0.1723, Train Accuracy: 0.7231, Val Loss: 0.1877, Val Accuracy: 0.6923\n",
            "Epoch 201/1000 - Train Loss: 0.1692, Train Accuracy: 0.7451, Val Loss: 0.1841, Val Accuracy: 0.7033\n",
            "Epoch 301/1000 - Train Loss: 0.1662, Train Accuracy: 0.7648, Val Loss: 0.1806, Val Accuracy: 0.7143\n",
            "Epoch 401/1000 - Train Loss: 0.1633, Train Accuracy: 0.7780, Val Loss: 0.1772, Val Accuracy: 0.7363\n",
            "Epoch 501/1000 - Train Loss: 0.1604, Train Accuracy: 0.7934, Val Loss: 0.1739, Val Accuracy: 0.7473\n",
            "Epoch 601/1000 - Train Loss: 0.1577, Train Accuracy: 0.8066, Val Loss: 0.1708, Val Accuracy: 0.7582\n",
            "Epoch 701/1000 - Train Loss: 0.1550, Train Accuracy: 0.8176, Val Loss: 0.1677, Val Accuracy: 0.7582\n",
            "Epoch 801/1000 - Train Loss: 0.1524, Train Accuracy: 0.8242, Val Loss: 0.1647, Val Accuracy: 0.7582\n",
            "Epoch 901/1000 - Train Loss: 0.1499, Train Accuracy: 0.8330, Val Loss: 0.1618, Val Accuracy: 0.7912\n",
            "Epoch 1/1000 - Train Loss: 0.3269, Train Accuracy: 0.3912, Val Loss: 0.2924, Val Accuracy: 0.4505\n",
            "Epoch 101/1000 - Train Loss: 0.2970, Train Accuracy: 0.4110, Val Loss: 0.2669, Val Accuracy: 0.4835\n",
            "Epoch 201/1000 - Train Loss: 0.2702, Train Accuracy: 0.4527, Val Loss: 0.2441, Val Accuracy: 0.4945\n",
            "Epoch 301/1000 - Train Loss: 0.2465, Train Accuracy: 0.5077, Val Loss: 0.2242, Val Accuracy: 0.5714\n",
            "Epoch 401/1000 - Train Loss: 0.2258, Train Accuracy: 0.5692, Val Loss: 0.2068, Val Accuracy: 0.6374\n",
            "Epoch 501/1000 - Train Loss: 0.2078, Train Accuracy: 0.6484, Val Loss: 0.1919, Val Accuracy: 0.7253\n",
            "Epoch 601/1000 - Train Loss: 0.1924, Train Accuracy: 0.6967, Val Loss: 0.1792, Val Accuracy: 0.7692\n",
            "Epoch 701/1000 - Train Loss: 0.1792, Train Accuracy: 0.7495, Val Loss: 0.1683, Val Accuracy: 0.8022\n",
            "Epoch 801/1000 - Train Loss: 0.1678, Train Accuracy: 0.7714, Val Loss: 0.1590, Val Accuracy: 0.8132\n",
            "Epoch 901/1000 - Train Loss: 0.1580, Train Accuracy: 0.8066, Val Loss: 0.1511, Val Accuracy: 0.8242\n",
            "Epoch 1/1000 - Train Loss: 0.2833, Train Accuracy: 0.3978, Val Loss: 0.2629, Val Accuracy: 0.4615\n",
            "Epoch 101/1000 - Train Loss: 0.1099, Train Accuracy: 0.8967, Val Loss: 0.1093, Val Accuracy: 0.9011\n",
            "Epoch 201/1000 - Train Loss: 0.0812, Train Accuracy: 0.9099, Val Loss: 0.0831, Val Accuracy: 0.9121\n",
            "Epoch 301/1000 - Train Loss: 0.0693, Train Accuracy: 0.9297, Val Loss: 0.0705, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0615, Train Accuracy: 0.9341, Val Loss: 0.0614, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0556, Train Accuracy: 0.9385, Val Loss: 0.0540, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0509, Train Accuracy: 0.9385, Val Loss: 0.0477, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0470, Train Accuracy: 0.9385, Val Loss: 0.0425, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0437, Train Accuracy: 0.9451, Val Loss: 0.0382, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0410, Train Accuracy: 0.9495, Val Loss: 0.0346, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2451, Train Accuracy: 0.6264, Val Loss: 0.2656, Val Accuracy: 0.5495\n",
            "Epoch 101/1000 - Train Loss: 0.1499, Train Accuracy: 0.8154, Val Loss: 0.1637, Val Accuracy: 0.7912\n",
            "Epoch 201/1000 - Train Loss: 0.1055, Train Accuracy: 0.8879, Val Loss: 0.1152, Val Accuracy: 0.8462\n",
            "Epoch 301/1000 - Train Loss: 0.0828, Train Accuracy: 0.9209, Val Loss: 0.0900, Val Accuracy: 0.8901\n",
            "Epoch 401/1000 - Train Loss: 0.0690, Train Accuracy: 0.9319, Val Loss: 0.0741, Val Accuracy: 0.9121\n",
            "Epoch 501/1000 - Train Loss: 0.0598, Train Accuracy: 0.9429, Val Loss: 0.0629, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0533, Train Accuracy: 0.9429, Val Loss: 0.0547, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0483, Train Accuracy: 0.9451, Val Loss: 0.0483, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0444, Train Accuracy: 0.9538, Val Loss: 0.0432, Val Accuracy: 0.9560\n",
            "Epoch 901/1000 - Train Loss: 0.0413, Train Accuracy: 0.9560, Val Loss: 0.0390, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2036, Train Accuracy: 0.7209, Val Loss: 0.2101, Val Accuracy: 0.7033\n",
            "Epoch 101/1000 - Train Loss: 0.1357, Train Accuracy: 0.8527, Val Loss: 0.1387, Val Accuracy: 0.8462\n",
            "Epoch 201/1000 - Train Loss: 0.1070, Train Accuracy: 0.9165, Val Loss: 0.1086, Val Accuracy: 0.9121\n",
            "Epoch 301/1000 - Train Loss: 0.0880, Train Accuracy: 0.9341, Val Loss: 0.0878, Val Accuracy: 0.9231\n",
            "Epoch 401/1000 - Train Loss: 0.0743, Train Accuracy: 0.9451, Val Loss: 0.0723, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0641, Train Accuracy: 0.9538, Val Loss: 0.0603, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0563, Train Accuracy: 0.9516, Val Loss: 0.0510, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0502, Train Accuracy: 0.9538, Val Loss: 0.0436, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0453, Train Accuracy: 0.9626, Val Loss: 0.0375, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0414, Train Accuracy: 0.9670, Val Loss: 0.0325, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2337, Train Accuracy: 0.6308, Val Loss: 0.2525, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.1905, Train Accuracy: 0.7033, Val Loss: 0.2055, Val Accuracy: 0.6374\n",
            "Epoch 201/1000 - Train Loss: 0.1569, Train Accuracy: 0.7934, Val Loss: 0.1703, Val Accuracy: 0.7253\n",
            "Epoch 301/1000 - Train Loss: 0.1297, Train Accuracy: 0.8703, Val Loss: 0.1415, Val Accuracy: 0.8242\n",
            "Epoch 401/1000 - Train Loss: 0.1081, Train Accuracy: 0.8989, Val Loss: 0.1182, Val Accuracy: 0.8571\n",
            "Epoch 501/1000 - Train Loss: 0.0916, Train Accuracy: 0.9231, Val Loss: 0.0998, Val Accuracy: 0.8901\n",
            "Epoch 601/1000 - Train Loss: 0.0791, Train Accuracy: 0.9341, Val Loss: 0.0854, Val Accuracy: 0.9231\n",
            "Epoch 701/1000 - Train Loss: 0.0696, Train Accuracy: 0.9319, Val Loss: 0.0740, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0622, Train Accuracy: 0.9341, Val Loss: 0.0649, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0564, Train Accuracy: 0.9341, Val Loss: 0.0575, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.1887, Train Accuracy: 0.6330, Val Loss: 0.2220, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.1156, Train Accuracy: 0.8725, Val Loss: 0.1276, Val Accuracy: 0.8022\n",
            "Epoch 201/1000 - Train Loss: 0.0870, Train Accuracy: 0.9297, Val Loss: 0.0899, Val Accuracy: 0.9231\n",
            "Epoch 301/1000 - Train Loss: 0.0718, Train Accuracy: 0.9451, Val Loss: 0.0705, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0621, Train Accuracy: 0.9495, Val Loss: 0.0581, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0553, Train Accuracy: 0.9495, Val Loss: 0.0494, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0503, Train Accuracy: 0.9538, Val Loss: 0.0428, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0463, Train Accuracy: 0.9538, Val Loss: 0.0378, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0431, Train Accuracy: 0.9560, Val Loss: 0.0337, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0404, Train Accuracy: 0.9560, Val Loss: 0.0304, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.3264, Train Accuracy: 0.5341, Val Loss: 0.3491, Val Accuracy: 0.4725\n",
            "Epoch 101/1000 - Train Loss: 0.2583, Train Accuracy: 0.6220, Val Loss: 0.2716, Val Accuracy: 0.5495\n",
            "Epoch 201/1000 - Train Loss: 0.2086, Train Accuracy: 0.6484, Val Loss: 0.2170, Val Accuracy: 0.5824\n",
            "Epoch 301/1000 - Train Loss: 0.1700, Train Accuracy: 0.7780, Val Loss: 0.1767, Val Accuracy: 0.7582\n",
            "Epoch 401/1000 - Train Loss: 0.1383, Train Accuracy: 0.8857, Val Loss: 0.1443, Val Accuracy: 0.8681\n",
            "Epoch 501/1000 - Train Loss: 0.1125, Train Accuracy: 0.9099, Val Loss: 0.1175, Val Accuracy: 0.9011\n",
            "Epoch 601/1000 - Train Loss: 0.0924, Train Accuracy: 0.9341, Val Loss: 0.0961, Val Accuracy: 0.9231\n",
            "Epoch 701/1000 - Train Loss: 0.0773, Train Accuracy: 0.9363, Val Loss: 0.0796, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0660, Train Accuracy: 0.9429, Val Loss: 0.0669, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0576, Train Accuracy: 0.9538, Val Loss: 0.0573, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.4496, Train Accuracy: 0.3714, Val Loss: 0.4064, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.2066, Train Accuracy: 0.7890, Val Loss: 0.2022, Val Accuracy: 0.7912\n",
            "Epoch 201/1000 - Train Loss: 0.1381, Train Accuracy: 0.9055, Val Loss: 0.1441, Val Accuracy: 0.8681\n",
            "Epoch 301/1000 - Train Loss: 0.1072, Train Accuracy: 0.9341, Val Loss: 0.1129, Val Accuracy: 0.9231\n",
            "Epoch 401/1000 - Train Loss: 0.0874, Train Accuracy: 0.9407, Val Loss: 0.0911, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0737, Train Accuracy: 0.9363, Val Loss: 0.0754, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0638, Train Accuracy: 0.9429, Val Loss: 0.0637, Val Accuracy: 0.9560\n",
            "Epoch 701/1000 - Train Loss: 0.0564, Train Accuracy: 0.9429, Val Loss: 0.0548, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0506, Train Accuracy: 0.9473, Val Loss: 0.0478, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0461, Train Accuracy: 0.9516, Val Loss: 0.0423, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.1832, Train Accuracy: 0.7824, Val Loss: 0.1942, Val Accuracy: 0.7363\n",
            "Epoch 101/1000 - Train Loss: 0.1256, Train Accuracy: 0.9033, Val Loss: 0.1354, Val Accuracy: 0.8571\n",
            "Epoch 201/1000 - Train Loss: 0.0940, Train Accuracy: 0.9275, Val Loss: 0.1019, Val Accuracy: 0.8901\n",
            "Epoch 301/1000 - Train Loss: 0.0752, Train Accuracy: 0.9385, Val Loss: 0.0810, Val Accuracy: 0.9341\n",
            "Epoch 401/1000 - Train Loss: 0.0631, Train Accuracy: 0.9451, Val Loss: 0.0668, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0546, Train Accuracy: 0.9538, Val Loss: 0.0564, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0484, Train Accuracy: 0.9560, Val Loss: 0.0486, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0438, Train Accuracy: 0.9582, Val Loss: 0.0425, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0401, Train Accuracy: 0.9626, Val Loss: 0.0376, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0372, Train Accuracy: 0.9670, Val Loss: 0.0337, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2912, Train Accuracy: 0.3692, Val Loss: 0.2907, Val Accuracy: 0.3736\n",
            "Epoch 101/1000 - Train Loss: 0.1666, Train Accuracy: 0.8088, Val Loss: 0.1701, Val Accuracy: 0.7692\n",
            "Epoch 201/1000 - Train Loss: 0.1225, Train Accuracy: 0.8813, Val Loss: 0.1241, Val Accuracy: 0.8352\n",
            "Epoch 301/1000 - Train Loss: 0.0997, Train Accuracy: 0.9033, Val Loss: 0.0994, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.0853, Train Accuracy: 0.9187, Val Loss: 0.0834, Val Accuracy: 0.9231\n",
            "Epoch 501/1000 - Train Loss: 0.0752, Train Accuracy: 0.9253, Val Loss: 0.0719, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0676, Train Accuracy: 0.9275, Val Loss: 0.0630, Val Accuracy: 0.9560\n",
            "Epoch 701/1000 - Train Loss: 0.0615, Train Accuracy: 0.9297, Val Loss: 0.0558, Val Accuracy: 0.9560\n",
            "Epoch 801/1000 - Train Loss: 0.0565, Train Accuracy: 0.9341, Val Loss: 0.0497, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0523, Train Accuracy: 0.9385, Val Loss: 0.0445, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2834, Train Accuracy: 0.1143, Val Loss: 0.2835, Val Accuracy: 0.1319\n",
            "Epoch 101/1000 - Train Loss: 0.2222, Train Accuracy: 0.6286, Val Loss: 0.2356, Val Accuracy: 0.5604\n",
            "Epoch 201/1000 - Train Loss: 0.1993, Train Accuracy: 0.6286, Val Loss: 0.2165, Val Accuracy: 0.5604\n",
            "Epoch 301/1000 - Train Loss: 0.1806, Train Accuracy: 0.6286, Val Loss: 0.1988, Val Accuracy: 0.5604\n",
            "Epoch 401/1000 - Train Loss: 0.1628, Train Accuracy: 0.6549, Val Loss: 0.1807, Val Accuracy: 0.6044\n",
            "Epoch 501/1000 - Train Loss: 0.1460, Train Accuracy: 0.7560, Val Loss: 0.1629, Val Accuracy: 0.6923\n",
            "Epoch 601/1000 - Train Loss: 0.1306, Train Accuracy: 0.8330, Val Loss: 0.1461, Val Accuracy: 0.7692\n",
            "Epoch 701/1000 - Train Loss: 0.1167, Train Accuracy: 0.8813, Val Loss: 0.1304, Val Accuracy: 0.8242\n",
            "Epoch 801/1000 - Train Loss: 0.1045, Train Accuracy: 0.9077, Val Loss: 0.1162, Val Accuracy: 0.8571\n",
            "Epoch 901/1000 - Train Loss: 0.0939, Train Accuracy: 0.9209, Val Loss: 0.1034, Val Accuracy: 0.8901\n",
            "Epoch 1/1000 - Train Loss: 0.2955, Train Accuracy: 0.3626, Val Loss: 0.3082, Val Accuracy: 0.3187\n",
            "Epoch 101/1000 - Train Loss: 0.0673, Train Accuracy: 0.9407, Val Loss: 0.0628, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0405, Train Accuracy: 0.9626, Val Loss: 0.0312, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0301, Train Accuracy: 0.9736, Val Loss: 0.0199, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0250, Train Accuracy: 0.9802, Val Loss: 0.0146, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0221, Train Accuracy: 0.9802, Val Loss: 0.0117, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0203, Train Accuracy: 0.9824, Val Loss: 0.0100, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0189, Train Accuracy: 0.9802, Val Loss: 0.0089, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0178, Train Accuracy: 0.9802, Val Loss: 0.0081, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0169, Train Accuracy: 0.9824, Val Loss: 0.0076, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3205, Train Accuracy: 0.1187, Val Loss: 0.3254, Val Accuracy: 0.1099\n",
            "Epoch 101/1000 - Train Loss: 0.1193, Train Accuracy: 0.9077, Val Loss: 0.1261, Val Accuracy: 0.8462\n",
            "Epoch 201/1000 - Train Loss: 0.0567, Train Accuracy: 0.9473, Val Loss: 0.0533, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0388, Train Accuracy: 0.9538, Val Loss: 0.0297, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0305, Train Accuracy: 0.9626, Val Loss: 0.0196, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0260, Train Accuracy: 0.9648, Val Loss: 0.0147, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0231, Train Accuracy: 0.9780, Val Loss: 0.0120, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0212, Train Accuracy: 0.9780, Val Loss: 0.0105, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0197, Train Accuracy: 0.9780, Val Loss: 0.0095, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0186, Train Accuracy: 0.9802, Val Loss: 0.0088, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3152, Train Accuracy: 0.3736, Val Loss: 0.2857, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0738, Train Accuracy: 0.9209, Val Loss: 0.0773, Val Accuracy: 0.9011\n",
            "Epoch 201/1000 - Train Loss: 0.0471, Train Accuracy: 0.9407, Val Loss: 0.0400, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0363, Train Accuracy: 0.9538, Val Loss: 0.0260, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0305, Train Accuracy: 0.9626, Val Loss: 0.0197, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0269, Train Accuracy: 0.9670, Val Loss: 0.0160, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0245, Train Accuracy: 0.9714, Val Loss: 0.0138, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0228, Train Accuracy: 0.9736, Val Loss: 0.0122, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0214, Train Accuracy: 0.9736, Val Loss: 0.0111, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0204, Train Accuracy: 0.9736, Val Loss: 0.0102, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2526, Train Accuracy: 0.6286, Val Loss: 0.2939, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.0799, Train Accuracy: 0.9451, Val Loss: 0.0882, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0470, Train Accuracy: 0.9582, Val Loss: 0.0479, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0345, Train Accuracy: 0.9714, Val Loss: 0.0321, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0278, Train Accuracy: 0.9736, Val Loss: 0.0228, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0235, Train Accuracy: 0.9802, Val Loss: 0.0166, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0211, Train Accuracy: 0.9780, Val Loss: 0.0138, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0196, Train Accuracy: 0.9780, Val Loss: 0.0121, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0185, Train Accuracy: 0.9780, Val Loss: 0.0109, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0177, Train Accuracy: 0.9780, Val Loss: 0.0099, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.1825, Train Accuracy: 0.8440, Val Loss: 0.1683, Val Accuracy: 0.9011\n",
            "Epoch 101/1000 - Train Loss: 0.0512, Train Accuracy: 0.9516, Val Loss: 0.0456, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0350, Train Accuracy: 0.9648, Val Loss: 0.0262, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0281, Train Accuracy: 0.9758, Val Loss: 0.0183, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0243, Train Accuracy: 0.9758, Val Loss: 0.0143, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0220, Train Accuracy: 0.9758, Val Loss: 0.0119, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0205, Train Accuracy: 0.9758, Val Loss: 0.0104, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0193, Train Accuracy: 0.9780, Val Loss: 0.0094, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0184, Train Accuracy: 0.9824, Val Loss: 0.0086, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0177, Train Accuracy: 0.9846, Val Loss: 0.0080, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3035, Train Accuracy: 0.3758, Val Loss: 0.2714, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0736, Train Accuracy: 0.9407, Val Loss: 0.0765, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0442, Train Accuracy: 0.9582, Val Loss: 0.0396, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0327, Train Accuracy: 0.9670, Val Loss: 0.0230, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0275, Train Accuracy: 0.9714, Val Loss: 0.0165, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0245, Train Accuracy: 0.9714, Val Loss: 0.0130, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0224, Train Accuracy: 0.9736, Val Loss: 0.0109, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0210, Train Accuracy: 0.9758, Val Loss: 0.0096, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0198, Train Accuracy: 0.9802, Val Loss: 0.0087, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0189, Train Accuracy: 0.9824, Val Loss: 0.0081, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.4305, Train Accuracy: 0.3714, Val Loss: 0.3836, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0740, Train Accuracy: 0.9187, Val Loss: 0.0771, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.0472, Train Accuracy: 0.9473, Val Loss: 0.0485, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0339, Train Accuracy: 0.9692, Val Loss: 0.0316, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0274, Train Accuracy: 0.9758, Val Loss: 0.0233, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0238, Train Accuracy: 0.9802, Val Loss: 0.0187, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0215, Train Accuracy: 0.9802, Val Loss: 0.0154, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0200, Train Accuracy: 0.9824, Val Loss: 0.0130, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0189, Train Accuracy: 0.9824, Val Loss: 0.0112, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0180, Train Accuracy: 0.9824, Val Loss: 0.0098, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2770, Train Accuracy: 0.3758, Val Loss: 0.2542, Val Accuracy: 0.4505\n",
            "Epoch 101/1000 - Train Loss: 0.1051, Train Accuracy: 0.9341, Val Loss: 0.1092, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0485, Train Accuracy: 0.9626, Val Loss: 0.0457, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0332, Train Accuracy: 0.9736, Val Loss: 0.0270, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0266, Train Accuracy: 0.9780, Val Loss: 0.0193, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0229, Train Accuracy: 0.9802, Val Loss: 0.0153, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0206, Train Accuracy: 0.9802, Val Loss: 0.0130, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0191, Train Accuracy: 0.9824, Val Loss: 0.0114, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0179, Train Accuracy: 0.9824, Val Loss: 0.0103, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0171, Train Accuracy: 0.9846, Val Loss: 0.0094, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3482, Train Accuracy: 0.3714, Val Loss: 0.3172, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.1081, Train Accuracy: 0.9231, Val Loss: 0.1079, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0563, Train Accuracy: 0.9495, Val Loss: 0.0487, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0393, Train Accuracy: 0.9604, Val Loss: 0.0286, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0315, Train Accuracy: 0.9692, Val Loss: 0.0202, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0270, Train Accuracy: 0.9714, Val Loss: 0.0157, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0241, Train Accuracy: 0.9736, Val Loss: 0.0131, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0220, Train Accuracy: 0.9780, Val Loss: 0.0114, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0205, Train Accuracy: 0.9802, Val Loss: 0.0102, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0192, Train Accuracy: 0.9824, Val Loss: 0.0094, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.4131, Train Accuracy: 0.3714, Val Loss: 0.3763, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0872, Train Accuracy: 0.9055, Val Loss: 0.0903, Val Accuracy: 0.8901\n",
            "Epoch 201/1000 - Train Loss: 0.0544, Train Accuracy: 0.9341, Val Loss: 0.0519, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0385, Train Accuracy: 0.9582, Val Loss: 0.0304, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0297, Train Accuracy: 0.9692, Val Loss: 0.0191, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0249, Train Accuracy: 0.9758, Val Loss: 0.0135, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0222, Train Accuracy: 0.9780, Val Loss: 0.0106, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0204, Train Accuracy: 0.9780, Val Loss: 0.0090, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0192, Train Accuracy: 0.9824, Val Loss: 0.0079, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0183, Train Accuracy: 0.9824, Val Loss: 0.0072, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.1782, Train Accuracy: 0.7978, Val Loss: 0.1907, Val Accuracy: 0.7582\n",
            "Epoch 101/1000 - Train Loss: 0.0531, Train Accuracy: 0.9341, Val Loss: 0.0618, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0286, Train Accuracy: 0.9714, Val Loss: 0.0219, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0217, Train Accuracy: 0.9758, Val Loss: 0.0130, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0189, Train Accuracy: 0.9802, Val Loss: 0.0099, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0173, Train Accuracy: 0.9846, Val Loss: 0.0082, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0161, Train Accuracy: 0.9846, Val Loss: 0.0072, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0153, Train Accuracy: 0.9868, Val Loss: 0.0064, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0145, Train Accuracy: 0.9868, Val Loss: 0.0058, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0139, Train Accuracy: 0.9868, Val Loss: 0.0054, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3641, Train Accuracy: 0.2198, Val Loss: 0.3426, Val Accuracy: 0.2967\n",
            "Epoch 101/1000 - Train Loss: 0.0448, Train Accuracy: 0.9538, Val Loss: 0.0410, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0282, Train Accuracy: 0.9626, Val Loss: 0.0204, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0228, Train Accuracy: 0.9714, Val Loss: 0.0146, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0200, Train Accuracy: 0.9758, Val Loss: 0.0115, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0181, Train Accuracy: 0.9802, Val Loss: 0.0095, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0168, Train Accuracy: 0.9824, Val Loss: 0.0082, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0158, Train Accuracy: 0.9824, Val Loss: 0.0072, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0150, Train Accuracy: 0.9846, Val Loss: 0.0065, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0144, Train Accuracy: 0.9846, Val Loss: 0.0059, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2777, Train Accuracy: 0.4220, Val Loss: 0.2924, Val Accuracy: 0.2967\n",
            "Epoch 101/1000 - Train Loss: 0.0528, Train Accuracy: 0.9473, Val Loss: 0.0520, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0278, Train Accuracy: 0.9758, Val Loss: 0.0219, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0214, Train Accuracy: 0.9802, Val Loss: 0.0135, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0188, Train Accuracy: 0.9824, Val Loss: 0.0100, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0172, Train Accuracy: 0.9802, Val Loss: 0.0082, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0160, Train Accuracy: 0.9802, Val Loss: 0.0071, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0151, Train Accuracy: 0.9824, Val Loss: 0.0063, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0144, Train Accuracy: 0.9846, Val Loss: 0.0057, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0138, Train Accuracy: 0.9846, Val Loss: 0.0053, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2776, Train Accuracy: 0.3692, Val Loss: 0.2620, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0499, Train Accuracy: 0.9473, Val Loss: 0.0483, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0277, Train Accuracy: 0.9780, Val Loss: 0.0192, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0209, Train Accuracy: 0.9824, Val Loss: 0.0113, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0181, Train Accuracy: 0.9846, Val Loss: 0.0083, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0165, Train Accuracy: 0.9824, Val Loss: 0.0068, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0155, Train Accuracy: 0.9824, Val Loss: 0.0060, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0054, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0140, Train Accuracy: 0.9824, Val Loss: 0.0050, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0134, Train Accuracy: 0.9824, Val Loss: 0.0047, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.5186, Train Accuracy: 0.3714, Val Loss: 0.4677, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0538, Train Accuracy: 0.9604, Val Loss: 0.0499, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0273, Train Accuracy: 0.9758, Val Loss: 0.0176, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0208, Train Accuracy: 0.9802, Val Loss: 0.0103, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0181, Train Accuracy: 0.9824, Val Loss: 0.0076, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0165, Train Accuracy: 0.9802, Val Loss: 0.0062, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0155, Train Accuracy: 0.9802, Val Loss: 0.0054, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0147, Train Accuracy: 0.9846, Val Loss: 0.0048, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0140, Train Accuracy: 0.9846, Val Loss: 0.0044, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0135, Train Accuracy: 0.9846, Val Loss: 0.0041, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2197, Train Accuracy: 0.6440, Val Loss: 0.2387, Val Accuracy: 0.5824\n",
            "Epoch 101/1000 - Train Loss: 0.0371, Train Accuracy: 0.9626, Val Loss: 0.0307, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0240, Train Accuracy: 0.9736, Val Loss: 0.0148, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0198, Train Accuracy: 0.9802, Val Loss: 0.0105, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0178, Train Accuracy: 0.9846, Val Loss: 0.0086, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0165, Train Accuracy: 0.9846, Val Loss: 0.0076, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0155, Train Accuracy: 0.9868, Val Loss: 0.0069, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0147, Train Accuracy: 0.9868, Val Loss: 0.0063, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0141, Train Accuracy: 0.9846, Val Loss: 0.0059, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0056, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2904, Train Accuracy: 0.6286, Val Loss: 0.3368, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.0480, Train Accuracy: 0.9516, Val Loss: 0.0478, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0283, Train Accuracy: 0.9714, Val Loss: 0.0216, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0225, Train Accuracy: 0.9758, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0198, Train Accuracy: 0.9780, Val Loss: 0.0108, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0181, Train Accuracy: 0.9780, Val Loss: 0.0090, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0169, Train Accuracy: 0.9780, Val Loss: 0.0078, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0159, Train Accuracy: 0.9802, Val Loss: 0.0069, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0152, Train Accuracy: 0.9824, Val Loss: 0.0063, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0145, Train Accuracy: 0.9824, Val Loss: 0.0058, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2392, Train Accuracy: 0.6462, Val Loss: 0.2559, Val Accuracy: 0.5934\n",
            "Epoch 101/1000 - Train Loss: 0.0413, Train Accuracy: 0.9560, Val Loss: 0.0347, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0266, Train Accuracy: 0.9758, Val Loss: 0.0193, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0211, Train Accuracy: 0.9802, Val Loss: 0.0144, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0184, Train Accuracy: 0.9824, Val Loss: 0.0119, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0167, Train Accuracy: 0.9824, Val Loss: 0.0103, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0155, Train Accuracy: 0.9868, Val Loss: 0.0090, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0146, Train Accuracy: 0.9868, Val Loss: 0.0079, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0139, Train Accuracy: 0.9868, Val Loss: 0.0070, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0134, Train Accuracy: 0.9868, Val Loss: 0.0063, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2201, Train Accuracy: 0.6330, Val Loss: 0.2349, Val Accuracy: 0.5714\n",
            "Epoch 101/1000 - Train Loss: 0.0417, Train Accuracy: 0.9604, Val Loss: 0.0340, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0241, Train Accuracy: 0.9802, Val Loss: 0.0150, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0193, Train Accuracy: 0.9802, Val Loss: 0.0102, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0172, Train Accuracy: 0.9824, Val Loss: 0.0081, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0160, Train Accuracy: 0.9846, Val Loss: 0.0069, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0152, Train Accuracy: 0.9846, Val Loss: 0.0062, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0145, Train Accuracy: 0.9846, Val Loss: 0.0056, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0140, Train Accuracy: 0.9846, Val Loss: 0.0052, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0049, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2265, Train Accuracy: 0.6286, Val Loss: 0.2536, Val Accuracy: 0.5604\n",
            "Epoch 101/1000 - Train Loss: 0.0528, Train Accuracy: 0.9538, Val Loss: 0.0545, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0293, Train Accuracy: 0.9714, Val Loss: 0.0239, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0226, Train Accuracy: 0.9780, Val Loss: 0.0152, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0196, Train Accuracy: 0.9824, Val Loss: 0.0114, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0177, Train Accuracy: 0.9846, Val Loss: 0.0094, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0164, Train Accuracy: 0.9824, Val Loss: 0.0081, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0154, Train Accuracy: 0.9824, Val Loss: 0.0072, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0147, Train Accuracy: 0.9824, Val Loss: 0.0065, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0140, Train Accuracy: 0.9846, Val Loss: 0.0059, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3939, Train Accuracy: 0.3692, Val Loss: 0.3797, Val Accuracy: 0.4176\n",
            "Epoch 101/1000 - Train Loss: 0.0289, Train Accuracy: 0.9736, Val Loss: 0.0230, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0176, Train Accuracy: 0.9846, Val Loss: 0.0100, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0150, Train Accuracy: 0.9846, Val Loss: 0.0073, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0060, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0127, Train Accuracy: 0.9868, Val Loss: 0.0051, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0120, Train Accuracy: 0.9868, Val Loss: 0.0045, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0115, Train Accuracy: 0.9890, Val Loss: 0.0040, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0110, Train Accuracy: 0.9890, Val Loss: 0.0037, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0107, Train Accuracy: 0.9890, Val Loss: 0.0034, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.4841, Train Accuracy: 0.3714, Val Loss: 0.4270, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0271, Train Accuracy: 0.9736, Val Loss: 0.0198, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0193, Train Accuracy: 0.9824, Val Loss: 0.0108, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0164, Train Accuracy: 0.9846, Val Loss: 0.0075, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0147, Train Accuracy: 0.9846, Val Loss: 0.0060, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0137, Train Accuracy: 0.9846, Val Loss: 0.0053, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0129, Train Accuracy: 0.9846, Val Loss: 0.0049, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0123, Train Accuracy: 0.9846, Val Loss: 0.0047, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0045, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0044, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2834, Train Accuracy: 0.3670, Val Loss: 0.2894, Val Accuracy: 0.3626\n",
            "Epoch 101/1000 - Train Loss: 0.0284, Train Accuracy: 0.9736, Val Loss: 0.0208, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0197, Train Accuracy: 0.9780, Val Loss: 0.0096, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0168, Train Accuracy: 0.9802, Val Loss: 0.0065, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0152, Train Accuracy: 0.9802, Val Loss: 0.0050, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0140, Train Accuracy: 0.9824, Val Loss: 0.0042, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0132, Train Accuracy: 0.9846, Val Loss: 0.0038, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0125, Train Accuracy: 0.9846, Val Loss: 0.0034, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0032, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0030, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2851, Train Accuracy: 0.4593, Val Loss: 0.2885, Val Accuracy: 0.4505\n",
            "Epoch 101/1000 - Train Loss: 0.0235, Train Accuracy: 0.9846, Val Loss: 0.0171, Val Accuracy: 1.0000\n",
            "Epoch 201/1000 - Train Loss: 0.0168, Train Accuracy: 0.9846, Val Loss: 0.0085, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0064, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0133, Train Accuracy: 0.9824, Val Loss: 0.0054, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0124, Train Accuracy: 0.9846, Val Loss: 0.0047, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0117, Train Accuracy: 0.9846, Val Loss: 0.0043, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0111, Train Accuracy: 0.9890, Val Loss: 0.0040, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0107, Train Accuracy: 0.9890, Val Loss: 0.0037, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0104, Train Accuracy: 0.9890, Val Loss: 0.0034, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3689, Train Accuracy: 0.4571, Val Loss: 0.3977, Val Accuracy: 0.4066\n",
            "Epoch 101/1000 - Train Loss: 0.0247, Train Accuracy: 0.9780, Val Loss: 0.0154, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0175, Train Accuracy: 0.9824, Val Loss: 0.0095, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0151, Train Accuracy: 0.9846, Val Loss: 0.0077, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0137, Train Accuracy: 0.9846, Val Loss: 0.0066, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0128, Train Accuracy: 0.9846, Val Loss: 0.0059, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0122, Train Accuracy: 0.9846, Val Loss: 0.0053, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0117, Train Accuracy: 0.9846, Val Loss: 0.0049, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0112, Train Accuracy: 0.9868, Val Loss: 0.0046, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0109, Train Accuracy: 0.9890, Val Loss: 0.0043, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3152, Train Accuracy: 0.4549, Val Loss: 0.3476, Val Accuracy: 0.3187\n",
            "Epoch 101/1000 - Train Loss: 0.0255, Train Accuracy: 0.9758, Val Loss: 0.0175, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0182, Train Accuracy: 0.9802, Val Loss: 0.0093, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0154, Train Accuracy: 0.9846, Val Loss: 0.0067, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0139, Train Accuracy: 0.9846, Val Loss: 0.0055, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0128, Train Accuracy: 0.9846, Val Loss: 0.0047, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0042, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0114, Train Accuracy: 0.9846, Val Loss: 0.0038, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0109, Train Accuracy: 0.9846, Val Loss: 0.0036, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0104, Train Accuracy: 0.9846, Val Loss: 0.0034, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2541, Train Accuracy: 0.4703, Val Loss: 0.2484, Val Accuracy: 0.5165\n",
            "Epoch 101/1000 - Train Loss: 0.0260, Train Accuracy: 0.9736, Val Loss: 0.0199, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0187, Train Accuracy: 0.9846, Val Loss: 0.0119, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0162, Train Accuracy: 0.9868, Val Loss: 0.0087, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0148, Train Accuracy: 0.9868, Val Loss: 0.0070, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0138, Train Accuracy: 0.9868, Val Loss: 0.0059, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0131, Train Accuracy: 0.9868, Val Loss: 0.0052, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0125, Train Accuracy: 0.9868, Val Loss: 0.0047, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0120, Train Accuracy: 0.9868, Val Loss: 0.0044, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9868, Val Loss: 0.0042, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2845, Train Accuracy: 0.4066, Val Loss: 0.2691, Val Accuracy: 0.4945\n",
            "Epoch 101/1000 - Train Loss: 0.0234, Train Accuracy: 0.9780, Val Loss: 0.0136, Val Accuracy: 1.0000\n",
            "Epoch 201/1000 - Train Loss: 0.0179, Train Accuracy: 0.9824, Val Loss: 0.0066, Val Accuracy: 1.0000\n",
            "Epoch 301/1000 - Train Loss: 0.0160, Train Accuracy: 0.9824, Val Loss: 0.0048, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0149, Train Accuracy: 0.9824, Val Loss: 0.0041, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0141, Train Accuracy: 0.9846, Val Loss: 0.0039, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0134, Train Accuracy: 0.9846, Val Loss: 0.0038, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0127, Train Accuracy: 0.9846, Val Loss: 0.0037, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0121, Train Accuracy: 0.9846, Val Loss: 0.0035, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0116, Train Accuracy: 0.9846, Val Loss: 0.0034, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2312, Train Accuracy: 0.6352, Val Loss: 0.2317, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.0295, Train Accuracy: 0.9692, Val Loss: 0.0210, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0200, Train Accuracy: 0.9802, Val Loss: 0.0089, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0171, Train Accuracy: 0.9802, Val Loss: 0.0064, Val Accuracy: 1.0000\n",
            "Epoch 401/1000 - Train Loss: 0.0154, Train Accuracy: 0.9824, Val Loss: 0.0052, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0143, Train Accuracy: 0.9824, Val Loss: 0.0045, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0134, Train Accuracy: 0.9846, Val Loss: 0.0041, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0127, Train Accuracy: 0.9846, Val Loss: 0.0039, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0121, Train Accuracy: 0.9846, Val Loss: 0.0037, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0116, Train Accuracy: 0.9846, Val Loss: 0.0037, Val Accuracy: 1.0000\n",
            "Early stopping at epoch 944 for learning rate 0.2\n",
            "Epoch 1/1000 - Train Loss: 0.2826, Train Accuracy: 0.3802, Val Loss: 0.2706, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0248, Train Accuracy: 0.9736, Val Loss: 0.0179, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0173, Train Accuracy: 0.9802, Val Loss: 0.0097, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0148, Train Accuracy: 0.9846, Val Loss: 0.0072, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0134, Train Accuracy: 0.9868, Val Loss: 0.0057, Val Accuracy: 1.0000\n",
            "Epoch 501/1000 - Train Loss: 0.0125, Train Accuracy: 0.9868, Val Loss: 0.0047, Val Accuracy: 1.0000\n",
            "Epoch 601/1000 - Train Loss: 0.0118, Train Accuracy: 0.9868, Val Loss: 0.0040, Val Accuracy: 1.0000\n",
            "Epoch 701/1000 - Train Loss: 0.0113, Train Accuracy: 0.9868, Val Loss: 0.0036, Val Accuracy: 1.0000\n",
            "Epoch 801/1000 - Train Loss: 0.0109, Train Accuracy: 0.9890, Val Loss: 0.0033, Val Accuracy: 1.0000\n",
            "Epoch 901/1000 - Train Loss: 0.0105, Train Accuracy: 0.9890, Val Loss: 0.0031, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3038, Train Accuracy: 0.2505, Val Loss: 0.3011, Val Accuracy: 0.2308\n",
            "Epoch 101/1000 - Train Loss: 0.2803, Train Accuracy: 0.3099, Val Loss: 0.2773, Val Accuracy: 0.2747\n",
            "Epoch 201/1000 - Train Loss: 0.2601, Train Accuracy: 0.4286, Val Loss: 0.2568, Val Accuracy: 0.4615\n",
            "Epoch 301/1000 - Train Loss: 0.2429, Train Accuracy: 0.6132, Val Loss: 0.2395, Val Accuracy: 0.6374\n",
            "Epoch 401/1000 - Train Loss: 0.2283, Train Accuracy: 0.7209, Val Loss: 0.2248, Val Accuracy: 0.7143\n",
            "Epoch 501/1000 - Train Loss: 0.2158, Train Accuracy: 0.7626, Val Loss: 0.2123, Val Accuracy: 0.7802\n",
            "Epoch 601/1000 - Train Loss: 0.2052, Train Accuracy: 0.7758, Val Loss: 0.2016, Val Accuracy: 0.7912\n",
            "Epoch 701/1000 - Train Loss: 0.1959, Train Accuracy: 0.7890, Val Loss: 0.1924, Val Accuracy: 0.8242\n",
            "Epoch 801/1000 - Train Loss: 0.1878, Train Accuracy: 0.7912, Val Loss: 0.1844, Val Accuracy: 0.8242\n",
            "Epoch 901/1000 - Train Loss: 0.1807, Train Accuracy: 0.8044, Val Loss: 0.1774, Val Accuracy: 0.8242\n",
            "Epoch 1/1000 - Train Loss: 0.2535, Train Accuracy: 0.6330, Val Loss: 0.2557, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.2428, Train Accuracy: 0.6308, Val Loss: 0.2466, Val Accuracy: 0.6593\n",
            "Epoch 201/1000 - Train Loss: 0.2324, Train Accuracy: 0.6352, Val Loss: 0.2377, Val Accuracy: 0.6593\n",
            "Epoch 301/1000 - Train Loss: 0.2223, Train Accuracy: 0.6462, Val Loss: 0.2289, Val Accuracy: 0.6593\n",
            "Epoch 401/1000 - Train Loss: 0.2126, Train Accuracy: 0.6527, Val Loss: 0.2203, Val Accuracy: 0.6813\n",
            "Epoch 501/1000 - Train Loss: 0.2034, Train Accuracy: 0.6659, Val Loss: 0.2120, Val Accuracy: 0.6923\n",
            "Epoch 601/1000 - Train Loss: 0.1946, Train Accuracy: 0.6769, Val Loss: 0.2040, Val Accuracy: 0.6923\n",
            "Epoch 701/1000 - Train Loss: 0.1864, Train Accuracy: 0.6879, Val Loss: 0.1963, Val Accuracy: 0.6923\n",
            "Epoch 801/1000 - Train Loss: 0.1786, Train Accuracy: 0.7055, Val Loss: 0.1890, Val Accuracy: 0.7143\n",
            "Epoch 901/1000 - Train Loss: 0.1714, Train Accuracy: 0.7341, Val Loss: 0.1821, Val Accuracy: 0.7253\n",
            "Epoch 1/1000 - Train Loss: 0.2925, Train Accuracy: 0.6132, Val Loss: 0.2644, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.2714, Train Accuracy: 0.6198, Val Loss: 0.2447, Val Accuracy: 0.6593\n",
            "Epoch 201/1000 - Train Loss: 0.2511, Train Accuracy: 0.6242, Val Loss: 0.2261, Val Accuracy: 0.6593\n",
            "Epoch 301/1000 - Train Loss: 0.2322, Train Accuracy: 0.6418, Val Loss: 0.2090, Val Accuracy: 0.6703\n",
            "Epoch 401/1000 - Train Loss: 0.2148, Train Accuracy: 0.6593, Val Loss: 0.1934, Val Accuracy: 0.7033\n",
            "Epoch 501/1000 - Train Loss: 0.1991, Train Accuracy: 0.6945, Val Loss: 0.1794, Val Accuracy: 0.7582\n",
            "Epoch 601/1000 - Train Loss: 0.1850, Train Accuracy: 0.7363, Val Loss: 0.1670, Val Accuracy: 0.7802\n",
            "Epoch 701/1000 - Train Loss: 0.1724, Train Accuracy: 0.7582, Val Loss: 0.1562, Val Accuracy: 0.7912\n",
            "Epoch 801/1000 - Train Loss: 0.1613, Train Accuracy: 0.7802, Val Loss: 0.1468, Val Accuracy: 0.8462\n",
            "Epoch 901/1000 - Train Loss: 0.1515, Train Accuracy: 0.7956, Val Loss: 0.1387, Val Accuracy: 0.8571\n",
            "Epoch 1/1000 - Train Loss: 0.4047, Train Accuracy: 0.2154, Val Loss: 0.4020, Val Accuracy: 0.1978\n",
            "Epoch 101/1000 - Train Loss: 0.3571, Train Accuracy: 0.2659, Val Loss: 0.3535, Val Accuracy: 0.2418\n",
            "Epoch 201/1000 - Train Loss: 0.3120, Train Accuracy: 0.3099, Val Loss: 0.3083, Val Accuracy: 0.2857\n",
            "Epoch 301/1000 - Train Loss: 0.2721, Train Accuracy: 0.4176, Val Loss: 0.2693, Val Accuracy: 0.4396\n",
            "Epoch 401/1000 - Train Loss: 0.2384, Train Accuracy: 0.5758, Val Loss: 0.2373, Val Accuracy: 0.6264\n",
            "Epoch 501/1000 - Train Loss: 0.2107, Train Accuracy: 0.7429, Val Loss: 0.2116, Val Accuracy: 0.7363\n",
            "Epoch 601/1000 - Train Loss: 0.1884, Train Accuracy: 0.8132, Val Loss: 0.1910, Val Accuracy: 0.8132\n",
            "Epoch 701/1000 - Train Loss: 0.1705, Train Accuracy: 0.8593, Val Loss: 0.1744, Val Accuracy: 0.8791\n",
            "Epoch 801/1000 - Train Loss: 0.1559, Train Accuracy: 0.8945, Val Loss: 0.1610, Val Accuracy: 0.9121\n",
            "Epoch 901/1000 - Train Loss: 0.1440, Train Accuracy: 0.9121, Val Loss: 0.1500, Val Accuracy: 0.9121\n",
            "Epoch 1/1000 - Train Loss: 0.3110, Train Accuracy: 0.1956, Val Loss: 0.2959, Val Accuracy: 0.2198\n",
            "Epoch 101/1000 - Train Loss: 0.2912, Train Accuracy: 0.2703, Val Loss: 0.2767, Val Accuracy: 0.3736\n",
            "Epoch 201/1000 - Train Loss: 0.2735, Train Accuracy: 0.3714, Val Loss: 0.2596, Val Accuracy: 0.4615\n",
            "Epoch 301/1000 - Train Loss: 0.2577, Train Accuracy: 0.4747, Val Loss: 0.2446, Val Accuracy: 0.5385\n",
            "Epoch 401/1000 - Train Loss: 0.2437, Train Accuracy: 0.5802, Val Loss: 0.2313, Val Accuracy: 0.6154\n",
            "Epoch 501/1000 - Train Loss: 0.2312, Train Accuracy: 0.6549, Val Loss: 0.2196, Val Accuracy: 0.6923\n",
            "Epoch 601/1000 - Train Loss: 0.2200, Train Accuracy: 0.7011, Val Loss: 0.2092, Val Accuracy: 0.7253\n",
            "Epoch 701/1000 - Train Loss: 0.2100, Train Accuracy: 0.7297, Val Loss: 0.1999, Val Accuracy: 0.7582\n",
            "Epoch 801/1000 - Train Loss: 0.2010, Train Accuracy: 0.7648, Val Loss: 0.1916, Val Accuracy: 0.8022\n",
            "Epoch 901/1000 - Train Loss: 0.1929, Train Accuracy: 0.7890, Val Loss: 0.1841, Val Accuracy: 0.8462\n",
            "Epoch 1/1000 - Train Loss: 0.2361, Train Accuracy: 0.6044, Val Loss: 0.2260, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.2262, Train Accuracy: 0.6615, Val Loss: 0.2162, Val Accuracy: 0.7033\n",
            "Epoch 201/1000 - Train Loss: 0.2172, Train Accuracy: 0.6989, Val Loss: 0.2072, Val Accuracy: 0.7253\n",
            "Epoch 301/1000 - Train Loss: 0.2088, Train Accuracy: 0.7231, Val Loss: 0.1989, Val Accuracy: 0.7363\n",
            "Epoch 401/1000 - Train Loss: 0.2010, Train Accuracy: 0.7670, Val Loss: 0.1912, Val Accuracy: 0.7802\n",
            "Epoch 501/1000 - Train Loss: 0.1937, Train Accuracy: 0.7868, Val Loss: 0.1841, Val Accuracy: 0.7912\n",
            "Epoch 601/1000 - Train Loss: 0.1869, Train Accuracy: 0.8000, Val Loss: 0.1775, Val Accuracy: 0.8022\n",
            "Epoch 701/1000 - Train Loss: 0.1805, Train Accuracy: 0.8132, Val Loss: 0.1713, Val Accuracy: 0.8132\n",
            "Epoch 801/1000 - Train Loss: 0.1744, Train Accuracy: 0.8176, Val Loss: 0.1655, Val Accuracy: 0.8242\n",
            "Epoch 901/1000 - Train Loss: 0.1688, Train Accuracy: 0.8286, Val Loss: 0.1601, Val Accuracy: 0.8242\n",
            "Epoch 1/1000 - Train Loss: 0.2380, Train Accuracy: 0.6352, Val Loss: 0.2235, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.2255, Train Accuracy: 0.6418, Val Loss: 0.2118, Val Accuracy: 0.6484\n",
            "Epoch 201/1000 - Train Loss: 0.2134, Train Accuracy: 0.6549, Val Loss: 0.2004, Val Accuracy: 0.6703\n",
            "Epoch 301/1000 - Train Loss: 0.2019, Train Accuracy: 0.6703, Val Loss: 0.1896, Val Accuracy: 0.6923\n",
            "Epoch 401/1000 - Train Loss: 0.1911, Train Accuracy: 0.6835, Val Loss: 0.1797, Val Accuracy: 0.7143\n",
            "Epoch 501/1000 - Train Loss: 0.1812, Train Accuracy: 0.7121, Val Loss: 0.1708, Val Accuracy: 0.7582\n",
            "Epoch 601/1000 - Train Loss: 0.1723, Train Accuracy: 0.7231, Val Loss: 0.1629, Val Accuracy: 0.7582\n",
            "Epoch 701/1000 - Train Loss: 0.1642, Train Accuracy: 0.7516, Val Loss: 0.1559, Val Accuracy: 0.7912\n",
            "Epoch 801/1000 - Train Loss: 0.1569, Train Accuracy: 0.7802, Val Loss: 0.1496, Val Accuracy: 0.8242\n",
            "Epoch 901/1000 - Train Loss: 0.1504, Train Accuracy: 0.7978, Val Loss: 0.1441, Val Accuracy: 0.8242\n",
            "Epoch 1/1000 - Train Loss: 0.2469, Train Accuracy: 0.6264, Val Loss: 0.2308, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.2345, Train Accuracy: 0.6286, Val Loss: 0.2197, Val Accuracy: 0.6593\n",
            "Epoch 201/1000 - Train Loss: 0.2224, Train Accuracy: 0.6308, Val Loss: 0.2087, Val Accuracy: 0.6593\n",
            "Epoch 301/1000 - Train Loss: 0.2108, Train Accuracy: 0.6374, Val Loss: 0.1979, Val Accuracy: 0.6593\n",
            "Epoch 401/1000 - Train Loss: 0.1999, Train Accuracy: 0.6440, Val Loss: 0.1878, Val Accuracy: 0.6593\n",
            "Epoch 501/1000 - Train Loss: 0.1899, Train Accuracy: 0.6571, Val Loss: 0.1785, Val Accuracy: 0.6813\n",
            "Epoch 601/1000 - Train Loss: 0.1808, Train Accuracy: 0.6857, Val Loss: 0.1701, Val Accuracy: 0.7253\n",
            "Epoch 701/1000 - Train Loss: 0.1726, Train Accuracy: 0.7143, Val Loss: 0.1626, Val Accuracy: 0.7473\n",
            "Epoch 801/1000 - Train Loss: 0.1653, Train Accuracy: 0.7473, Val Loss: 0.1560, Val Accuracy: 0.7582\n",
            "Epoch 901/1000 - Train Loss: 0.1587, Train Accuracy: 0.7736, Val Loss: 0.1501, Val Accuracy: 0.8022\n",
            "Epoch 1/1000 - Train Loss: 0.3170, Train Accuracy: 0.3648, Val Loss: 0.3184, Val Accuracy: 0.3407\n",
            "Epoch 101/1000 - Train Loss: 0.2957, Train Accuracy: 0.3692, Val Loss: 0.2948, Val Accuracy: 0.3516\n",
            "Epoch 201/1000 - Train Loss: 0.2774, Train Accuracy: 0.3912, Val Loss: 0.2747, Val Accuracy: 0.3956\n",
            "Epoch 301/1000 - Train Loss: 0.2618, Train Accuracy: 0.4527, Val Loss: 0.2575, Val Accuracy: 0.4505\n",
            "Epoch 401/1000 - Train Loss: 0.2485, Train Accuracy: 0.5670, Val Loss: 0.2430, Val Accuracy: 0.5604\n",
            "Epoch 501/1000 - Train Loss: 0.2371, Train Accuracy: 0.6264, Val Loss: 0.2307, Val Accuracy: 0.6154\n",
            "Epoch 601/1000 - Train Loss: 0.2274, Train Accuracy: 0.6791, Val Loss: 0.2201, Val Accuracy: 0.7033\n",
            "Epoch 701/1000 - Train Loss: 0.2189, Train Accuracy: 0.7121, Val Loss: 0.2110, Val Accuracy: 0.7253\n",
            "Epoch 801/1000 - Train Loss: 0.2115, Train Accuracy: 0.7275, Val Loss: 0.2031, Val Accuracy: 0.7692\n",
            "Epoch 901/1000 - Train Loss: 0.2050, Train Accuracy: 0.7363, Val Loss: 0.1962, Val Accuracy: 0.7802\n",
            "Epoch 1/1000 - Train Loss: 0.3540, Train Accuracy: 0.3099, Val Loss: 0.3277, Val Accuracy: 0.3626\n",
            "Epoch 101/1000 - Train Loss: 0.3202, Train Accuracy: 0.3824, Val Loss: 0.2956, Val Accuracy: 0.4396\n",
            "Epoch 201/1000 - Train Loss: 0.2900, Train Accuracy: 0.4527, Val Loss: 0.2669, Val Accuracy: 0.4945\n",
            "Epoch 301/1000 - Train Loss: 0.2635, Train Accuracy: 0.5363, Val Loss: 0.2418, Val Accuracy: 0.5824\n",
            "Epoch 401/1000 - Train Loss: 0.2405, Train Accuracy: 0.6110, Val Loss: 0.2205, Val Accuracy: 0.6703\n",
            "Epoch 501/1000 - Train Loss: 0.2207, Train Accuracy: 0.6769, Val Loss: 0.2025, Val Accuracy: 0.7473\n",
            "Epoch 601/1000 - Train Loss: 0.2038, Train Accuracy: 0.7297, Val Loss: 0.1873, Val Accuracy: 0.7912\n",
            "Epoch 701/1000 - Train Loss: 0.1892, Train Accuracy: 0.7780, Val Loss: 0.1744, Val Accuracy: 0.8242\n",
            "Epoch 801/1000 - Train Loss: 0.1765, Train Accuracy: 0.8198, Val Loss: 0.1634, Val Accuracy: 0.8462\n",
            "Epoch 901/1000 - Train Loss: 0.1656, Train Accuracy: 0.8396, Val Loss: 0.1540, Val Accuracy: 0.8571\n",
            "Epoch 1/1000 - Train Loss: 0.2337, Train Accuracy: 0.4879, Val Loss: 0.2434, Val Accuracy: 0.4066\n",
            "Epoch 101/1000 - Train Loss: 0.1736, Train Accuracy: 0.9099, Val Loss: 0.1811, Val Accuracy: 0.8791\n",
            "Epoch 201/1000 - Train Loss: 0.1382, Train Accuracy: 0.9099, Val Loss: 0.1470, Val Accuracy: 0.8791\n",
            "Epoch 301/1000 - Train Loss: 0.1102, Train Accuracy: 0.9297, Val Loss: 0.1208, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.0889, Train Accuracy: 0.9429, Val Loss: 0.1009, Val Accuracy: 0.9231\n",
            "Epoch 501/1000 - Train Loss: 0.0736, Train Accuracy: 0.9516, Val Loss: 0.0864, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0627, Train Accuracy: 0.9582, Val Loss: 0.0759, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0548, Train Accuracy: 0.9648, Val Loss: 0.0681, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0489, Train Accuracy: 0.9648, Val Loss: 0.0622, Val Accuracy: 0.9560\n",
            "Epoch 901/1000 - Train Loss: 0.0443, Train Accuracy: 0.9692, Val Loss: 0.0576, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2565, Train Accuracy: 0.6264, Val Loss: 0.2461, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.2148, Train Accuracy: 0.6352, Val Loss: 0.2114, Val Accuracy: 0.6813\n",
            "Epoch 201/1000 - Train Loss: 0.1791, Train Accuracy: 0.7846, Val Loss: 0.1804, Val Accuracy: 0.8132\n",
            "Epoch 301/1000 - Train Loss: 0.1481, Train Accuracy: 0.8659, Val Loss: 0.1523, Val Accuracy: 0.8352\n",
            "Epoch 401/1000 - Train Loss: 0.1224, Train Accuracy: 0.8945, Val Loss: 0.1281, Val Accuracy: 0.8791\n",
            "Epoch 501/1000 - Train Loss: 0.1020, Train Accuracy: 0.9055, Val Loss: 0.1087, Val Accuracy: 0.8791\n",
            "Epoch 601/1000 - Train Loss: 0.0865, Train Accuracy: 0.9121, Val Loss: 0.0937, Val Accuracy: 0.9121\n",
            "Epoch 701/1000 - Train Loss: 0.0748, Train Accuracy: 0.9275, Val Loss: 0.0825, Val Accuracy: 0.9231\n",
            "Epoch 801/1000 - Train Loss: 0.0659, Train Accuracy: 0.9319, Val Loss: 0.0739, Val Accuracy: 0.9341\n",
            "Epoch 901/1000 - Train Loss: 0.0588, Train Accuracy: 0.9341, Val Loss: 0.0672, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.2796, Train Accuracy: 0.6286, Val Loss: 0.2766, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.1926, Train Accuracy: 0.6505, Val Loss: 0.1943, Val Accuracy: 0.6593\n",
            "Epoch 201/1000 - Train Loss: 0.1451, Train Accuracy: 0.8352, Val Loss: 0.1500, Val Accuracy: 0.8242\n",
            "Epoch 301/1000 - Train Loss: 0.1164, Train Accuracy: 0.9143, Val Loss: 0.1228, Val Accuracy: 0.8681\n",
            "Epoch 401/1000 - Train Loss: 0.0963, Train Accuracy: 0.9275, Val Loss: 0.1038, Val Accuracy: 0.9231\n",
            "Epoch 501/1000 - Train Loss: 0.0814, Train Accuracy: 0.9363, Val Loss: 0.0900, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0702, Train Accuracy: 0.9429, Val Loss: 0.0797, Val Accuracy: 0.9231\n",
            "Epoch 701/1000 - Train Loss: 0.0617, Train Accuracy: 0.9473, Val Loss: 0.0718, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0551, Train Accuracy: 0.9604, Val Loss: 0.0657, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0499, Train Accuracy: 0.9670, Val Loss: 0.0608, Val Accuracy: 0.9560\n",
            "Epoch 1/1000 - Train Loss: 0.3864, Train Accuracy: 0.3714, Val Loss: 0.4090, Val Accuracy: 0.3407\n",
            "Epoch 101/1000 - Train Loss: 0.2250, Train Accuracy: 0.6330, Val Loss: 0.2334, Val Accuracy: 0.5714\n",
            "Epoch 201/1000 - Train Loss: 0.1740, Train Accuracy: 0.9385, Val Loss: 0.1783, Val Accuracy: 0.9121\n",
            "Epoch 301/1000 - Train Loss: 0.1420, Train Accuracy: 0.9407, Val Loss: 0.1461, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.1170, Train Accuracy: 0.9341, Val Loss: 0.1219, Val Accuracy: 0.9011\n",
            "Epoch 501/1000 - Train Loss: 0.0976, Train Accuracy: 0.9319, Val Loss: 0.1034, Val Accuracy: 0.9121\n",
            "Epoch 601/1000 - Train Loss: 0.0830, Train Accuracy: 0.9319, Val Loss: 0.0894, Val Accuracy: 0.9121\n",
            "Epoch 701/1000 - Train Loss: 0.0720, Train Accuracy: 0.9385, Val Loss: 0.0789, Val Accuracy: 0.9121\n",
            "Epoch 801/1000 - Train Loss: 0.0638, Train Accuracy: 0.9407, Val Loss: 0.0708, Val Accuracy: 0.9121\n",
            "Epoch 901/1000 - Train Loss: 0.0574, Train Accuracy: 0.9451, Val Loss: 0.0644, Val Accuracy: 0.9121\n",
            "Epoch 1/1000 - Train Loss: 0.3889, Train Accuracy: 0.2176, Val Loss: 0.3969, Val Accuracy: 0.1978\n",
            "Epoch 101/1000 - Train Loss: 0.2323, Train Accuracy: 0.6154, Val Loss: 0.2394, Val Accuracy: 0.5934\n",
            "Epoch 201/1000 - Train Loss: 0.1587, Train Accuracy: 0.8044, Val Loss: 0.1672, Val Accuracy: 0.7802\n",
            "Epoch 301/1000 - Train Loss: 0.1187, Train Accuracy: 0.8791, Val Loss: 0.1281, Val Accuracy: 0.8791\n",
            "Epoch 401/1000 - Train Loss: 0.0942, Train Accuracy: 0.9121, Val Loss: 0.1043, Val Accuracy: 0.9121\n",
            "Epoch 501/1000 - Train Loss: 0.0778, Train Accuracy: 0.9297, Val Loss: 0.0886, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0662, Train Accuracy: 0.9429, Val Loss: 0.0777, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0577, Train Accuracy: 0.9495, Val Loss: 0.0695, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0512, Train Accuracy: 0.9582, Val Loss: 0.0633, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0462, Train Accuracy: 0.9604, Val Loss: 0.0583, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.2120, Train Accuracy: 0.7560, Val Loss: 0.2073, Val Accuracy: 0.7692\n",
            "Epoch 101/1000 - Train Loss: 0.1501, Train Accuracy: 0.8484, Val Loss: 0.1437, Val Accuracy: 0.8462\n",
            "Epoch 201/1000 - Train Loss: 0.1242, Train Accuracy: 0.8659, Val Loss: 0.1197, Val Accuracy: 0.8571\n",
            "Epoch 301/1000 - Train Loss: 0.1058, Train Accuracy: 0.8813, Val Loss: 0.1036, Val Accuracy: 0.8901\n",
            "Epoch 401/1000 - Train Loss: 0.0916, Train Accuracy: 0.9033, Val Loss: 0.0914, Val Accuracy: 0.8901\n",
            "Epoch 501/1000 - Train Loss: 0.0804, Train Accuracy: 0.9077, Val Loss: 0.0819, Val Accuracy: 0.9011\n",
            "Epoch 601/1000 - Train Loss: 0.0715, Train Accuracy: 0.9231, Val Loss: 0.0744, Val Accuracy: 0.9121\n",
            "Epoch 701/1000 - Train Loss: 0.0641, Train Accuracy: 0.9341, Val Loss: 0.0683, Val Accuracy: 0.9231\n",
            "Epoch 801/1000 - Train Loss: 0.0580, Train Accuracy: 0.9385, Val Loss: 0.0631, Val Accuracy: 0.9231\n",
            "Epoch 901/1000 - Train Loss: 0.0527, Train Accuracy: 0.9385, Val Loss: 0.0586, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.2217, Train Accuracy: 0.6264, Val Loss: 0.2254, Val Accuracy: 0.5495\n",
            "Epoch 101/1000 - Train Loss: 0.1279, Train Accuracy: 0.9055, Val Loss: 0.1287, Val Accuracy: 0.9011\n",
            "Epoch 201/1000 - Train Loss: 0.0961, Train Accuracy: 0.9143, Val Loss: 0.0976, Val Accuracy: 0.8791\n",
            "Epoch 301/1000 - Train Loss: 0.0795, Train Accuracy: 0.9231, Val Loss: 0.0824, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.0690, Train Accuracy: 0.9275, Val Loss: 0.0731, Val Accuracy: 0.9231\n",
            "Epoch 501/1000 - Train Loss: 0.0616, Train Accuracy: 0.9319, Val Loss: 0.0666, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0560, Train Accuracy: 0.9385, Val Loss: 0.0615, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0515, Train Accuracy: 0.9429, Val Loss: 0.0574, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0479, Train Accuracy: 0.9473, Val Loss: 0.0539, Val Accuracy: 0.9560\n",
            "Epoch 901/1000 - Train Loss: 0.0448, Train Accuracy: 0.9473, Val Loss: 0.0509, Val Accuracy: 0.9560\n",
            "Epoch 1/1000 - Train Loss: 0.3776, Train Accuracy: 0.3714, Val Loss: 0.4116, Val Accuracy: 0.3407\n",
            "Epoch 101/1000 - Train Loss: 0.1677, Train Accuracy: 0.8659, Val Loss: 0.1835, Val Accuracy: 0.7912\n",
            "Epoch 201/1000 - Train Loss: 0.1245, Train Accuracy: 0.9495, Val Loss: 0.1348, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.1051, Train Accuracy: 0.9451, Val Loss: 0.1145, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.0911, Train Accuracy: 0.9473, Val Loss: 0.1006, Val Accuracy: 0.9231\n",
            "Epoch 501/1000 - Train Loss: 0.0799, Train Accuracy: 0.9516, Val Loss: 0.0897, Val Accuracy: 0.9231\n",
            "Epoch 601/1000 - Train Loss: 0.0707, Train Accuracy: 0.9516, Val Loss: 0.0808, Val Accuracy: 0.9231\n",
            "Epoch 701/1000 - Train Loss: 0.0632, Train Accuracy: 0.9560, Val Loss: 0.0734, Val Accuracy: 0.9231\n",
            "Epoch 801/1000 - Train Loss: 0.0570, Train Accuracy: 0.9538, Val Loss: 0.0673, Val Accuracy: 0.9231\n",
            "Epoch 901/1000 - Train Loss: 0.0518, Train Accuracy: 0.9560, Val Loss: 0.0621, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.2897, Train Accuracy: 0.6066, Val Loss: 0.2758, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.2036, Train Accuracy: 0.6352, Val Loss: 0.1977, Val Accuracy: 0.6593\n",
            "Epoch 201/1000 - Train Loss: 0.1526, Train Accuracy: 0.7868, Val Loss: 0.1492, Val Accuracy: 0.8022\n",
            "Epoch 301/1000 - Train Loss: 0.1228, Train Accuracy: 0.8769, Val Loss: 0.1220, Val Accuracy: 0.9011\n",
            "Epoch 401/1000 - Train Loss: 0.1021, Train Accuracy: 0.9055, Val Loss: 0.1036, Val Accuracy: 0.9121\n",
            "Epoch 501/1000 - Train Loss: 0.0870, Train Accuracy: 0.9187, Val Loss: 0.0902, Val Accuracy: 0.9231\n",
            "Epoch 601/1000 - Train Loss: 0.0756, Train Accuracy: 0.9363, Val Loss: 0.0801, Val Accuracy: 0.9231\n",
            "Epoch 701/1000 - Train Loss: 0.0666, Train Accuracy: 0.9429, Val Loss: 0.0722, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0595, Train Accuracy: 0.9473, Val Loss: 0.0659, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0537, Train Accuracy: 0.9516, Val Loss: 0.0608, Val Accuracy: 0.9560\n",
            "Epoch 1/1000 - Train Loss: 0.2590, Train Accuracy: 0.6286, Val Loss: 0.2417, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.1769, Train Accuracy: 0.6593, Val Loss: 0.1722, Val Accuracy: 0.6923\n",
            "Epoch 201/1000 - Train Loss: 0.1381, Train Accuracy: 0.8418, Val Loss: 0.1403, Val Accuracy: 0.8571\n",
            "Epoch 301/1000 - Train Loss: 0.1144, Train Accuracy: 0.9143, Val Loss: 0.1203, Val Accuracy: 0.8901\n",
            "Epoch 401/1000 - Train Loss: 0.0974, Train Accuracy: 0.9165, Val Loss: 0.1054, Val Accuracy: 0.8901\n",
            "Epoch 501/1000 - Train Loss: 0.0845, Train Accuracy: 0.9209, Val Loss: 0.0940, Val Accuracy: 0.8901\n",
            "Epoch 601/1000 - Train Loss: 0.0747, Train Accuracy: 0.9209, Val Loss: 0.0850, Val Accuracy: 0.8901\n",
            "Epoch 701/1000 - Train Loss: 0.0668, Train Accuracy: 0.9297, Val Loss: 0.0777, Val Accuracy: 0.9011\n",
            "Epoch 801/1000 - Train Loss: 0.0604, Train Accuracy: 0.9385, Val Loss: 0.0717, Val Accuracy: 0.9231\n",
            "Epoch 901/1000 - Train Loss: 0.0551, Train Accuracy: 0.9429, Val Loss: 0.0666, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.4445, Train Accuracy: 0.3714, Val Loss: 0.4614, Val Accuracy: 0.3407\n",
            "Epoch 101/1000 - Train Loss: 0.0669, Train Accuracy: 0.9473, Val Loss: 0.0690, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0403, Train Accuracy: 0.9648, Val Loss: 0.0458, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0300, Train Accuracy: 0.9758, Val Loss: 0.0356, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0249, Train Accuracy: 0.9824, Val Loss: 0.0301, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0220, Train Accuracy: 0.9802, Val Loss: 0.0269, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0201, Train Accuracy: 0.9780, Val Loss: 0.0248, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0188, Train Accuracy: 0.9802, Val Loss: 0.0233, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0178, Train Accuracy: 0.9802, Val Loss: 0.0222, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0170, Train Accuracy: 0.9802, Val Loss: 0.0212, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2224, Train Accuracy: 0.6308, Val Loss: 0.2174, Val Accuracy: 0.6374\n",
            "Epoch 101/1000 - Train Loss: 0.0767, Train Accuracy: 0.9209, Val Loss: 0.0803, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0473, Train Accuracy: 0.9495, Val Loss: 0.0544, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0344, Train Accuracy: 0.9626, Val Loss: 0.0410, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0278, Train Accuracy: 0.9714, Val Loss: 0.0330, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0240, Train Accuracy: 0.9736, Val Loss: 0.0281, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0217, Train Accuracy: 0.9758, Val Loss: 0.0251, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0201, Train Accuracy: 0.9802, Val Loss: 0.0231, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0190, Train Accuracy: 0.9802, Val Loss: 0.0217, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0182, Train Accuracy: 0.9802, Val Loss: 0.0207, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.3187, Train Accuracy: 0.3055, Val Loss: 0.2910, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0868, Train Accuracy: 0.9121, Val Loss: 0.0889, Val Accuracy: 0.8901\n",
            "Epoch 201/1000 - Train Loss: 0.0500, Train Accuracy: 0.9473, Val Loss: 0.0572, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0352, Train Accuracy: 0.9604, Val Loss: 0.0422, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0278, Train Accuracy: 0.9736, Val Loss: 0.0338, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0237, Train Accuracy: 0.9802, Val Loss: 0.0291, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0212, Train Accuracy: 0.9802, Val Loss: 0.0262, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0196, Train Accuracy: 0.9802, Val Loss: 0.0242, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0184, Train Accuracy: 0.9802, Val Loss: 0.0227, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0175, Train Accuracy: 0.9824, Val Loss: 0.0214, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2494, Train Accuracy: 0.6220, Val Loss: 0.2391, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.0791, Train Accuracy: 0.9341, Val Loss: 0.0856, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.0441, Train Accuracy: 0.9560, Val Loss: 0.0548, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0321, Train Accuracy: 0.9670, Val Loss: 0.0426, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0261, Train Accuracy: 0.9758, Val Loss: 0.0356, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0226, Train Accuracy: 0.9780, Val Loss: 0.0311, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0205, Train Accuracy: 0.9824, Val Loss: 0.0282, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0191, Train Accuracy: 0.9802, Val Loss: 0.0261, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0181, Train Accuracy: 0.9780, Val Loss: 0.0244, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0173, Train Accuracy: 0.9780, Val Loss: 0.0232, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2582, Train Accuracy: 0.6308, Val Loss: 0.2371, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.0833, Train Accuracy: 0.9165, Val Loss: 0.0863, Val Accuracy: 0.9011\n",
            "Epoch 201/1000 - Train Loss: 0.0496, Train Accuracy: 0.9582, Val Loss: 0.0568, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0362, Train Accuracy: 0.9670, Val Loss: 0.0439, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0296, Train Accuracy: 0.9758, Val Loss: 0.0370, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0258, Train Accuracy: 0.9736, Val Loss: 0.0327, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0234, Train Accuracy: 0.9780, Val Loss: 0.0298, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0217, Train Accuracy: 0.9802, Val Loss: 0.0275, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0204, Train Accuracy: 0.9824, Val Loss: 0.0256, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0193, Train Accuracy: 0.9846, Val Loss: 0.0239, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.1754, Train Accuracy: 0.7099, Val Loss: 0.1703, Val Accuracy: 0.7253\n",
            "Epoch 101/1000 - Train Loss: 0.0675, Train Accuracy: 0.9209, Val Loss: 0.0687, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0462, Train Accuracy: 0.9516, Val Loss: 0.0493, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0352, Train Accuracy: 0.9582, Val Loss: 0.0401, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0277, Train Accuracy: 0.9648, Val Loss: 0.0352, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0237, Train Accuracy: 0.9758, Val Loss: 0.0323, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0214, Train Accuracy: 0.9824, Val Loss: 0.0304, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0199, Train Accuracy: 0.9824, Val Loss: 0.0289, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0189, Train Accuracy: 0.9824, Val Loss: 0.0277, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0181, Train Accuracy: 0.9824, Val Loss: 0.0266, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.4158, Train Accuracy: 0.2462, Val Loss: 0.4384, Val Accuracy: 0.2088\n",
            "Epoch 101/1000 - Train Loss: 0.0656, Train Accuracy: 0.9341, Val Loss: 0.0804, Val Accuracy: 0.9011\n",
            "Epoch 201/1000 - Train Loss: 0.0418, Train Accuracy: 0.9604, Val Loss: 0.0536, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0310, Train Accuracy: 0.9714, Val Loss: 0.0402, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0251, Train Accuracy: 0.9780, Val Loss: 0.0326, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0218, Train Accuracy: 0.9824, Val Loss: 0.0280, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0198, Train Accuracy: 0.9824, Val Loss: 0.0251, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0185, Train Accuracy: 0.9824, Val Loss: 0.0230, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0175, Train Accuracy: 0.9824, Val Loss: 0.0215, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0167, Train Accuracy: 0.9824, Val Loss: 0.0202, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2313, Train Accuracy: 0.6593, Val Loss: 0.2119, Val Accuracy: 0.7143\n",
            "Epoch 101/1000 - Train Loss: 0.0594, Train Accuracy: 0.9363, Val Loss: 0.0654, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0393, Train Accuracy: 0.9604, Val Loss: 0.0467, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0306, Train Accuracy: 0.9670, Val Loss: 0.0374, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0258, Train Accuracy: 0.9714, Val Loss: 0.0318, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0228, Train Accuracy: 0.9802, Val Loss: 0.0282, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0208, Train Accuracy: 0.9802, Val Loss: 0.0257, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0194, Train Accuracy: 0.9824, Val Loss: 0.0238, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0183, Train Accuracy: 0.9824, Val Loss: 0.0224, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0175, Train Accuracy: 0.9824, Val Loss: 0.0213, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.1865, Train Accuracy: 0.7780, Val Loss: 0.1866, Val Accuracy: 0.7802\n",
            "Epoch 101/1000 - Train Loss: 0.0696, Train Accuracy: 0.9253, Val Loss: 0.0738, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.0470, Train Accuracy: 0.9451, Val Loss: 0.0531, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0362, Train Accuracy: 0.9626, Val Loss: 0.0425, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0298, Train Accuracy: 0.9648, Val Loss: 0.0355, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0258, Train Accuracy: 0.9692, Val Loss: 0.0306, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0231, Train Accuracy: 0.9692, Val Loss: 0.0271, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0212, Train Accuracy: 0.9780, Val Loss: 0.0246, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0198, Train Accuracy: 0.9780, Val Loss: 0.0227, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0187, Train Accuracy: 0.9780, Val Loss: 0.0212, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.3029, Train Accuracy: 0.6264, Val Loss: 0.2744, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.0743, Train Accuracy: 0.9319, Val Loss: 0.0835, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.0451, Train Accuracy: 0.9516, Val Loss: 0.0588, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0345, Train Accuracy: 0.9648, Val Loss: 0.0483, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0293, Train Accuracy: 0.9692, Val Loss: 0.0423, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0263, Train Accuracy: 0.9692, Val Loss: 0.0383, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0243, Train Accuracy: 0.9692, Val Loss: 0.0350, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0226, Train Accuracy: 0.9736, Val Loss: 0.0319, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0213, Train Accuracy: 0.9736, Val Loss: 0.0292, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0202, Train Accuracy: 0.9758, Val Loss: 0.0270, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.1990, Train Accuracy: 0.7582, Val Loss: 0.1898, Val Accuracy: 0.8352\n",
            "Epoch 101/1000 - Train Loss: 0.0516, Train Accuracy: 0.9473, Val Loss: 0.0582, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0288, Train Accuracy: 0.9692, Val Loss: 0.0335, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0214, Train Accuracy: 0.9846, Val Loss: 0.0240, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0183, Train Accuracy: 0.9846, Val Loss: 0.0199, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0166, Train Accuracy: 0.9846, Val Loss: 0.0177, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0155, Train Accuracy: 0.9868, Val Loss: 0.0163, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0147, Train Accuracy: 0.9868, Val Loss: 0.0153, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0141, Train Accuracy: 0.9868, Val Loss: 0.0146, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0135, Train Accuracy: 0.9868, Val Loss: 0.0140, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2263, Train Accuracy: 0.6505, Val Loss: 0.2239, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0460, Train Accuracy: 0.9429, Val Loss: 0.0564, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0304, Train Accuracy: 0.9604, Val Loss: 0.0375, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0244, Train Accuracy: 0.9670, Val Loss: 0.0291, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0212, Train Accuracy: 0.9758, Val Loss: 0.0248, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0191, Train Accuracy: 0.9780, Val Loss: 0.0223, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0177, Train Accuracy: 0.9780, Val Loss: 0.0206, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0166, Train Accuracy: 0.9802, Val Loss: 0.0194, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0157, Train Accuracy: 0.9802, Val Loss: 0.0184, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0150, Train Accuracy: 0.9802, Val Loss: 0.0176, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2511, Train Accuracy: 0.4286, Val Loss: 0.2620, Val Accuracy: 0.4286\n",
            "Epoch 101/1000 - Train Loss: 0.0402, Train Accuracy: 0.9538, Val Loss: 0.0499, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0242, Train Accuracy: 0.9780, Val Loss: 0.0307, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0195, Train Accuracy: 0.9802, Val Loss: 0.0238, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0173, Train Accuracy: 0.9846, Val Loss: 0.0205, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0160, Train Accuracy: 0.9846, Val Loss: 0.0185, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0151, Train Accuracy: 0.9846, Val Loss: 0.0171, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0143, Train Accuracy: 0.9846, Val Loss: 0.0161, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0138, Train Accuracy: 0.9846, Val Loss: 0.0153, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0133, Train Accuracy: 0.9846, Val Loss: 0.0147, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2476, Train Accuracy: 0.5033, Val Loss: 0.2607, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0346, Train Accuracy: 0.9714, Val Loss: 0.0440, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0243, Train Accuracy: 0.9780, Val Loss: 0.0314, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0207, Train Accuracy: 0.9802, Val Loss: 0.0260, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0187, Train Accuracy: 0.9802, Val Loss: 0.0229, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0173, Train Accuracy: 0.9824, Val Loss: 0.0207, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0162, Train Accuracy: 0.9824, Val Loss: 0.0190, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0153, Train Accuracy: 0.9846, Val Loss: 0.0176, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0146, Train Accuracy: 0.9846, Val Loss: 0.0164, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0140, Train Accuracy: 0.9846, Val Loss: 0.0154, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2785, Train Accuracy: 0.5824, Val Loss: 0.2871, Val Accuracy: 0.5714\n",
            "Epoch 101/1000 - Train Loss: 0.0411, Train Accuracy: 0.9560, Val Loss: 0.0569, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0260, Train Accuracy: 0.9736, Val Loss: 0.0348, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0210, Train Accuracy: 0.9802, Val Loss: 0.0270, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0186, Train Accuracy: 0.9802, Val Loss: 0.0236, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0172, Train Accuracy: 0.9802, Val Loss: 0.0216, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0161, Train Accuracy: 0.9824, Val Loss: 0.0203, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0153, Train Accuracy: 0.9846, Val Loss: 0.0193, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0147, Train Accuracy: 0.9846, Val Loss: 0.0185, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0141, Train Accuracy: 0.9846, Val Loss: 0.0177, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2982, Train Accuracy: 0.4044, Val Loss: 0.3141, Val Accuracy: 0.3187\n",
            "Epoch 101/1000 - Train Loss: 0.0325, Train Accuracy: 0.9670, Val Loss: 0.0399, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0232, Train Accuracy: 0.9758, Val Loss: 0.0276, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0196, Train Accuracy: 0.9780, Val Loss: 0.0222, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0175, Train Accuracy: 0.9780, Val Loss: 0.0191, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0161, Train Accuracy: 0.9802, Val Loss: 0.0172, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0151, Train Accuracy: 0.9846, Val Loss: 0.0158, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0143, Train Accuracy: 0.9846, Val Loss: 0.0148, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0137, Train Accuracy: 0.9846, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0131, Train Accuracy: 0.9846, Val Loss: 0.0135, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2239, Train Accuracy: 0.6747, Val Loss: 0.2364, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0334, Train Accuracy: 0.9692, Val Loss: 0.0441, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0224, Train Accuracy: 0.9802, Val Loss: 0.0298, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0188, Train Accuracy: 0.9846, Val Loss: 0.0245, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0170, Train Accuracy: 0.9824, Val Loss: 0.0216, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0158, Train Accuracy: 0.9824, Val Loss: 0.0196, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0150, Train Accuracy: 0.9824, Val Loss: 0.0182, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0143, Train Accuracy: 0.9824, Val Loss: 0.0171, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0138, Train Accuracy: 0.9824, Val Loss: 0.0162, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0133, Train Accuracy: 0.9824, Val Loss: 0.0156, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.1312, Train Accuracy: 0.8945, Val Loss: 0.1206, Val Accuracy: 0.9451\n",
            "Epoch 101/1000 - Train Loss: 0.0362, Train Accuracy: 0.9714, Val Loss: 0.0448, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0241, Train Accuracy: 0.9736, Val Loss: 0.0330, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0198, Train Accuracy: 0.9802, Val Loss: 0.0278, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0176, Train Accuracy: 0.9824, Val Loss: 0.0245, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0161, Train Accuracy: 0.9824, Val Loss: 0.0220, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0151, Train Accuracy: 0.9824, Val Loss: 0.0201, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0142, Train Accuracy: 0.9824, Val Loss: 0.0186, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0174, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0130, Train Accuracy: 0.9868, Val Loss: 0.0165, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.4278, Train Accuracy: 0.3714, Val Loss: 0.4539, Val Accuracy: 0.3407\n",
            "Epoch 101/1000 - Train Loss: 0.0463, Train Accuracy: 0.9604, Val Loss: 0.0562, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0277, Train Accuracy: 0.9692, Val Loss: 0.0354, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0218, Train Accuracy: 0.9780, Val Loss: 0.0274, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0190, Train Accuracy: 0.9824, Val Loss: 0.0234, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0174, Train Accuracy: 0.9846, Val Loss: 0.0208, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0162, Train Accuracy: 0.9846, Val Loss: 0.0191, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0154, Train Accuracy: 0.9824, Val Loss: 0.0177, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0147, Train Accuracy: 0.9846, Val Loss: 0.0166, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0141, Train Accuracy: 0.9846, Val Loss: 0.0157, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3230, Train Accuracy: 0.2527, Val Loss: 0.3058, Val Accuracy: 0.3187\n",
            "Epoch 101/1000 - Train Loss: 0.0392, Train Accuracy: 0.9626, Val Loss: 0.0451, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0235, Train Accuracy: 0.9758, Val Loss: 0.0257, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0189, Train Accuracy: 0.9802, Val Loss: 0.0189, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0167, Train Accuracy: 0.9824, Val Loss: 0.0159, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0155, Train Accuracy: 0.9824, Val Loss: 0.0143, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0146, Train Accuracy: 0.9846, Val Loss: 0.0133, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0139, Train Accuracy: 0.9846, Val Loss: 0.0127, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0134, Train Accuracy: 0.9846, Val Loss: 0.0122, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0129, Train Accuracy: 0.9846, Val Loss: 0.0119, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.4756, Train Accuracy: 0.3714, Val Loss: 0.5095, Val Accuracy: 0.3407\n",
            "Epoch 101/1000 - Train Loss: 0.0319, Train Accuracy: 0.9670, Val Loss: 0.0406, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0209, Train Accuracy: 0.9846, Val Loss: 0.0252, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0173, Train Accuracy: 0.9846, Val Loss: 0.0198, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0154, Train Accuracy: 0.9846, Val Loss: 0.0171, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0142, Train Accuracy: 0.9846, Val Loss: 0.0154, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0132, Train Accuracy: 0.9846, Val Loss: 0.0143, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0125, Train Accuracy: 0.9846, Val Loss: 0.0135, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0128, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0123, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2412, Train Accuracy: 0.6264, Val Loss: 0.2365, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0382, Train Accuracy: 0.9604, Val Loss: 0.0483, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0227, Train Accuracy: 0.9736, Val Loss: 0.0307, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0186, Train Accuracy: 0.9802, Val Loss: 0.0251, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0164, Train Accuracy: 0.9824, Val Loss: 0.0217, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0150, Train Accuracy: 0.9846, Val Loss: 0.0190, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0138, Train Accuracy: 0.9824, Val Loss: 0.0170, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0129, Train Accuracy: 0.9824, Val Loss: 0.0154, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0122, Train Accuracy: 0.9824, Val Loss: 0.0140, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0129, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2428, Train Accuracy: 0.4879, Val Loss: 0.2568, Val Accuracy: 0.4176\n",
            "Epoch 101/1000 - Train Loss: 0.0239, Train Accuracy: 0.9780, Val Loss: 0.0293, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0180, Train Accuracy: 0.9802, Val Loss: 0.0230, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0160, Train Accuracy: 0.9802, Val Loss: 0.0197, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0171, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0153, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0128, Train Accuracy: 0.9846, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0122, Train Accuracy: 0.9846, Val Loss: 0.0132, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0116, Train Accuracy: 0.9846, Val Loss: 0.0125, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0112, Train Accuracy: 0.9846, Val Loss: 0.0120, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3900, Train Accuracy: 0.2901, Val Loss: 0.3610, Val Accuracy: 0.4066\n",
            "Epoch 101/1000 - Train Loss: 0.0235, Train Accuracy: 0.9780, Val Loss: 0.0317, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0170, Train Accuracy: 0.9824, Val Loss: 0.0241, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0149, Train Accuracy: 0.9824, Val Loss: 0.0210, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0137, Train Accuracy: 0.9824, Val Loss: 0.0186, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0127, Train Accuracy: 0.9824, Val Loss: 0.0165, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0150, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0113, Train Accuracy: 0.9868, Val Loss: 0.0138, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0108, Train Accuracy: 0.9890, Val Loss: 0.0130, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0104, Train Accuracy: 0.9890, Val Loss: 0.0123, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3119, Train Accuracy: 0.3451, Val Loss: 0.3260, Val Accuracy: 0.2967\n",
            "Epoch 101/1000 - Train Loss: 0.0300, Train Accuracy: 0.9736, Val Loss: 0.0413, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0195, Train Accuracy: 0.9802, Val Loss: 0.0273, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0165, Train Accuracy: 0.9824, Val Loss: 0.0226, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0148, Train Accuracy: 0.9824, Val Loss: 0.0191, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0135, Train Accuracy: 0.9824, Val Loss: 0.0166, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0126, Train Accuracy: 0.9846, Val Loss: 0.0150, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0139, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0114, Train Accuracy: 0.9846, Val Loss: 0.0131, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0110, Train Accuracy: 0.9868, Val Loss: 0.0125, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2499, Train Accuracy: 0.5868, Val Loss: 0.2347, Val Accuracy: 0.6374\n",
            "Epoch 101/1000 - Train Loss: 0.0215, Train Accuracy: 0.9824, Val Loss: 0.0276, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0168, Train Accuracy: 0.9824, Val Loss: 0.0197, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0148, Train Accuracy: 0.9846, Val Loss: 0.0168, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0151, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0127, Train Accuracy: 0.9846, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0133, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0127, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0110, Train Accuracy: 0.9846, Val Loss: 0.0123, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0106, Train Accuracy: 0.9868, Val Loss: 0.0119, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2965, Train Accuracy: 0.6264, Val Loss: 0.2791, Val Accuracy: 0.6593\n",
            "Epoch 101/1000 - Train Loss: 0.0235, Train Accuracy: 0.9824, Val Loss: 0.0312, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0169, Train Accuracy: 0.9802, Val Loss: 0.0214, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0144, Train Accuracy: 0.9824, Val Loss: 0.0173, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0129, Train Accuracy: 0.9824, Val Loss: 0.0148, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0133, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0113, Train Accuracy: 0.9868, Val Loss: 0.0123, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0107, Train Accuracy: 0.9890, Val Loss: 0.0116, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0103, Train Accuracy: 0.9890, Val Loss: 0.0111, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0100, Train Accuracy: 0.9890, Val Loss: 0.0107, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3072, Train Accuracy: 0.3473, Val Loss: 0.3050, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.0291, Train Accuracy: 0.9648, Val Loss: 0.0323, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0197, Train Accuracy: 0.9780, Val Loss: 0.0222, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0165, Train Accuracy: 0.9868, Val Loss: 0.0181, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0147, Train Accuracy: 0.9868, Val Loss: 0.0156, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0136, Train Accuracy: 0.9868, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0127, Train Accuracy: 0.9868, Val Loss: 0.0132, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0120, Train Accuracy: 0.9868, Val Loss: 0.0125, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0115, Train Accuracy: 0.9868, Val Loss: 0.0120, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0110, Train Accuracy: 0.9890, Val Loss: 0.0116, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2321, Train Accuracy: 0.6330, Val Loss: 0.2109, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0235, Train Accuracy: 0.9758, Val Loss: 0.0287, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0176, Train Accuracy: 0.9824, Val Loss: 0.0203, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0152, Train Accuracy: 0.9846, Val Loss: 0.0170, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0138, Train Accuracy: 0.9846, Val Loss: 0.0152, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0129, Train Accuracy: 0.9846, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0122, Train Accuracy: 0.9846, Val Loss: 0.0133, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0117, Train Accuracy: 0.9846, Val Loss: 0.0128, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0112, Train Accuracy: 0.9846, Val Loss: 0.0124, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0109, Train Accuracy: 0.9846, Val Loss: 0.0120, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.1734, Train Accuracy: 0.6549, Val Loss: 0.1661, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0252, Train Accuracy: 0.9714, Val Loss: 0.0351, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0191, Train Accuracy: 0.9780, Val Loss: 0.0249, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0164, Train Accuracy: 0.9802, Val Loss: 0.0198, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0147, Train Accuracy: 0.9824, Val Loss: 0.0170, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0153, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0127, Train Accuracy: 0.9846, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0120, Train Accuracy: 0.9846, Val Loss: 0.0133, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0126, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0110, Train Accuracy: 0.9868, Val Loss: 0.0122, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.4191, Train Accuracy: 0.3714, Val Loss: 0.4374, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.3659, Train Accuracy: 0.3714, Val Loss: 0.3765, Val Accuracy: 0.3297\n",
            "Epoch 201/1000 - Train Loss: 0.3184, Train Accuracy: 0.3758, Val Loss: 0.3228, Val Accuracy: 0.3407\n",
            "Epoch 301/1000 - Train Loss: 0.2784, Train Accuracy: 0.4154, Val Loss: 0.2784, Val Accuracy: 0.4066\n",
            "Epoch 401/1000 - Train Loss: 0.2452, Train Accuracy: 0.4857, Val Loss: 0.2423, Val Accuracy: 0.5165\n",
            "Epoch 501/1000 - Train Loss: 0.2179, Train Accuracy: 0.5912, Val Loss: 0.2131, Val Accuracy: 0.6264\n",
            "Epoch 601/1000 - Train Loss: 0.1955, Train Accuracy: 0.6681, Val Loss: 0.1895, Val Accuracy: 0.7143\n",
            "Epoch 701/1000 - Train Loss: 0.1770, Train Accuracy: 0.7407, Val Loss: 0.1702, Val Accuracy: 0.7692\n",
            "Epoch 801/1000 - Train Loss: 0.1617, Train Accuracy: 0.7934, Val Loss: 0.1545, Val Accuracy: 0.8242\n",
            "Epoch 901/1000 - Train Loss: 0.1490, Train Accuracy: 0.8396, Val Loss: 0.1416, Val Accuracy: 0.8901\n",
            "Epoch 1/1000 - Train Loss: 0.3091, Train Accuracy: 0.6286, Val Loss: 0.2811, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.2876, Train Accuracy: 0.6286, Val Loss: 0.2615, Val Accuracy: 0.6703\n",
            "Epoch 201/1000 - Train Loss: 0.2629, Train Accuracy: 0.6286, Val Loss: 0.2399, Val Accuracy: 0.6703\n",
            "Epoch 301/1000 - Train Loss: 0.2375, Train Accuracy: 0.6308, Val Loss: 0.2183, Val Accuracy: 0.6703\n",
            "Epoch 401/1000 - Train Loss: 0.2138, Train Accuracy: 0.6396, Val Loss: 0.1983, Val Accuracy: 0.6813\n",
            "Epoch 501/1000 - Train Loss: 0.1933, Train Accuracy: 0.6615, Val Loss: 0.1807, Val Accuracy: 0.6813\n",
            "Epoch 601/1000 - Train Loss: 0.1758, Train Accuracy: 0.6857, Val Loss: 0.1654, Val Accuracy: 0.6813\n",
            "Epoch 701/1000 - Train Loss: 0.1611, Train Accuracy: 0.7143, Val Loss: 0.1524, Val Accuracy: 0.6923\n",
            "Epoch 801/1000 - Train Loss: 0.1486, Train Accuracy: 0.7516, Val Loss: 0.1412, Val Accuracy: 0.7473\n",
            "Epoch 901/1000 - Train Loss: 0.1380, Train Accuracy: 0.7934, Val Loss: 0.1316, Val Accuracy: 0.7912\n",
            "Epoch 1/1000 - Train Loss: 0.4276, Train Accuracy: 0.3714, Val Loss: 0.4480, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.3902, Train Accuracy: 0.3714, Val Loss: 0.4062, Val Accuracy: 0.3297\n",
            "Epoch 201/1000 - Train Loss: 0.3541, Train Accuracy: 0.3714, Val Loss: 0.3661, Val Accuracy: 0.3297\n",
            "Epoch 301/1000 - Train Loss: 0.3206, Train Accuracy: 0.3714, Val Loss: 0.3289, Val Accuracy: 0.3297\n",
            "Epoch 401/1000 - Train Loss: 0.2902, Train Accuracy: 0.3780, Val Loss: 0.2954, Val Accuracy: 0.3407\n",
            "Epoch 501/1000 - Train Loss: 0.2634, Train Accuracy: 0.3868, Val Loss: 0.2659, Val Accuracy: 0.3626\n",
            "Epoch 601/1000 - Train Loss: 0.2401, Train Accuracy: 0.4264, Val Loss: 0.2403, Val Accuracy: 0.3956\n",
            "Epoch 701/1000 - Train Loss: 0.2201, Train Accuracy: 0.5231, Val Loss: 0.2183, Val Accuracy: 0.5055\n",
            "Epoch 801/1000 - Train Loss: 0.2030, Train Accuracy: 0.6308, Val Loss: 0.1996, Val Accuracy: 0.6044\n",
            "Epoch 901/1000 - Train Loss: 0.1885, Train Accuracy: 0.7143, Val Loss: 0.1838, Val Accuracy: 0.7473\n",
            "Epoch 1/1000 - Train Loss: 0.3785, Train Accuracy: 0.3714, Val Loss: 0.3920, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.3339, Train Accuracy: 0.3714, Val Loss: 0.3422, Val Accuracy: 0.3297\n",
            "Epoch 201/1000 - Train Loss: 0.2936, Train Accuracy: 0.3714, Val Loss: 0.2975, Val Accuracy: 0.3187\n",
            "Epoch 301/1000 - Train Loss: 0.2586, Train Accuracy: 0.4637, Val Loss: 0.2590, Val Accuracy: 0.4286\n",
            "Epoch 401/1000 - Train Loss: 0.2291, Train Accuracy: 0.6440, Val Loss: 0.2268, Val Accuracy: 0.6484\n",
            "Epoch 501/1000 - Train Loss: 0.2047, Train Accuracy: 0.7363, Val Loss: 0.2005, Val Accuracy: 0.7692\n",
            "Epoch 601/1000 - Train Loss: 0.1848, Train Accuracy: 0.7868, Val Loss: 0.1792, Val Accuracy: 0.8022\n",
            "Epoch 701/1000 - Train Loss: 0.1686, Train Accuracy: 0.8220, Val Loss: 0.1621, Val Accuracy: 0.8571\n",
            "Epoch 801/1000 - Train Loss: 0.1553, Train Accuracy: 0.8374, Val Loss: 0.1482, Val Accuracy: 0.8681\n",
            "Epoch 901/1000 - Train Loss: 0.1445, Train Accuracy: 0.8637, Val Loss: 0.1369, Val Accuracy: 0.8791\n",
            "Epoch 1/1000 - Train Loss: 0.2895, Train Accuracy: 0.6088, Val Loss: 0.2641, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.2748, Train Accuracy: 0.6000, Val Loss: 0.2520, Val Accuracy: 0.6484\n",
            "Epoch 201/1000 - Train Loss: 0.2601, Train Accuracy: 0.6022, Val Loss: 0.2398, Val Accuracy: 0.6484\n",
            "Epoch 301/1000 - Train Loss: 0.2458, Train Accuracy: 0.6000, Val Loss: 0.2280, Val Accuracy: 0.6484\n",
            "Epoch 401/1000 - Train Loss: 0.2323, Train Accuracy: 0.6066, Val Loss: 0.2167, Val Accuracy: 0.6593\n",
            "Epoch 501/1000 - Train Loss: 0.2198, Train Accuracy: 0.6132, Val Loss: 0.2062, Val Accuracy: 0.6703\n",
            "Epoch 601/1000 - Train Loss: 0.2085, Train Accuracy: 0.6220, Val Loss: 0.1964, Val Accuracy: 0.6703\n",
            "Epoch 701/1000 - Train Loss: 0.1982, Train Accuracy: 0.6308, Val Loss: 0.1875, Val Accuracy: 0.6813\n",
            "Epoch 801/1000 - Train Loss: 0.1890, Train Accuracy: 0.6571, Val Loss: 0.1794, Val Accuracy: 0.6813\n",
            "Epoch 901/1000 - Train Loss: 0.1807, Train Accuracy: 0.6725, Val Loss: 0.1719, Val Accuracy: 0.6813\n",
            "Epoch 1/1000 - Train Loss: 0.5052, Train Accuracy: 0.3714, Val Loss: 0.5394, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.4722, Train Accuracy: 0.3714, Val Loss: 0.5030, Val Accuracy: 0.3297\n",
            "Epoch 201/1000 - Train Loss: 0.4372, Train Accuracy: 0.3714, Val Loss: 0.4643, Val Accuracy: 0.3297\n",
            "Epoch 301/1000 - Train Loss: 0.4019, Train Accuracy: 0.3714, Val Loss: 0.4251, Val Accuracy: 0.3297\n",
            "Epoch 401/1000 - Train Loss: 0.3679, Train Accuracy: 0.3714, Val Loss: 0.3871, Val Accuracy: 0.3297\n",
            "Epoch 501/1000 - Train Loss: 0.3363, Train Accuracy: 0.3714, Val Loss: 0.3518, Val Accuracy: 0.3297\n",
            "Epoch 601/1000 - Train Loss: 0.3079, Train Accuracy: 0.3736, Val Loss: 0.3199, Val Accuracy: 0.3297\n",
            "Epoch 701/1000 - Train Loss: 0.2829, Train Accuracy: 0.3758, Val Loss: 0.2919, Val Accuracy: 0.3407\n",
            "Epoch 801/1000 - Train Loss: 0.2612, Train Accuracy: 0.3890, Val Loss: 0.2676, Val Accuracy: 0.3626\n",
            "Epoch 901/1000 - Train Loss: 0.2427, Train Accuracy: 0.4264, Val Loss: 0.2468, Val Accuracy: 0.4066\n",
            "Epoch 1/1000 - Train Loss: 0.2619, Train Accuracy: 0.4286, Val Loss: 0.2587, Val Accuracy: 0.4286\n",
            "Epoch 101/1000 - Train Loss: 0.2439, Train Accuracy: 0.4989, Val Loss: 0.2395, Val Accuracy: 0.4835\n",
            "Epoch 201/1000 - Train Loss: 0.2279, Train Accuracy: 0.5626, Val Loss: 0.2227, Val Accuracy: 0.6154\n",
            "Epoch 301/1000 - Train Loss: 0.2140, Train Accuracy: 0.6022, Val Loss: 0.2081, Val Accuracy: 0.6703\n",
            "Epoch 401/1000 - Train Loss: 0.2019, Train Accuracy: 0.6769, Val Loss: 0.1954, Val Accuracy: 0.6923\n",
            "Epoch 501/1000 - Train Loss: 0.1914, Train Accuracy: 0.7275, Val Loss: 0.1844, Val Accuracy: 0.7802\n",
            "Epoch 601/1000 - Train Loss: 0.1822, Train Accuracy: 0.7758, Val Loss: 0.1749, Val Accuracy: 0.8132\n",
            "Epoch 701/1000 - Train Loss: 0.1743, Train Accuracy: 0.8110, Val Loss: 0.1666, Val Accuracy: 0.8352\n",
            "Epoch 801/1000 - Train Loss: 0.1673, Train Accuracy: 0.8264, Val Loss: 0.1594, Val Accuracy: 0.8571\n",
            "Epoch 901/1000 - Train Loss: 0.1611, Train Accuracy: 0.8308, Val Loss: 0.1530, Val Accuracy: 0.8571\n",
            "Epoch 1/1000 - Train Loss: 0.2309, Train Accuracy: 0.6286, Val Loss: 0.2044, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.2240, Train Accuracy: 0.6286, Val Loss: 0.1986, Val Accuracy: 0.6703\n",
            "Epoch 201/1000 - Train Loss: 0.2176, Train Accuracy: 0.6308, Val Loss: 0.1931, Val Accuracy: 0.6703\n",
            "Epoch 301/1000 - Train Loss: 0.2116, Train Accuracy: 0.6308, Val Loss: 0.1880, Val Accuracy: 0.6703\n",
            "Epoch 401/1000 - Train Loss: 0.2059, Train Accuracy: 0.6352, Val Loss: 0.1833, Val Accuracy: 0.6813\n",
            "Epoch 501/1000 - Train Loss: 0.2006, Train Accuracy: 0.6374, Val Loss: 0.1788, Val Accuracy: 0.6813\n",
            "Epoch 601/1000 - Train Loss: 0.1956, Train Accuracy: 0.6396, Val Loss: 0.1747, Val Accuracy: 0.7033\n",
            "Epoch 701/1000 - Train Loss: 0.1910, Train Accuracy: 0.6462, Val Loss: 0.1708, Val Accuracy: 0.7033\n",
            "Epoch 801/1000 - Train Loss: 0.1866, Train Accuracy: 0.6484, Val Loss: 0.1672, Val Accuracy: 0.7143\n",
            "Epoch 901/1000 - Train Loss: 0.1825, Train Accuracy: 0.6571, Val Loss: 0.1638, Val Accuracy: 0.7143\n",
            "Epoch 1/1000 - Train Loss: 0.2463, Train Accuracy: 0.5824, Val Loss: 0.2338, Val Accuracy: 0.6484\n",
            "Epoch 101/1000 - Train Loss: 0.2382, Train Accuracy: 0.6088, Val Loss: 0.2249, Val Accuracy: 0.6923\n",
            "Epoch 201/1000 - Train Loss: 0.2308, Train Accuracy: 0.6264, Val Loss: 0.2169, Val Accuracy: 0.6923\n",
            "Epoch 301/1000 - Train Loss: 0.2241, Train Accuracy: 0.6352, Val Loss: 0.2098, Val Accuracy: 0.7033\n",
            "Epoch 401/1000 - Train Loss: 0.2179, Train Accuracy: 0.6484, Val Loss: 0.2033, Val Accuracy: 0.7033\n",
            "Epoch 501/1000 - Train Loss: 0.2122, Train Accuracy: 0.6593, Val Loss: 0.1975, Val Accuracy: 0.7143\n",
            "Epoch 601/1000 - Train Loss: 0.2069, Train Accuracy: 0.6747, Val Loss: 0.1921, Val Accuracy: 0.7143\n",
            "Epoch 701/1000 - Train Loss: 0.2020, Train Accuracy: 0.6813, Val Loss: 0.1871, Val Accuracy: 0.7253\n",
            "Epoch 801/1000 - Train Loss: 0.1973, Train Accuracy: 0.6901, Val Loss: 0.1825, Val Accuracy: 0.7253\n",
            "Epoch 901/1000 - Train Loss: 0.1929, Train Accuracy: 0.7055, Val Loss: 0.1782, Val Accuracy: 0.7253\n",
            "Epoch 1/1000 - Train Loss: 0.2030, Train Accuracy: 0.6549, Val Loss: 0.1972, Val Accuracy: 0.7033\n",
            "Epoch 101/1000 - Train Loss: 0.1810, Train Accuracy: 0.6879, Val Loss: 0.1769, Val Accuracy: 0.7363\n",
            "Epoch 201/1000 - Train Loss: 0.1623, Train Accuracy: 0.7385, Val Loss: 0.1592, Val Accuracy: 0.7582\n",
            "Epoch 301/1000 - Train Loss: 0.1466, Train Accuracy: 0.7802, Val Loss: 0.1442, Val Accuracy: 0.7912\n",
            "Epoch 401/1000 - Train Loss: 0.1337, Train Accuracy: 0.8000, Val Loss: 0.1317, Val Accuracy: 0.8022\n",
            "Epoch 501/1000 - Train Loss: 0.1231, Train Accuracy: 0.8462, Val Loss: 0.1213, Val Accuracy: 0.8352\n",
            "Epoch 601/1000 - Train Loss: 0.1143, Train Accuracy: 0.8725, Val Loss: 0.1126, Val Accuracy: 0.8791\n",
            "Epoch 701/1000 - Train Loss: 0.1070, Train Accuracy: 0.8857, Val Loss: 0.1053, Val Accuracy: 0.8791\n",
            "Epoch 801/1000 - Train Loss: 0.1009, Train Accuracy: 0.8945, Val Loss: 0.0992, Val Accuracy: 0.8791\n",
            "Epoch 901/1000 - Train Loss: 0.0956, Train Accuracy: 0.8989, Val Loss: 0.0940, Val Accuracy: 0.8901\n",
            "Epoch 1/1000 - Train Loss: 0.3249, Train Accuracy: 0.6286, Val Loss: 0.2897, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.2085, Train Accuracy: 0.6286, Val Loss: 0.1910, Val Accuracy: 0.6703\n",
            "Epoch 201/1000 - Train Loss: 0.1447, Train Accuracy: 0.7626, Val Loss: 0.1357, Val Accuracy: 0.7912\n",
            "Epoch 301/1000 - Train Loss: 0.1181, Train Accuracy: 0.8857, Val Loss: 0.1116, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.1021, Train Accuracy: 0.9143, Val Loss: 0.0968, Val Accuracy: 0.9231\n",
            "Epoch 501/1000 - Train Loss: 0.0904, Train Accuracy: 0.9297, Val Loss: 0.0859, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0810, Train Accuracy: 0.9319, Val Loss: 0.0774, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0733, Train Accuracy: 0.9363, Val Loss: 0.0705, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0668, Train Accuracy: 0.9407, Val Loss: 0.0648, Val Accuracy: 0.9341\n",
            "Epoch 901/1000 - Train Loss: 0.0613, Train Accuracy: 0.9429, Val Loss: 0.0600, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.3018, Train Accuracy: 0.3846, Val Loss: 0.3359, Val Accuracy: 0.2967\n",
            "Epoch 101/1000 - Train Loss: 0.1915, Train Accuracy: 0.8308, Val Loss: 0.2034, Val Accuracy: 0.7692\n",
            "Epoch 201/1000 - Train Loss: 0.1416, Train Accuracy: 0.9187, Val Loss: 0.1448, Val Accuracy: 0.9011\n",
            "Epoch 301/1000 - Train Loss: 0.1095, Train Accuracy: 0.9407, Val Loss: 0.1093, Val Accuracy: 0.9231\n",
            "Epoch 401/1000 - Train Loss: 0.0872, Train Accuracy: 0.9516, Val Loss: 0.0858, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0719, Train Accuracy: 0.9582, Val Loss: 0.0699, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0612, Train Accuracy: 0.9582, Val Loss: 0.0591, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0536, Train Accuracy: 0.9648, Val Loss: 0.0515, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0478, Train Accuracy: 0.9670, Val Loss: 0.0459, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0434, Train Accuracy: 0.9692, Val Loss: 0.0417, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.2532, Train Accuracy: 0.6286, Val Loss: 0.2176, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.1753, Train Accuracy: 0.6462, Val Loss: 0.1539, Val Accuracy: 0.6813\n",
            "Epoch 201/1000 - Train Loss: 0.1417, Train Accuracy: 0.7868, Val Loss: 0.1261, Val Accuracy: 0.8352\n",
            "Epoch 301/1000 - Train Loss: 0.1225, Train Accuracy: 0.8615, Val Loss: 0.1101, Val Accuracy: 0.8901\n",
            "Epoch 401/1000 - Train Loss: 0.1087, Train Accuracy: 0.9055, Val Loss: 0.0984, Val Accuracy: 0.9231\n",
            "Epoch 501/1000 - Train Loss: 0.0976, Train Accuracy: 0.9209, Val Loss: 0.0892, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0885, Train Accuracy: 0.9275, Val Loss: 0.0816, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0807, Train Accuracy: 0.9319, Val Loss: 0.0753, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0740, Train Accuracy: 0.9341, Val Loss: 0.0699, Val Accuracy: 0.9341\n",
            "Epoch 901/1000 - Train Loss: 0.0683, Train Accuracy: 0.9407, Val Loss: 0.0653, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.2481, Train Accuracy: 0.6286, Val Loss: 0.2288, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.1731, Train Accuracy: 0.6352, Val Loss: 0.1636, Val Accuracy: 0.6813\n",
            "Epoch 201/1000 - Train Loss: 0.1403, Train Accuracy: 0.8484, Val Loss: 0.1335, Val Accuracy: 0.8352\n",
            "Epoch 301/1000 - Train Loss: 0.1211, Train Accuracy: 0.9165, Val Loss: 0.1154, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.1066, Train Accuracy: 0.9385, Val Loss: 0.1019, Val Accuracy: 0.9341\n",
            "Epoch 501/1000 - Train Loss: 0.0946, Train Accuracy: 0.9473, Val Loss: 0.0908, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0841, Train Accuracy: 0.9516, Val Loss: 0.0813, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0749, Train Accuracy: 0.9582, Val Loss: 0.0729, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0669, Train Accuracy: 0.9604, Val Loss: 0.0655, Val Accuracy: 0.9341\n",
            "Epoch 901/1000 - Train Loss: 0.0599, Train Accuracy: 0.9626, Val Loss: 0.0590, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.2113, Train Accuracy: 0.6462, Val Loss: 0.2331, Val Accuracy: 0.6264\n",
            "Epoch 101/1000 - Train Loss: 0.0865, Train Accuracy: 0.9253, Val Loss: 0.0850, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0617, Train Accuracy: 0.9473, Val Loss: 0.0570, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0520, Train Accuracy: 0.9560, Val Loss: 0.0472, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0464, Train Accuracy: 0.9560, Val Loss: 0.0420, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0425, Train Accuracy: 0.9582, Val Loss: 0.0386, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0395, Train Accuracy: 0.9626, Val Loss: 0.0361, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0371, Train Accuracy: 0.9626, Val Loss: 0.0341, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0351, Train Accuracy: 0.9670, Val Loss: 0.0325, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0334, Train Accuracy: 0.9670, Val Loss: 0.0311, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.3286, Train Accuracy: 0.3011, Val Loss: 0.3426, Val Accuracy: 0.2747\n",
            "Epoch 101/1000 - Train Loss: 0.1473, Train Accuracy: 0.8769, Val Loss: 0.1406, Val Accuracy: 0.9121\n",
            "Epoch 201/1000 - Train Loss: 0.1020, Train Accuracy: 0.9077, Val Loss: 0.0948, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0806, Train Accuracy: 0.9385, Val Loss: 0.0745, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0673, Train Accuracy: 0.9495, Val Loss: 0.0622, Val Accuracy: 0.9560\n",
            "Epoch 501/1000 - Train Loss: 0.0580, Train Accuracy: 0.9560, Val Loss: 0.0538, Val Accuracy: 0.9560\n",
            "Epoch 601/1000 - Train Loss: 0.0512, Train Accuracy: 0.9670, Val Loss: 0.0478, Val Accuracy: 0.9560\n",
            "Epoch 701/1000 - Train Loss: 0.0460, Train Accuracy: 0.9670, Val Loss: 0.0433, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0419, Train Accuracy: 0.9692, Val Loss: 0.0398, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0387, Train Accuracy: 0.9692, Val Loss: 0.0370, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.2512, Train Accuracy: 0.6286, Val Loss: 0.2399, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.1752, Train Accuracy: 0.6835, Val Loss: 0.1682, Val Accuracy: 0.7033\n",
            "Epoch 201/1000 - Train Loss: 0.1274, Train Accuracy: 0.8571, Val Loss: 0.1207, Val Accuracy: 0.8462\n",
            "Epoch 301/1000 - Train Loss: 0.0984, Train Accuracy: 0.9209, Val Loss: 0.0917, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.0804, Train Accuracy: 0.9385, Val Loss: 0.0741, Val Accuracy: 0.9341\n",
            "Epoch 501/1000 - Train Loss: 0.0684, Train Accuracy: 0.9451, Val Loss: 0.0628, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0599, Train Accuracy: 0.9538, Val Loss: 0.0549, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0536, Train Accuracy: 0.9538, Val Loss: 0.0492, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0486, Train Accuracy: 0.9560, Val Loss: 0.0449, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0446, Train Accuracy: 0.9560, Val Loss: 0.0415, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.3122, Train Accuracy: 0.3692, Val Loss: 0.3256, Val Accuracy: 0.3077\n",
            "Epoch 101/1000 - Train Loss: 0.1618, Train Accuracy: 0.8396, Val Loss: 0.1512, Val Accuracy: 0.8571\n",
            "Epoch 201/1000 - Train Loss: 0.1144, Train Accuracy: 0.8747, Val Loss: 0.1020, Val Accuracy: 0.9121\n",
            "Epoch 301/1000 - Train Loss: 0.0902, Train Accuracy: 0.9011, Val Loss: 0.0796, Val Accuracy: 0.9231\n",
            "Epoch 401/1000 - Train Loss: 0.0750, Train Accuracy: 0.9209, Val Loss: 0.0662, Val Accuracy: 0.9341\n",
            "Epoch 501/1000 - Train Loss: 0.0645, Train Accuracy: 0.9275, Val Loss: 0.0572, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0569, Train Accuracy: 0.9385, Val Loss: 0.0506, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0510, Train Accuracy: 0.9495, Val Loss: 0.0456, Val Accuracy: 0.9560\n",
            "Epoch 801/1000 - Train Loss: 0.0461, Train Accuracy: 0.9538, Val Loss: 0.0417, Val Accuracy: 0.9560\n",
            "Epoch 901/1000 - Train Loss: 0.0419, Train Accuracy: 0.9538, Val Loss: 0.0386, Val Accuracy: 0.9670\n",
            "Epoch 1/1000 - Train Loss: 0.3001, Train Accuracy: 0.3714, Val Loss: 0.3051, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.1949, Train Accuracy: 0.8440, Val Loss: 0.1902, Val Accuracy: 0.8901\n",
            "Epoch 201/1000 - Train Loss: 0.1574, Train Accuracy: 0.8857, Val Loss: 0.1503, Val Accuracy: 0.9231\n",
            "Epoch 301/1000 - Train Loss: 0.1311, Train Accuracy: 0.9143, Val Loss: 0.1235, Val Accuracy: 0.9341\n",
            "Epoch 401/1000 - Train Loss: 0.1102, Train Accuracy: 0.9231, Val Loss: 0.1031, Val Accuracy: 0.9341\n",
            "Epoch 501/1000 - Train Loss: 0.0940, Train Accuracy: 0.9297, Val Loss: 0.0878, Val Accuracy: 0.9341\n",
            "Epoch 601/1000 - Train Loss: 0.0814, Train Accuracy: 0.9363, Val Loss: 0.0762, Val Accuracy: 0.9341\n",
            "Epoch 701/1000 - Train Loss: 0.0717, Train Accuracy: 0.9407, Val Loss: 0.0673, Val Accuracy: 0.9341\n",
            "Epoch 801/1000 - Train Loss: 0.0640, Train Accuracy: 0.9385, Val Loss: 0.0604, Val Accuracy: 0.9341\n",
            "Epoch 901/1000 - Train Loss: 0.0577, Train Accuracy: 0.9385, Val Loss: 0.0549, Val Accuracy: 0.9341\n",
            "Epoch 1/1000 - Train Loss: 0.3099, Train Accuracy: 0.3714, Val Loss: 0.3167, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.1973, Train Accuracy: 0.8484, Val Loss: 0.1871, Val Accuracy: 0.8791\n",
            "Epoch 201/1000 - Train Loss: 0.1510, Train Accuracy: 0.8769, Val Loss: 0.1374, Val Accuracy: 0.9011\n",
            "Epoch 301/1000 - Train Loss: 0.1222, Train Accuracy: 0.9011, Val Loss: 0.1085, Val Accuracy: 0.9121\n",
            "Epoch 401/1000 - Train Loss: 0.1020, Train Accuracy: 0.9121, Val Loss: 0.0892, Val Accuracy: 0.9341\n",
            "Epoch 501/1000 - Train Loss: 0.0871, Train Accuracy: 0.9275, Val Loss: 0.0754, Val Accuracy: 0.9451\n",
            "Epoch 601/1000 - Train Loss: 0.0758, Train Accuracy: 0.9319, Val Loss: 0.0653, Val Accuracy: 0.9451\n",
            "Epoch 701/1000 - Train Loss: 0.0670, Train Accuracy: 0.9363, Val Loss: 0.0577, Val Accuracy: 0.9451\n",
            "Epoch 801/1000 - Train Loss: 0.0599, Train Accuracy: 0.9451, Val Loss: 0.0519, Val Accuracy: 0.9451\n",
            "Epoch 901/1000 - Train Loss: 0.0540, Train Accuracy: 0.9451, Val Loss: 0.0474, Val Accuracy: 0.9451\n",
            "Epoch 1/1000 - Train Loss: 0.2215, Train Accuracy: 0.7385, Val Loss: 0.2233, Val Accuracy: 0.7143\n",
            "Epoch 101/1000 - Train Loss: 0.0759, Train Accuracy: 0.9363, Val Loss: 0.0729, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0422, Train Accuracy: 0.9582, Val Loss: 0.0417, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0301, Train Accuracy: 0.9758, Val Loss: 0.0312, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0242, Train Accuracy: 0.9802, Val Loss: 0.0256, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0208, Train Accuracy: 0.9824, Val Loss: 0.0221, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0187, Train Accuracy: 0.9868, Val Loss: 0.0198, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0174, Train Accuracy: 0.9846, Val Loss: 0.0181, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0164, Train Accuracy: 0.9824, Val Loss: 0.0169, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0157, Train Accuracy: 0.9824, Val Loss: 0.0160, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2760, Train Accuracy: 0.4000, Val Loss: 0.2744, Val Accuracy: 0.4396\n",
            "Epoch 101/1000 - Train Loss: 0.0814, Train Accuracy: 0.9341, Val Loss: 0.0760, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0456, Train Accuracy: 0.9495, Val Loss: 0.0441, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0327, Train Accuracy: 0.9626, Val Loss: 0.0327, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0264, Train Accuracy: 0.9758, Val Loss: 0.0270, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0229, Train Accuracy: 0.9780, Val Loss: 0.0237, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0208, Train Accuracy: 0.9802, Val Loss: 0.0214, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0194, Train Accuracy: 0.9802, Val Loss: 0.0198, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0183, Train Accuracy: 0.9802, Val Loss: 0.0186, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0175, Train Accuracy: 0.9824, Val Loss: 0.0176, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3846, Train Accuracy: 0.3714, Val Loss: 0.4083, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.0591, Train Accuracy: 0.9495, Val Loss: 0.0560, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0373, Train Accuracy: 0.9648, Val Loss: 0.0371, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0284, Train Accuracy: 0.9736, Val Loss: 0.0295, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0236, Train Accuracy: 0.9802, Val Loss: 0.0251, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0208, Train Accuracy: 0.9846, Val Loss: 0.0223, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0190, Train Accuracy: 0.9846, Val Loss: 0.0204, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0178, Train Accuracy: 0.9846, Val Loss: 0.0190, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0169, Train Accuracy: 0.9846, Val Loss: 0.0179, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0161, Train Accuracy: 0.9868, Val Loss: 0.0170, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2481, Train Accuracy: 0.5473, Val Loss: 0.2607, Val Accuracy: 0.3956\n",
            "Epoch 101/1000 - Train Loss: 0.0662, Train Accuracy: 0.9429, Val Loss: 0.0633, Val Accuracy: 0.9231\n",
            "Epoch 201/1000 - Train Loss: 0.0385, Train Accuracy: 0.9670, Val Loss: 0.0373, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0287, Train Accuracy: 0.9736, Val Loss: 0.0285, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0240, Train Accuracy: 0.9802, Val Loss: 0.0242, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0213, Train Accuracy: 0.9824, Val Loss: 0.0215, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0196, Train Accuracy: 0.9824, Val Loss: 0.0196, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0183, Train Accuracy: 0.9868, Val Loss: 0.0182, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0173, Train Accuracy: 0.9846, Val Loss: 0.0171, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0166, Train Accuracy: 0.9846, Val Loss: 0.0162, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2369, Train Accuracy: 0.5780, Val Loss: 0.2340, Val Accuracy: 0.6044\n",
            "Epoch 101/1000 - Train Loss: 0.0563, Train Accuracy: 0.9451, Val Loss: 0.0517, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0388, Train Accuracy: 0.9604, Val Loss: 0.0358, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0313, Train Accuracy: 0.9670, Val Loss: 0.0297, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0269, Train Accuracy: 0.9692, Val Loss: 0.0263, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0240, Train Accuracy: 0.9736, Val Loss: 0.0241, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0219, Train Accuracy: 0.9736, Val Loss: 0.0224, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0204, Train Accuracy: 0.9736, Val Loss: 0.0210, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0192, Train Accuracy: 0.9802, Val Loss: 0.0198, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0183, Train Accuracy: 0.9802, Val Loss: 0.0188, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.1840, Train Accuracy: 0.6945, Val Loss: 0.1800, Val Accuracy: 0.6923\n",
            "Epoch 101/1000 - Train Loss: 0.0633, Train Accuracy: 0.9429, Val Loss: 0.0586, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0406, Train Accuracy: 0.9670, Val Loss: 0.0375, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0305, Train Accuracy: 0.9758, Val Loss: 0.0289, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0250, Train Accuracy: 0.9846, Val Loss: 0.0243, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0218, Train Accuracy: 0.9824, Val Loss: 0.0214, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0198, Train Accuracy: 0.9846, Val Loss: 0.0196, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0184, Train Accuracy: 0.9868, Val Loss: 0.0183, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0174, Train Accuracy: 0.9868, Val Loss: 0.0173, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0167, Train Accuracy: 0.9868, Val Loss: 0.0166, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3438, Train Accuracy: 0.3714, Val Loss: 0.3599, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.1185, Train Accuracy: 0.8791, Val Loss: 0.1111, Val Accuracy: 0.9011\n",
            "Epoch 201/1000 - Train Loss: 0.0677, Train Accuracy: 0.9297, Val Loss: 0.0647, Val Accuracy: 0.9231\n",
            "Epoch 301/1000 - Train Loss: 0.0445, Train Accuracy: 0.9451, Val Loss: 0.0446, Val Accuracy: 0.9231\n",
            "Epoch 401/1000 - Train Loss: 0.0329, Train Accuracy: 0.9604, Val Loss: 0.0344, Val Accuracy: 0.9451\n",
            "Epoch 501/1000 - Train Loss: 0.0271, Train Accuracy: 0.9670, Val Loss: 0.0290, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0236, Train Accuracy: 0.9714, Val Loss: 0.0255, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0213, Train Accuracy: 0.9780, Val Loss: 0.0231, Val Accuracy: 0.9670\n",
            "Epoch 801/1000 - Train Loss: 0.0198, Train Accuracy: 0.9780, Val Loss: 0.0212, Val Accuracy: 0.9670\n",
            "Epoch 901/1000 - Train Loss: 0.0186, Train Accuracy: 0.9802, Val Loss: 0.0197, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.5315, Train Accuracy: 0.3275, Val Loss: 0.5497, Val Accuracy: 0.3187\n",
            "Epoch 101/1000 - Train Loss: 0.0641, Train Accuracy: 0.9341, Val Loss: 0.0580, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0383, Train Accuracy: 0.9560, Val Loss: 0.0369, Val Accuracy: 0.9451\n",
            "Epoch 301/1000 - Train Loss: 0.0284, Train Accuracy: 0.9780, Val Loss: 0.0281, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0237, Train Accuracy: 0.9846, Val Loss: 0.0233, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0210, Train Accuracy: 0.9846, Val Loss: 0.0204, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0194, Train Accuracy: 0.9824, Val Loss: 0.0184, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0182, Train Accuracy: 0.9824, Val Loss: 0.0170, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0174, Train Accuracy: 0.9802, Val Loss: 0.0159, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0168, Train Accuracy: 0.9802, Val Loss: 0.0151, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.3190, Train Accuracy: 0.6286, Val Loss: 0.2959, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0792, Train Accuracy: 0.9297, Val Loss: 0.0719, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0460, Train Accuracy: 0.9451, Val Loss: 0.0439, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0345, Train Accuracy: 0.9582, Val Loss: 0.0347, Val Accuracy: 0.9451\n",
            "Epoch 401/1000 - Train Loss: 0.0286, Train Accuracy: 0.9670, Val Loss: 0.0294, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0250, Train Accuracy: 0.9692, Val Loss: 0.0260, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0227, Train Accuracy: 0.9736, Val Loss: 0.0236, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0210, Train Accuracy: 0.9802, Val Loss: 0.0219, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0198, Train Accuracy: 0.9802, Val Loss: 0.0205, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0188, Train Accuracy: 0.9802, Val Loss: 0.0194, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.5027, Train Accuracy: 0.3714, Val Loss: 0.5438, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.0738, Train Accuracy: 0.9319, Val Loss: 0.0662, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0414, Train Accuracy: 0.9626, Val Loss: 0.0377, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0299, Train Accuracy: 0.9714, Val Loss: 0.0284, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0243, Train Accuracy: 0.9780, Val Loss: 0.0237, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0212, Train Accuracy: 0.9824, Val Loss: 0.0208, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0193, Train Accuracy: 0.9824, Val Loss: 0.0189, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0181, Train Accuracy: 0.9846, Val Loss: 0.0175, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0172, Train Accuracy: 0.9846, Val Loss: 0.0163, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0165, Train Accuracy: 0.9846, Val Loss: 0.0154, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2278, Train Accuracy: 0.5253, Val Loss: 0.2249, Val Accuracy: 0.5165\n",
            "Epoch 101/1000 - Train Loss: 0.0336, Train Accuracy: 0.9692, Val Loss: 0.0331, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0234, Train Accuracy: 0.9758, Val Loss: 0.0233, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0196, Train Accuracy: 0.9824, Val Loss: 0.0191, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0176, Train Accuracy: 0.9824, Val Loss: 0.0167, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0163, Train Accuracy: 0.9824, Val Loss: 0.0150, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0154, Train Accuracy: 0.9824, Val Loss: 0.0137, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0127, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0140, Train Accuracy: 0.9824, Val Loss: 0.0119, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0135, Train Accuracy: 0.9824, Val Loss: 0.0111, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3114, Train Accuracy: 0.2659, Val Loss: 0.3305, Val Accuracy: 0.2198\n",
            "Epoch 101/1000 - Train Loss: 0.0366, Train Accuracy: 0.9692, Val Loss: 0.0363, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0239, Train Accuracy: 0.9824, Val Loss: 0.0252, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0196, Train Accuracy: 0.9846, Val Loss: 0.0210, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0174, Train Accuracy: 0.9868, Val Loss: 0.0186, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0160, Train Accuracy: 0.9868, Val Loss: 0.0169, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0150, Train Accuracy: 0.9868, Val Loss: 0.0156, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0142, Train Accuracy: 0.9868, Val Loss: 0.0145, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0136, Train Accuracy: 0.9868, Val Loss: 0.0137, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0131, Train Accuracy: 0.9868, Val Loss: 0.0129, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2465, Train Accuracy: 0.5187, Val Loss: 0.2473, Val Accuracy: 0.5055\n",
            "Epoch 101/1000 - Train Loss: 0.0390, Train Accuracy: 0.9758, Val Loss: 0.0353, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0247, Train Accuracy: 0.9780, Val Loss: 0.0240, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0202, Train Accuracy: 0.9802, Val Loss: 0.0203, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0179, Train Accuracy: 0.9824, Val Loss: 0.0182, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0165, Train Accuracy: 0.9824, Val Loss: 0.0168, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0154, Train Accuracy: 0.9824, Val Loss: 0.0156, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0146, Train Accuracy: 0.9846, Val Loss: 0.0145, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0139, Train Accuracy: 0.9868, Val Loss: 0.0137, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0134, Train Accuracy: 0.9868, Val Loss: 0.0129, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2883, Train Accuracy: 0.6286, Val Loss: 0.2613, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0760, Train Accuracy: 0.9319, Val Loss: 0.0720, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0393, Train Accuracy: 0.9648, Val Loss: 0.0411, Val Accuracy: 0.9341\n",
            "Epoch 301/1000 - Train Loss: 0.0276, Train Accuracy: 0.9714, Val Loss: 0.0302, Val Accuracy: 0.9560\n",
            "Epoch 401/1000 - Train Loss: 0.0228, Train Accuracy: 0.9802, Val Loss: 0.0250, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0201, Train Accuracy: 0.9802, Val Loss: 0.0220, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0184, Train Accuracy: 0.9802, Val Loss: 0.0201, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0173, Train Accuracy: 0.9802, Val Loss: 0.0186, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0164, Train Accuracy: 0.9824, Val Loss: 0.0174, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0157, Train Accuracy: 0.9824, Val Loss: 0.0165, Val Accuracy: 0.9780\n",
            "Epoch 1/1000 - Train Loss: 0.2513, Train Accuracy: 0.5209, Val Loss: 0.2460, Val Accuracy: 0.5055\n",
            "Epoch 101/1000 - Train Loss: 0.0366, Train Accuracy: 0.9626, Val Loss: 0.0358, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0240, Train Accuracy: 0.9780, Val Loss: 0.0241, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0196, Train Accuracy: 0.9846, Val Loss: 0.0189, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0175, Train Accuracy: 0.9846, Val Loss: 0.0158, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0162, Train Accuracy: 0.9846, Val Loss: 0.0137, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0153, Train Accuracy: 0.9846, Val Loss: 0.0122, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0146, Train Accuracy: 0.9868, Val Loss: 0.0111, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0140, Train Accuracy: 0.9868, Val Loss: 0.0102, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0135, Train Accuracy: 0.9868, Val Loss: 0.0094, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.2071, Train Accuracy: 0.7011, Val Loss: 0.2051, Val Accuracy: 0.6923\n",
            "Epoch 101/1000 - Train Loss: 0.0359, Train Accuracy: 0.9670, Val Loss: 0.0354, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0232, Train Accuracy: 0.9780, Val Loss: 0.0246, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0189, Train Accuracy: 0.9780, Val Loss: 0.0207, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0168, Train Accuracy: 0.9802, Val Loss: 0.0185, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0155, Train Accuracy: 0.9824, Val Loss: 0.0169, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0156, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0139, Train Accuracy: 0.9846, Val Loss: 0.0145, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0133, Train Accuracy: 0.9846, Val Loss: 0.0135, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0128, Train Accuracy: 0.9846, Val Loss: 0.0127, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2645, Train Accuracy: 0.6286, Val Loss: 0.2542, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0395, Train Accuracy: 0.9582, Val Loss: 0.0401, Val Accuracy: 0.9341\n",
            "Epoch 201/1000 - Train Loss: 0.0252, Train Accuracy: 0.9824, Val Loss: 0.0265, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0205, Train Accuracy: 0.9824, Val Loss: 0.0215, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0182, Train Accuracy: 0.9824, Val Loss: 0.0188, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0168, Train Accuracy: 0.9802, Val Loss: 0.0171, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0158, Train Accuracy: 0.9824, Val Loss: 0.0158, Val Accuracy: 0.9780\n",
            "Epoch 701/1000 - Train Loss: 0.0151, Train Accuracy: 0.9824, Val Loss: 0.0147, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0145, Train Accuracy: 0.9846, Val Loss: 0.0139, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0139, Train Accuracy: 0.9846, Val Loss: 0.0131, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.1837, Train Accuracy: 0.6549, Val Loss: 0.1713, Val Accuracy: 0.6813\n",
            "Epoch 101/1000 - Train Loss: 0.0439, Train Accuracy: 0.9604, Val Loss: 0.0408, Val Accuracy: 0.9451\n",
            "Epoch 201/1000 - Train Loss: 0.0271, Train Accuracy: 0.9736, Val Loss: 0.0258, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0215, Train Accuracy: 0.9802, Val Loss: 0.0202, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0188, Train Accuracy: 0.9824, Val Loss: 0.0172, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0172, Train Accuracy: 0.9824, Val Loss: 0.0153, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0160, Train Accuracy: 0.9824, Val Loss: 0.0140, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0152, Train Accuracy: 0.9824, Val Loss: 0.0129, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0145, Train Accuracy: 0.9846, Val Loss: 0.0121, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0139, Train Accuracy: 0.9846, Val Loss: 0.0114, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3914, Train Accuracy: 0.2747, Val Loss: 0.4147, Val Accuracy: 0.2747\n",
            "Epoch 101/1000 - Train Loss: 0.0429, Train Accuracy: 0.9582, Val Loss: 0.0391, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0259, Train Accuracy: 0.9714, Val Loss: 0.0260, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0205, Train Accuracy: 0.9802, Val Loss: 0.0215, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0180, Train Accuracy: 0.9824, Val Loss: 0.0191, Val Accuracy: 0.9780\n",
            "Epoch 501/1000 - Train Loss: 0.0165, Train Accuracy: 0.9846, Val Loss: 0.0174, Val Accuracy: 0.9780\n",
            "Epoch 601/1000 - Train Loss: 0.0155, Train Accuracy: 0.9868, Val Loss: 0.0162, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0147, Train Accuracy: 0.9868, Val Loss: 0.0151, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0141, Train Accuracy: 0.9846, Val Loss: 0.0143, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0136, Train Accuracy: 0.9846, Val Loss: 0.0135, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3760, Train Accuracy: 0.3714, Val Loss: 0.3879, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.0369, Train Accuracy: 0.9692, Val Loss: 0.0352, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0242, Train Accuracy: 0.9780, Val Loss: 0.0251, Val Accuracy: 0.9670\n",
            "Epoch 301/1000 - Train Loss: 0.0203, Train Accuracy: 0.9780, Val Loss: 0.0215, Val Accuracy: 0.9670\n",
            "Epoch 401/1000 - Train Loss: 0.0183, Train Accuracy: 0.9802, Val Loss: 0.0194, Val Accuracy: 0.9670\n",
            "Epoch 501/1000 - Train Loss: 0.0169, Train Accuracy: 0.9780, Val Loss: 0.0180, Val Accuracy: 0.9670\n",
            "Epoch 601/1000 - Train Loss: 0.0159, Train Accuracy: 0.9780, Val Loss: 0.0168, Val Accuracy: 0.9670\n",
            "Epoch 701/1000 - Train Loss: 0.0151, Train Accuracy: 0.9802, Val Loss: 0.0159, Val Accuracy: 0.9780\n",
            "Epoch 801/1000 - Train Loss: 0.0145, Train Accuracy: 0.9824, Val Loss: 0.0150, Val Accuracy: 0.9780\n",
            "Epoch 901/1000 - Train Loss: 0.0139, Train Accuracy: 0.9824, Val Loss: 0.0142, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.3714, Train Accuracy: 0.3714, Val Loss: 0.3801, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.0253, Train Accuracy: 0.9714, Val Loss: 0.0259, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0187, Train Accuracy: 0.9758, Val Loss: 0.0192, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0163, Train Accuracy: 0.9824, Val Loss: 0.0162, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0149, Train Accuracy: 0.9824, Val Loss: 0.0142, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0139, Train Accuracy: 0.9846, Val Loss: 0.0128, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0131, Train Accuracy: 0.9846, Val Loss: 0.0116, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0124, Train Accuracy: 0.9846, Val Loss: 0.0106, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0098, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0091, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2851, Train Accuracy: 0.4022, Val Loss: 0.2929, Val Accuracy: 0.4176\n",
            "Epoch 101/1000 - Train Loss: 0.0269, Train Accuracy: 0.9692, Val Loss: 0.0292, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0189, Train Accuracy: 0.9780, Val Loss: 0.0207, Val Accuracy: 0.9560\n",
            "Epoch 301/1000 - Train Loss: 0.0160, Train Accuracy: 0.9846, Val Loss: 0.0167, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0143, Train Accuracy: 0.9846, Val Loss: 0.0141, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0132, Train Accuracy: 0.9846, Val Loss: 0.0122, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0124, Train Accuracy: 0.9846, Val Loss: 0.0108, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0118, Train Accuracy: 0.9846, Val Loss: 0.0097, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0113, Train Accuracy: 0.9846, Val Loss: 0.0088, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0108, Train Accuracy: 0.9868, Val Loss: 0.0080, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2884, Train Accuracy: 0.3846, Val Loss: 0.2969, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.0246, Train Accuracy: 0.9780, Val Loss: 0.0249, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0184, Train Accuracy: 0.9846, Val Loss: 0.0185, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0158, Train Accuracy: 0.9824, Val Loss: 0.0156, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0143, Train Accuracy: 0.9846, Val Loss: 0.0136, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0132, Train Accuracy: 0.9846, Val Loss: 0.0121, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0124, Train Accuracy: 0.9846, Val Loss: 0.0109, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0117, Train Accuracy: 0.9846, Val Loss: 0.0100, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0112, Train Accuracy: 0.9868, Val Loss: 0.0091, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0108, Train Accuracy: 0.9868, Val Loss: 0.0084, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2628, Train Accuracy: 0.4681, Val Loss: 0.2651, Val Accuracy: 0.4286\n",
            "Epoch 101/1000 - Train Loss: 0.0273, Train Accuracy: 0.9758, Val Loss: 0.0252, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0189, Train Accuracy: 0.9824, Val Loss: 0.0170, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0161, Train Accuracy: 0.9824, Val Loss: 0.0142, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0144, Train Accuracy: 0.9824, Val Loss: 0.0125, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0133, Train Accuracy: 0.9868, Val Loss: 0.0112, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0125, Train Accuracy: 0.9868, Val Loss: 0.0102, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0118, Train Accuracy: 0.9868, Val Loss: 0.0093, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0113, Train Accuracy: 0.9868, Val Loss: 0.0085, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0109, Train Accuracy: 0.9868, Val Loss: 0.0079, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.4210, Train Accuracy: 0.3714, Val Loss: 0.4404, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.0258, Train Accuracy: 0.9714, Val Loss: 0.0257, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0183, Train Accuracy: 0.9802, Val Loss: 0.0177, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0159, Train Accuracy: 0.9824, Val Loss: 0.0144, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0145, Train Accuracy: 0.9824, Val Loss: 0.0123, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0134, Train Accuracy: 0.9824, Val Loss: 0.0109, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0126, Train Accuracy: 0.9846, Val Loss: 0.0098, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0119, Train Accuracy: 0.9868, Val Loss: 0.0088, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0114, Train Accuracy: 0.9868, Val Loss: 0.0080, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0109, Train Accuracy: 0.9868, Val Loss: 0.0073, Val Accuracy: 1.0000\n",
            "Epoch 1/1000 - Train Loss: 0.3635, Train Accuracy: 0.3736, Val Loss: 0.3770, Val Accuracy: 0.3297\n",
            "Epoch 101/1000 - Train Loss: 0.0258, Train Accuracy: 0.9692, Val Loss: 0.0260, Val Accuracy: 0.9670\n",
            "Epoch 201/1000 - Train Loss: 0.0189, Train Accuracy: 0.9780, Val Loss: 0.0187, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0163, Train Accuracy: 0.9802, Val Loss: 0.0154, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0146, Train Accuracy: 0.9824, Val Loss: 0.0133, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0134, Train Accuracy: 0.9846, Val Loss: 0.0116, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0125, Train Accuracy: 0.9846, Val Loss: 0.0104, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0117, Train Accuracy: 0.9846, Val Loss: 0.0093, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0111, Train Accuracy: 0.9868, Val Loss: 0.0084, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0107, Train Accuracy: 0.9890, Val Loss: 0.0077, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2567, Train Accuracy: 0.4000, Val Loss: 0.2699, Val Accuracy: 0.3516\n",
            "Epoch 101/1000 - Train Loss: 0.0242, Train Accuracy: 0.9824, Val Loss: 0.0222, Val Accuracy: 0.9890\n",
            "Epoch 201/1000 - Train Loss: 0.0171, Train Accuracy: 0.9824, Val Loss: 0.0153, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0146, Train Accuracy: 0.9846, Val Loss: 0.0124, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0133, Train Accuracy: 0.9868, Val Loss: 0.0107, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0125, Train Accuracy: 0.9868, Val Loss: 0.0094, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0118, Train Accuracy: 0.9868, Val Loss: 0.0085, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0113, Train Accuracy: 0.9868, Val Loss: 0.0077, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0109, Train Accuracy: 0.9890, Val Loss: 0.0070, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0105, Train Accuracy: 0.9890, Val Loss: 0.0065, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.1904, Train Accuracy: 0.6352, Val Loss: 0.1860, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0244, Train Accuracy: 0.9780, Val Loss: 0.0243, Val Accuracy: 0.9780\n",
            "Epoch 201/1000 - Train Loss: 0.0176, Train Accuracy: 0.9846, Val Loss: 0.0179, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0154, Train Accuracy: 0.9868, Val Loss: 0.0155, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0140, Train Accuracy: 0.9846, Val Loss: 0.0138, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0131, Train Accuracy: 0.9846, Val Loss: 0.0126, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0124, Train Accuracy: 0.9846, Val Loss: 0.0116, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0119, Train Accuracy: 0.9868, Val Loss: 0.0107, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0114, Train Accuracy: 0.9890, Val Loss: 0.0099, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0111, Train Accuracy: 0.9890, Val Loss: 0.0092, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2529, Train Accuracy: 0.6286, Val Loss: 0.2306, Val Accuracy: 0.6703\n",
            "Epoch 101/1000 - Train Loss: 0.0294, Train Accuracy: 0.9670, Val Loss: 0.0308, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0197, Train Accuracy: 0.9780, Val Loss: 0.0199, Val Accuracy: 0.9780\n",
            "Epoch 301/1000 - Train Loss: 0.0170, Train Accuracy: 0.9824, Val Loss: 0.0162, Val Accuracy: 0.9780\n",
            "Epoch 401/1000 - Train Loss: 0.0152, Train Accuracy: 0.9824, Val Loss: 0.0139, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0140, Train Accuracy: 0.9824, Val Loss: 0.0124, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0131, Train Accuracy: 0.9846, Val Loss: 0.0112, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0125, Train Accuracy: 0.9846, Val Loss: 0.0102, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0119, Train Accuracy: 0.9846, Val Loss: 0.0094, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0115, Train Accuracy: 0.9846, Val Loss: 0.0087, Val Accuracy: 0.9890\n",
            "Epoch 1/1000 - Train Loss: 0.2623, Train Accuracy: 0.5802, Val Loss: 0.2543, Val Accuracy: 0.6264\n",
            "Epoch 101/1000 - Train Loss: 0.0278, Train Accuracy: 0.9692, Val Loss: 0.0273, Val Accuracy: 0.9560\n",
            "Epoch 201/1000 - Train Loss: 0.0196, Train Accuracy: 0.9802, Val Loss: 0.0189, Val Accuracy: 0.9890\n",
            "Epoch 301/1000 - Train Loss: 0.0165, Train Accuracy: 0.9802, Val Loss: 0.0151, Val Accuracy: 0.9890\n",
            "Epoch 401/1000 - Train Loss: 0.0147, Train Accuracy: 0.9824, Val Loss: 0.0128, Val Accuracy: 0.9890\n",
            "Epoch 501/1000 - Train Loss: 0.0136, Train Accuracy: 0.9824, Val Loss: 0.0113, Val Accuracy: 0.9890\n",
            "Epoch 601/1000 - Train Loss: 0.0127, Train Accuracy: 0.9846, Val Loss: 0.0102, Val Accuracy: 0.9890\n",
            "Epoch 701/1000 - Train Loss: 0.0120, Train Accuracy: 0.9868, Val Loss: 0.0093, Val Accuracy: 0.9890\n",
            "Epoch 801/1000 - Train Loss: 0.0115, Train Accuracy: 0.9868, Val Loss: 0.0085, Val Accuracy: 0.9890\n",
            "Epoch 901/1000 - Train Loss: 0.0110, Train Accuracy: 0.9868, Val Loss: 0.0079, Val Accuracy: 0.9890\n",
            "                              fold  train_loss  train_accuracy  val_loss  \\\n",
            "learning_rate initialization                                               \n",
            "0.001         0                3.0    0.165190        0.818901  0.166617   \n",
            "              1                3.0    0.163596        0.756923  0.167320   \n",
            "              2                3.0    0.156269        0.793407  0.153732   \n",
            "              3                3.0    0.153323        0.850989  0.154109   \n",
            "              4                3.0    0.166310        0.749451  0.169301   \n",
            "              5                3.0    0.176786        0.728352  0.180329   \n",
            "              6                3.0    0.161322        0.804835  0.156219   \n",
            "              7                3.0    0.183043        0.750330  0.174207   \n",
            "              8                3.0    0.179217        0.774505  0.178302   \n",
            "              9                3.0    0.148174        0.837363  0.145084   \n",
            "0.010         0                3.0    0.044113        0.958242  0.044453   \n",
            "              1                3.0    0.047432        0.952527  0.048742   \n",
            "              2                3.0    0.045812        0.962637  0.046580   \n",
            "              3                3.0    0.048350        0.956484  0.048271   \n",
            "              4                3.0    0.041556        0.957802  0.041529   \n",
            "              5                3.0    0.041658        0.958242  0.043319   \n",
            "              6                3.0    0.040137        0.956484  0.040012   \n",
            "              7                3.0    0.042389        0.961319  0.041806   \n",
            "              8                3.0    0.046293        0.953407  0.044160   \n",
            "              9                3.0    0.054620        0.946813  0.056997   \n",
            "0.050         0                3.0    0.016229        0.982418  0.016278   \n",
            "              1                3.0    0.016933        0.981978  0.016544   \n",
            "              2                3.0    0.017247        0.982857  0.017274   \n",
            "              3                3.0    0.016490        0.981538  0.017217   \n",
            "              4                3.0    0.017659        0.982857  0.018095   \n",
            "              5                3.0    0.016496        0.984615  0.017386   \n",
            "              6                3.0    0.017160        0.981099  0.017396   \n",
            "              7                3.0    0.016628        0.984176  0.016911   \n",
            "              8                3.0    0.017508        0.981538  0.017835   \n",
            "              9                3.0    0.016801        0.981538  0.016840   \n",
            "0.100         0                3.0    0.013313        0.985934  0.013364   \n",
            "              1                3.0    0.013514        0.984615  0.014038   \n",
            "              2                3.0    0.013223        0.985495  0.013470   \n",
            "              3                3.0    0.013419        0.985055  0.014123   \n",
            "              4                3.0    0.013206        0.984615  0.013120   \n",
            "              5                3.0    0.012827        0.985495  0.013186   \n",
            "              6                3.0    0.013248        0.983736  0.013878   \n",
            "              7                3.0    0.013171        0.985495  0.013472   \n",
            "              8                3.0    0.013287        0.985055  0.013605   \n",
            "              9                3.0    0.013048        0.985055  0.013302   \n",
            "0.200         0                3.0    0.010784        0.987253  0.010937   \n",
            "              1                3.0    0.010757        0.986374  0.011021   \n",
            "              2                3.0    0.010939        0.985934  0.010917   \n",
            "              3                3.0    0.010341        0.988132  0.010607   \n",
            "              4                3.0    0.010817        0.986813  0.011069   \n",
            "              5                3.0    0.010293        0.987692  0.010575   \n",
            "              6                3.0    0.010280        0.988132  0.010064   \n",
            "              7                3.0    0.010672        0.988132  0.010987   \n",
            "              8                3.0    0.010991        0.985934  0.011204   \n",
            "              9                3.0    0.010611        0.988132  0.010729   \n",
            "\n",
            "                              val_accuracy  \n",
            "learning_rate initialization                \n",
            "0.001         0                   0.817582  \n",
            "              1                   0.756044  \n",
            "              2                   0.810989  \n",
            "              3                   0.848352  \n",
            "              4                   0.756044  \n",
            "              5                   0.712088  \n",
            "              6                   0.815385  \n",
            "              7                   0.791209  \n",
            "              8                   0.769231  \n",
            "              9                   0.835165  \n",
            "0.010         0                   0.956044  \n",
            "              1                   0.960440  \n",
            "              2                   0.964835  \n",
            "              3                   0.951648  \n",
            "              4                   0.953846  \n",
            "              5                   0.956044  \n",
            "              6                   0.958242  \n",
            "              7                   0.969231  \n",
            "              8                   0.964835  \n",
            "              9                   0.938462  \n",
            "0.050         0                   0.982418  \n",
            "              1                   0.982418  \n",
            "              2                   0.982418  \n",
            "              3                   0.982418  \n",
            "              4                   0.980220  \n",
            "              5                   0.982418  \n",
            "              6                   0.982418  \n",
            "              7                   0.982418  \n",
            "              8                   0.978022  \n",
            "              9                   0.982418  \n",
            "0.100         0                   0.984615  \n",
            "              1                   0.984615  \n",
            "              2                   0.984615  \n",
            "              3                   0.984615  \n",
            "              4                   0.986813  \n",
            "              5                   0.986813  \n",
            "              6                   0.984615  \n",
            "              7                   0.984615  \n",
            "              8                   0.984615  \n",
            "              9                   0.984615  \n",
            "0.200         0                   0.989011  \n",
            "              1                   0.991209  \n",
            "              2                   0.989011  \n",
            "              3                   0.989011  \n",
            "              4                   0.986813  \n",
            "              5                   0.986813  \n",
            "              6                   0.991209  \n",
            "              7                   0.986813  \n",
            "              8                   0.989011  \n",
            "              9                   0.989011  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    return accuracy_score(np.argmax(labels, axis=1), np.argmax(predictions, axis=1))\n",
        "\n",
        "# Hyperparameters\n",
        "num_initializations = 10\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1,0.2]\n",
        "epochs = 1000\n",
        "patience = 10\n",
        "# Store the original training data\n",
        "X_train_original = X_train\n",
        "y_train_original = y_train\n",
        "\n",
        "\n",
        "# Initialize matrix to store updated weights and biases\n",
        "updated_weights_biases = np.zeros((len(learning_rates), num_initializations,input_size * hidden_size + hidden_size + hidden_size * output_size + output_size))\n",
        "\n",
        "\n",
        "# Cross-validation setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
        "\n",
        "# Initialize matrix to store updated weights and biases\n",
        "results = []\n",
        "for train_index, val_index in kf.split(X_train_original): # Use the original X_train for splitting\n",
        "    X_train_fold, X_val = X_train_original[train_index], X_train_original[val_index]\n",
        "    y_train_fold, y_val = y_train_original[train_index], y_train_original[val_index]\n",
        "\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    for lr_idx, learning_rate in enumerate(learning_rates):\n",
        "        for init_idx in range(num_initializations):\n",
        "            # Retrieve initial weights and biases\n",
        "            W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)  # Xavier initialization\n",
        "            b1 = np.zeros((1, hidden_size))\n",
        "            W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)  # Xavier initialization\n",
        "            b2 = np.zeros((1, output_size))\n",
        "\n",
        "            # Training process with backpropagation\n",
        "            best_val_loss = float('inf')\n",
        "            best_train_loss = float('inf')\n",
        "            best_train_accuracy = 0\n",
        "            best_val_accuracy = 0\n",
        "            patience_counter = 0\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                # Forward propagation\n",
        "                a1_train, a2_train = forward_propagation(X_train, W1, b1, W2, b2)\n",
        "                a1_val, a2_val = forward_propagation(X_val, W1, b1, W2, b2)\n",
        "\n",
        "                # Compute training and validation loss\n",
        "                train_loss = np.mean((a2_train - y_train) ** 2)\n",
        "                val_loss = np.mean((a2_val - y_val) ** 2)\n",
        "\n",
        "                # Compute training and validation accuracy\n",
        "                train_accuracy = compute_accuracy(a2_train, y_train)\n",
        "                val_accuracy = compute_accuracy(a2_val, y_val)\n",
        "\n",
        "                # Backward propagation\n",
        "                dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, a1_train, a2_train, W1, W2)\n",
        "\n",
        "                # Update weights and biases\n",
        "                W1 -= learning_rate * dW1\n",
        "                b1 -= learning_rate * db1\n",
        "                W2 -= learning_rate * dW2\n",
        "                b2 -= learning_rate * db2\n",
        "\n",
        "                # Track best validation loss and accuracy\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_train_loss = train_loss\n",
        "                    best_train_accuracy = train_accuracy\n",
        "                    best_val_accuracy = val_accuracy\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        print(f\"Early stopping at epoch {epoch + 1} for learning rate {learning_rate}\")\n",
        "                        break\n",
        "\n",
        "                # Print training and validation metrics\n",
        "                if epoch % 100 == 0:  # Print every 100 epochs\n",
        "                    print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "            # Store the updated weights and biases\n",
        "            idx = (lr_idx, init_idx)\n",
        "            updated_weights_biases[idx[0], idx[1], :] = np.concatenate([W1.flatten(), b1.flatten(), W2.flatten(), b2.flatten()])\n",
        "\n",
        "            results.append({\n",
        "                'learning_rate': learning_rate,\n",
        "                'initialization':init_idx,\n",
        "                'fold': len(results) // (num_initializations * len(learning_rates)) + 1,\n",
        "                'train_loss': best_train_loss,\n",
        "                'train_accuracy': best_train_accuracy,\n",
        "                'val_loss': best_val_loss,\n",
        "                'val_accuracy': best_val_accuracy\n",
        "            })\n",
        "\n",
        "# Aggregate results\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "mean_results = results_df.groupby(['learning_rate', 'initialization']).mean()\n",
        "\n",
        "print(mean_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkPhEhrOC8YA",
        "outputId": "ab5272cf-8113-48c6-b45c-480ad431d30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(455, 30)\n",
            "(455, 2)\n",
            "(91, 30)\n",
            "(91, 2)\n"
          ]
        }
      ],
      "source": [
        " print(X_train.shape)\n",
        " print(y_train.shape)\n",
        " print(X_val.shape)\n",
        " print(y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiVDHNWOC2wL",
        "outputId": "ea0e88d3-687c-4fa0-d14f-796594ed5163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the updated weights and biases matrix: (5, 10, 167)\n",
            "Updated weights and biases matrix:\n",
            "[[[-0.22957684  0.44709624  0.20670588 ...  1.59012343  0.28540357\n",
            "   -0.28540357]\n",
            "  [ 0.29616377 -0.09754563 -0.27140158 ... -1.32880739 -0.18803613\n",
            "    0.18803613]\n",
            "  [-0.09690713  0.28163799 -0.17360499 ...  0.40915746  0.32253474\n",
            "   -0.32253474]\n",
            "  ...\n",
            "  [ 0.37710261  0.26877841 -0.2989743  ...  0.17967245 -0.12622952\n",
            "    0.12622952]\n",
            "  [ 0.03647079 -0.17374999 -0.35261195 ... -0.93554273  0.03921484\n",
            "   -0.03921484]\n",
            "  [ 0.16108269  0.26761214 -0.01564679 ...  0.62039547 -0.05670913\n",
            "    0.05670913]]\n",
            "\n",
            " [[-0.29412367 -0.20501163 -0.21632109 ... -0.54192675 -0.70236712\n",
            "    0.70236712]\n",
            "  [ 0.32704375 -0.07089488  0.09837886 ...  0.55680455  0.44599614\n",
            "   -0.44599614]\n",
            "  [-0.39072507  0.33056313  0.01226628 ... -0.29973522 -0.68138175\n",
            "    0.68138175]\n",
            "  ...\n",
            "  [-0.03287107  0.25941157 -0.37102298 ...  0.82085759  0.27574865\n",
            "   -0.27574865]\n",
            "  [ 0.2510877  -0.60465741  0.41133725 ... -0.14470714  0.39514011\n",
            "   -0.39514011]\n",
            "  [ 0.27875604 -0.09881102 -0.2534589  ...  0.39317691  0.35616025\n",
            "   -0.35616025]]\n",
            "\n",
            " [[-0.06001795 -0.21657971 -0.28650034 ... -1.21408351  0.11831282\n",
            "   -0.11831282]\n",
            "  [ 0.20494839  0.42861668  0.11237912 ... -0.52890251 -0.07623993\n",
            "    0.07623993]\n",
            "  [ 0.03226196  0.40590194 -0.01670841 ...  0.65800538  0.66802968\n",
            "   -0.66802968]\n",
            "  ...\n",
            "  [ 0.38329166 -0.11221877  0.16973702 ...  0.18617397  0.52400007\n",
            "   -0.52400007]\n",
            "  [-0.26951451 -0.23483846  0.18879617 ... -0.6315788  -0.54604275\n",
            "    0.54604275]\n",
            "  [-0.26693912 -0.01629917 -0.47586789 ...  1.04550789  0.43958724\n",
            "   -0.43958724]]\n",
            "\n",
            " [[ 0.44204126 -0.50825243  0.14269074 ... -0.28186727  0.20996442\n",
            "   -0.20996442]\n",
            "  [-0.17387553 -0.46895412  0.46876845 ...  0.54032156  0.28404881\n",
            "   -0.28404881]\n",
            "  [-0.03218198  0.55528017 -0.49535147 ... -0.36008075  0.34483537\n",
            "   -0.34483537]\n",
            "  ...\n",
            "  [ 0.27378884 -0.32828729  0.07388323 ... -1.09520178 -0.38815218\n",
            "    0.38815218]\n",
            "  [-0.48104032 -0.18707903  0.51002016 ...  2.46061244  0.40330726\n",
            "   -0.40330726]\n",
            "  [ 0.3558068   0.43661839 -0.30906458 ... -2.32274751  0.40007705\n",
            "   -0.40007705]]\n",
            "\n",
            " [[-0.05115392 -0.02513929 -0.53329447 ...  0.92269346  0.55891669\n",
            "   -0.55891669]\n",
            "  [-0.0408491  -0.05214372  0.11023731 ... -2.16325508  0.44717603\n",
            "   -0.44717603]\n",
            "  [ 0.33467218 -0.02610891 -0.16757024 ... -0.10177481 -0.2174943\n",
            "    0.2174943 ]\n",
            "  ...\n",
            "  [ 0.05671932 -0.064811    0.0120654  ...  0.73209738 -0.41650756\n",
            "    0.41650756]\n",
            "  [-0.00569561  0.02421343 -0.29005607 ...  0.42067855 -0.76758959\n",
            "    0.76758959]\n",
            "  [ 0.53559626 -0.28389791 -0.20856171 ...  0.80802059 -0.1370839\n",
            "    0.1370839 ]]]\n"
          ]
        }
      ],
      "source": [
        "# Print the matrix\n",
        "print(\"Shape of the updated weights and biases matrix:\", updated_weights_biases.shape)\n",
        "print(\"Updated weights and biases matrix:\")\n",
        "print(updated_weights_biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZxdBCz8DPXB",
        "outputId": "4bbe893e-29d7-4b7c-d4c5-41ad7eafeb20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate = 0.001\n",
            "Sample 1 Initialization 1: 0.46 0.54 | Initialization 2: 0.82 0.18 | Initialization 3: 0.47 0.53 | Initialization 4: 0.57 0.43 | Initialization 5: 0.71 0.29 | Initialization 6: 0.40 0.60 | Initialization 7: 0.52 0.48 | Initialization 8: 0.75 0.25 | Initialization 9: 0.64 0.36 | Initialization 10: 0.73 0.27 | \n",
            "Sample 2 Initialization 1: 0.23 0.77 | Initialization 2: 0.66 0.34 | Initialization 3: 0.26 0.74 | Initialization 4: 0.40 0.60 | Initialization 5: 0.61 0.39 | Initialization 6: 0.38 0.62 | Initialization 7: 0.27 0.73 | Initialization 8: 0.72 0.28 | Initialization 9: 0.68 0.32 | Initialization 10: 0.38 0.62 | \n",
            "Sample 3 Initialization 1: 0.21 0.79 | Initialization 2: 0.65 0.35 | Initialization 3: 0.28 0.72 | Initialization 4: 0.36 0.64 | Initialization 5: 0.68 0.32 | Initialization 6: 0.34 0.66 | Initialization 7: 0.37 0.63 | Initialization 8: 0.68 0.32 | Initialization 9: 0.62 0.38 | Initialization 10: 0.35 0.65 | \n",
            "Sample 4 Initialization 1: 0.55 0.45 | Initialization 2: 0.81 0.19 | Initialization 3: 0.47 0.53 | Initialization 4: 0.42 0.58 | Initialization 5: 0.65 0.35 | Initialization 6: 0.47 0.53 | Initialization 7: 0.56 0.44 | Initialization 8: 0.68 0.32 | Initialization 9: 0.54 0.46 | Initialization 10: 0.79 0.21 | \n",
            "Sample 5 Initialization 1: 0.61 0.39 | Initialization 2: 0.90 0.10 | Initialization 3: 0.55 0.45 | Initialization 4: 0.47 0.53 | Initialization 5: 0.66 0.34 | Initialization 6: 0.44 0.56 | Initialization 7: 0.58 0.42 | Initialization 8: 0.72 0.28 | Initialization 9: 0.55 0.45 | Initialization 10: 0.83 0.17 | \n",
            "Sample 6 Initialization 1: 0.19 0.81 | Initialization 2: 0.25 0.75 | Initialization 3: 0.20 0.80 | Initialization 4: 0.28 0.72 | Initialization 5: 0.33 0.67 | Initialization 6: 0.31 0.69 | Initialization 7: 0.19 0.81 | Initialization 8: 0.49 0.51 | Initialization 9: 0.40 0.60 | Initialization 10: 0.16 0.84 | \n",
            "Sample 7 Initialization 1: 0.14 0.86 | Initialization 2: 0.34 0.66 | Initialization 3: 0.15 0.85 | Initialization 4: 0.17 0.83 | Initialization 5: 0.46 0.54 | Initialization 6: 0.34 0.66 | Initialization 7: 0.18 0.82 | Initialization 8: 0.57 0.43 | Initialization 9: 0.59 0.41 | Initialization 10: 0.23 0.77 | \n",
            "Sample 8 Initialization 1: 0.39 0.61 | Initialization 2: 0.74 0.26 | Initialization 3: 0.27 0.73 | Initialization 4: 0.44 0.56 | Initialization 5: 0.63 0.37 | Initialization 6: 0.41 0.59 | Initialization 7: 0.30 0.70 | Initialization 8: 0.67 0.33 | Initialization 9: 0.64 0.36 | Initialization 10: 0.48 0.52 | \n",
            "Sample 9 Initialization 1: 0.37 0.63 | Initialization 2: 0.63 0.37 | Initialization 3: 0.36 0.64 | Initialization 4: 0.44 0.56 | Initialization 5: 0.63 0.37 | Initialization 6: 0.36 0.64 | Initialization 7: 0.30 0.70 | Initialization 8: 0.63 0.37 | Initialization 9: 0.52 0.48 | Initialization 10: 0.58 0.42 | \n",
            "Sample 10 Initialization 1: 0.65 0.35 | Initialization 2: 0.93 0.07 | Initialization 3: 0.61 0.39 | Initialization 4: 0.74 0.26 | Initialization 5: 0.73 0.27 | Initialization 6: 0.44 0.56 | Initialization 7: 0.53 0.47 | Initialization 8: 0.85 0.15 | Initialization 9: 0.74 0.26 | Initialization 10: 0.87 0.13 | \n",
            "Sample 11 Initialization 1: 0.45 0.55 | Initialization 2: 0.89 0.11 | Initialization 3: 0.54 0.46 | Initialization 4: 0.69 0.31 | Initialization 5: 0.76 0.24 | Initialization 6: 0.46 0.54 | Initialization 7: 0.67 0.33 | Initialization 8: 0.86 0.14 | Initialization 9: 0.70 0.30 | Initialization 10: 0.84 0.16 | \n",
            "Sample 12 Initialization 1: 0.30 0.70 | Initialization 2: 0.80 0.20 | Initialization 3: 0.33 0.67 | Initialization 4: 0.41 0.59 | Initialization 5: 0.63 0.37 | Initialization 6: 0.35 0.65 | Initialization 7: 0.38 0.62 | Initialization 8: 0.73 0.27 | Initialization 9: 0.65 0.35 | Initialization 10: 0.53 0.47 | \n",
            "Sample 13 Initialization 1: 0.57 0.43 | Initialization 2: 0.93 0.07 | Initialization 3: 0.53 0.47 | Initialization 4: 0.71 0.29 | Initialization 5: 0.81 0.19 | Initialization 6: 0.44 0.56 | Initialization 7: 0.63 0.37 | Initialization 8: 0.86 0.14 | Initialization 9: 0.72 0.28 | Initialization 10: 0.86 0.14 | \n",
            "Sample 14 Initialization 1: 0.27 0.73 | Initialization 2: 0.65 0.35 | Initialization 3: 0.19 0.81 | Initialization 4: 0.22 0.78 | Initialization 5: 0.68 0.32 | Initialization 6: 0.46 0.54 | Initialization 7: 0.45 0.55 | Initialization 8: 0.57 0.43 | Initialization 9: 0.51 0.49 | Initialization 10: 0.53 0.47 | \n",
            "Sample 15 Initialization 1: 0.69 0.31 | Initialization 2: 0.91 0.09 | Initialization 3: 0.58 0.42 | Initialization 4: 0.78 0.22 | Initialization 5: 0.81 0.19 | Initialization 6: 0.44 0.56 | Initialization 7: 0.54 0.46 | Initialization 8: 0.84 0.16 | Initialization 9: 0.73 0.27 | Initialization 10: 0.80 0.20 | \n",
            "Sample 16 Initialization 1: 0.11 0.89 | Initialization 2: 0.53 0.47 | Initialization 3: 0.18 0.82 | Initialization 4: 0.17 0.83 | Initialization 5: 0.44 0.56 | Initialization 6: 0.52 0.48 | Initialization 7: 0.34 0.66 | Initialization 8: 0.58 0.42 | Initialization 9: 0.49 0.51 | Initialization 10: 0.53 0.47 | \n",
            "Sample 17 Initialization 1: 0.63 0.37 | Initialization 2: 0.93 0.07 | Initialization 3: 0.50 0.50 | Initialization 4: 0.67 0.33 | Initialization 5: 0.82 0.18 | Initialization 6: 0.43 0.57 | Initialization 7: 0.62 0.38 | Initialization 8: 0.83 0.17 | Initialization 9: 0.72 0.28 | Initialization 10: 0.76 0.24 | \n",
            "Sample 18 Initialization 1: 0.61 0.39 | Initialization 2: 0.94 0.06 | Initialization 3: 0.58 0.42 | Initialization 4: 0.74 0.26 | Initialization 5: 0.83 0.17 | Initialization 6: 0.58 0.42 | Initialization 7: 0.73 0.27 | Initialization 8: 0.78 0.22 | Initialization 9: 0.61 0.39 | Initialization 10: 0.92 0.08 | \n",
            "Sample 19 Initialization 1: 0.69 0.31 | Initialization 2: 0.93 0.07 | Initialization 3: 0.53 0.47 | Initialization 4: 0.77 0.23 | Initialization 5: 0.79 0.21 | Initialization 6: 0.61 0.39 | Initialization 7: 0.67 0.33 | Initialization 8: 0.76 0.24 | Initialization 9: 0.64 0.36 | Initialization 10: 0.89 0.11 | \n",
            "Sample 20 Initialization 1: 0.15 0.85 | Initialization 2: 0.44 0.56 | Initialization 3: 0.17 0.83 | Initialization 4: 0.20 0.80 | Initialization 5: 0.52 0.48 | Initialization 6: 0.31 0.69 | Initialization 7: 0.21 0.79 | Initialization 8: 0.58 0.42 | Initialization 9: 0.53 0.47 | Initialization 10: 0.29 0.71 | \n",
            "Sample 21 Initialization 1: 0.53 0.47 | Initialization 2: 0.75 0.25 | Initialization 3: 0.46 0.54 | Initialization 4: 0.64 0.36 | Initialization 5: 0.65 0.35 | Initialization 6: 0.47 0.53 | Initialization 7: 0.41 0.59 | Initialization 8: 0.78 0.22 | Initialization 9: 0.64 0.36 | Initialization 10: 0.64 0.36 | \n",
            "Sample 22 Initialization 1: 0.46 0.54 | Initialization 2: 0.88 0.12 | Initialization 3: 0.50 0.50 | Initialization 4: 0.67 0.33 | Initialization 5: 0.80 0.20 | Initialization 6: 0.45 0.55 | Initialization 7: 0.62 0.38 | Initialization 8: 0.78 0.22 | Initialization 9: 0.62 0.38 | Initialization 10: 0.82 0.18 | \n",
            "Sample 23 Initialization 1: 0.17 0.83 | Initialization 2: 0.30 0.70 | Initialization 3: 0.16 0.84 | Initialization 4: 0.22 0.78 | Initialization 5: 0.48 0.52 | Initialization 6: 0.29 0.71 | Initialization 7: 0.18 0.82 | Initialization 8: 0.51 0.49 | Initialization 9: 0.44 0.56 | Initialization 10: 0.17 0.83 | \n",
            "Sample 24 Initialization 1: 0.63 0.37 | Initialization 2: 0.94 0.06 | Initialization 3: 0.66 0.34 | Initialization 4: 0.77 0.23 | Initialization 5: 0.76 0.24 | Initialization 6: 0.52 0.48 | Initialization 7: 0.71 0.29 | Initialization 8: 0.87 0.13 | Initialization 9: 0.74 0.26 | Initialization 10: 0.93 0.07 | \n",
            "Sample 25 Initialization 1: 0.61 0.39 | Initialization 2: 0.92 0.08 | Initialization 3: 0.55 0.45 | Initialization 4: 0.73 0.27 | Initialization 5: 0.77 0.23 | Initialization 6: 0.48 0.52 | Initialization 7: 0.65 0.35 | Initialization 8: 0.83 0.17 | Initialization 9: 0.68 0.32 | Initialization 10: 0.93 0.07 | \n",
            "Sample 26 Initialization 1: 0.36 0.64 | Initialization 2: 0.88 0.12 | Initialization 3: 0.31 0.69 | Initialization 4: 0.56 0.44 | Initialization 5: 0.79 0.21 | Initialization 6: 0.47 0.53 | Initialization 7: 0.63 0.37 | Initialization 8: 0.60 0.40 | Initialization 9: 0.60 0.40 | Initialization 10: 0.71 0.29 | \n",
            "Sample 27 Initialization 1: 0.67 0.33 | Initialization 2: 0.91 0.09 | Initialization 3: 0.57 0.43 | Initialization 4: 0.76 0.24 | Initialization 5: 0.76 0.24 | Initialization 6: 0.43 0.57 | Initialization 7: 0.45 0.55 | Initialization 8: 0.82 0.18 | Initialization 9: 0.71 0.29 | Initialization 10: 0.79 0.21 | \n",
            "Sample 28 Initialization 1: 0.55 0.45 | Initialization 2: 0.85 0.15 | Initialization 3: 0.59 0.41 | Initialization 4: 0.41 0.59 | Initialization 5: 0.66 0.34 | Initialization 6: 0.42 0.58 | Initialization 7: 0.70 0.30 | Initialization 8: 0.79 0.21 | Initialization 9: 0.58 0.42 | Initialization 10: 0.83 0.17 | \n",
            "Sample 29 Initialization 1: 0.57 0.43 | Initialization 2: 0.90 0.10 | Initialization 3: 0.53 0.47 | Initialization 4: 0.71 0.29 | Initialization 5: 0.79 0.21 | Initialization 6: 0.48 0.52 | Initialization 7: 0.63 0.37 | Initialization 8: 0.83 0.17 | Initialization 9: 0.67 0.33 | Initialization 10: 0.86 0.14 | \n",
            "Sample 30 Initialization 1: 0.20 0.80 | Initialization 2: 0.33 0.67 | Initialization 3: 0.27 0.73 | Initialization 4: 0.33 0.67 | Initialization 5: 0.36 0.64 | Initialization 6: 0.35 0.65 | Initialization 7: 0.19 0.81 | Initialization 8: 0.56 0.44 | Initialization 9: 0.47 0.53 | Initialization 10: 0.30 0.70 | \n",
            "Sample 31 Initialization 1: 0.58 0.42 | Initialization 2: 0.90 0.10 | Initialization 3: 0.51 0.49 | Initialization 4: 0.71 0.29 | Initialization 5: 0.70 0.30 | Initialization 6: 0.47 0.53 | Initialization 7: 0.61 0.39 | Initialization 8: 0.82 0.18 | Initialization 9: 0.67 0.33 | Initialization 10: 0.93 0.07 | \n",
            "Sample 32 Initialization 1: 0.69 0.31 | Initialization 2: 0.94 0.06 | Initialization 3: 0.56 0.44 | Initialization 4: 0.75 0.25 | Initialization 5: 0.81 0.19 | Initialization 6: 0.50 0.50 | Initialization 7: 0.62 0.38 | Initialization 8: 0.85 0.15 | Initialization 9: 0.74 0.26 | Initialization 10: 0.85 0.15 | \n",
            "Sample 33 Initialization 1: 0.52 0.48 | Initialization 2: 0.89 0.11 | Initialization 3: 0.43 0.57 | Initialization 4: 0.64 0.36 | Initialization 5: 0.79 0.21 | Initialization 6: 0.49 0.51 | Initialization 7: 0.73 0.27 | Initialization 8: 0.65 0.35 | Initialization 9: 0.62 0.38 | Initialization 10: 0.83 0.17 | \n",
            "Sample 34 Initialization 1: 0.64 0.36 | Initialization 2: 0.91 0.09 | Initialization 3: 0.53 0.47 | Initialization 4: 0.74 0.26 | Initialization 5: 0.80 0.20 | Initialization 6: 0.47 0.53 | Initialization 7: 0.56 0.44 | Initialization 8: 0.85 0.15 | Initialization 9: 0.72 0.28 | Initialization 10: 0.82 0.18 | \n",
            "Sample 35 Initialization 1: 0.64 0.36 | Initialization 2: 0.90 0.10 | Initialization 3: 0.51 0.49 | Initialization 4: 0.73 0.27 | Initialization 5: 0.81 0.19 | Initialization 6: 0.54 0.46 | Initialization 7: 0.69 0.31 | Initialization 8: 0.78 0.22 | Initialization 9: 0.66 0.34 | Initialization 10: 0.87 0.13 | \n",
            "Sample 36 Initialization 1: 0.63 0.37 | Initialization 2: 0.91 0.09 | Initialization 3: 0.47 0.53 | Initialization 4: 0.70 0.30 | Initialization 5: 0.86 0.14 | Initialization 6: 0.45 0.55 | Initialization 7: 0.69 0.31 | Initialization 8: 0.78 0.22 | Initialization 9: 0.70 0.30 | Initialization 10: 0.86 0.14 | \n",
            "Sample 37 Initialization 1: 0.38 0.62 | Initialization 2: 0.83 0.17 | Initialization 3: 0.47 0.53 | Initialization 4: 0.57 0.43 | Initialization 5: 0.66 0.34 | Initialization 6: 0.42 0.58 | Initialization 7: 0.52 0.48 | Initialization 8: 0.84 0.16 | Initialization 9: 0.70 0.30 | Initialization 10: 0.70 0.30 | \n",
            "Sample 38 Initialization 1: 0.65 0.35 | Initialization 2: 0.95 0.05 | Initialization 3: 0.50 0.50 | Initialization 4: 0.65 0.35 | Initialization 5: 0.78 0.22 | Initialization 6: 0.40 0.60 | Initialization 7: 0.68 0.32 | Initialization 8: 0.80 0.20 | Initialization 9: 0.71 0.29 | Initialization 10: 0.86 0.14 | \n",
            "Sample 39 Initialization 1: 0.23 0.77 | Initialization 2: 0.62 0.38 | Initialization 3: 0.27 0.73 | Initialization 4: 0.32 0.68 | Initialization 5: 0.60 0.40 | Initialization 6: 0.32 0.68 | Initialization 7: 0.27 0.73 | Initialization 8: 0.71 0.29 | Initialization 9: 0.61 0.39 | Initialization 10: 0.30 0.70 | \n",
            "Sample 40 Initialization 1: 0.44 0.56 | Initialization 2: 0.88 0.12 | Initialization 3: 0.45 0.55 | Initialization 4: 0.69 0.31 | Initialization 5: 0.76 0.24 | Initialization 6: 0.48 0.52 | Initialization 7: 0.62 0.38 | Initialization 8: 0.83 0.17 | Initialization 9: 0.70 0.30 | Initialization 10: 0.86 0.14 | \n",
            "Sample 41 Initialization 1: 0.59 0.41 | Initialization 2: 0.92 0.08 | Initialization 3: 0.53 0.47 | Initialization 4: 0.70 0.30 | Initialization 5: 0.78 0.22 | Initialization 6: 0.50 0.50 | Initialization 7: 0.68 0.32 | Initialization 8: 0.79 0.21 | Initialization 9: 0.65 0.35 | Initialization 10: 0.92 0.08 | \n",
            "Sample 42 Initialization 1: 0.25 0.75 | Initialization 2: 0.58 0.42 | Initialization 3: 0.30 0.70 | Initialization 4: 0.23 0.77 | Initialization 5: 0.55 0.45 | Initialization 6: 0.35 0.65 | Initialization 7: 0.48 0.52 | Initialization 8: 0.63 0.37 | Initialization 9: 0.54 0.46 | Initialization 10: 0.39 0.61 | \n",
            "Sample 43 Initialization 1: 0.56 0.44 | Initialization 2: 0.87 0.13 | Initialization 3: 0.46 0.54 | Initialization 4: 0.71 0.29 | Initialization 5: 0.80 0.20 | Initialization 6: 0.45 0.55 | Initialization 7: 0.57 0.43 | Initialization 8: 0.76 0.24 | Initialization 9: 0.66 0.34 | Initialization 10: 0.77 0.23 | \n",
            "Sample 44 Initialization 1: 0.65 0.35 | Initialization 2: 0.91 0.09 | Initialization 3: 0.60 0.40 | Initialization 4: 0.76 0.24 | Initialization 5: 0.76 0.24 | Initialization 6: 0.50 0.50 | Initialization 7: 0.61 0.39 | Initialization 8: 0.85 0.15 | Initialization 9: 0.71 0.29 | Initialization 10: 0.87 0.13 | \n",
            "Sample 45 Initialization 1: 0.45 0.55 | Initialization 2: 0.90 0.10 | Initialization 3: 0.35 0.65 | Initialization 4: 0.51 0.49 | Initialization 5: 0.83 0.17 | Initialization 6: 0.48 0.52 | Initialization 7: 0.74 0.26 | Initialization 8: 0.67 0.33 | Initialization 9: 0.61 0.39 | Initialization 10: 0.80 0.20 | \n",
            "Sample 46 Initialization 1: 0.63 0.37 | Initialization 2: 0.83 0.17 | Initialization 3: 0.59 0.41 | Initialization 4: 0.66 0.34 | Initialization 5: 0.74 0.26 | Initialization 6: 0.45 0.55 | Initialization 7: 0.57 0.43 | Initialization 8: 0.79 0.21 | Initialization 9: 0.63 0.37 | Initialization 10: 0.73 0.27 | \n",
            "Sample 47 Initialization 1: 0.55 0.45 | Initialization 2: 0.81 0.19 | Initialization 3: 0.37 0.63 | Initialization 4: 0.63 0.37 | Initialization 5: 0.80 0.20 | Initialization 6: 0.47 0.53 | Initialization 7: 0.68 0.32 | Initialization 8: 0.58 0.42 | Initialization 9: 0.50 0.50 | Initialization 10: 0.71 0.29 | \n",
            "Sample 48 Initialization 1: 0.63 0.37 | Initialization 2: 0.93 0.07 | Initialization 3: 0.65 0.35 | Initialization 4: 0.74 0.26 | Initialization 5: 0.81 0.19 | Initialization 6: 0.56 0.44 | Initialization 7: 0.73 0.27 | Initialization 8: 0.82 0.18 | Initialization 9: 0.67 0.33 | Initialization 10: 0.93 0.07 | \n",
            "Sample 49 Initialization 1: 0.47 0.53 | Initialization 2: 0.81 0.19 | Initialization 3: 0.53 0.47 | Initialization 4: 0.73 0.27 | Initialization 5: 0.75 0.25 | Initialization 6: 0.42 0.58 | Initialization 7: 0.54 0.46 | Initialization 8: 0.80 0.20 | Initialization 9: 0.65 0.35 | Initialization 10: 0.79 0.21 | \n",
            "Sample 50 Initialization 1: 0.68 0.32 | Initialization 2: 0.91 0.09 | Initialization 3: 0.61 0.39 | Initialization 4: 0.56 0.44 | Initialization 5: 0.54 0.46 | Initialization 6: 0.41 0.59 | Initialization 7: 0.44 0.56 | Initialization 8: 0.80 0.20 | Initialization 9: 0.64 0.36 | Initialization 10: 0.81 0.19 | \n",
            "Sample 51 Initialization 1: 0.25 0.75 | Initialization 2: 0.64 0.36 | Initialization 3: 0.30 0.70 | Initialization 4: 0.39 0.61 | Initialization 5: 0.67 0.33 | Initialization 6: 0.30 0.70 | Initialization 7: 0.31 0.69 | Initialization 8: 0.74 0.26 | Initialization 9: 0.61 0.39 | Initialization 10: 0.33 0.67 | \n",
            "Sample 52 Initialization 1: 0.17 0.83 | Initialization 2: 0.33 0.67 | Initialization 3: 0.21 0.79 | Initialization 4: 0.22 0.78 | Initialization 5: 0.43 0.57 | Initialization 6: 0.28 0.72 | Initialization 7: 0.20 0.80 | Initialization 8: 0.54 0.46 | Initialization 9: 0.52 0.48 | Initialization 10: 0.24 0.76 | \n",
            "Sample 53 Initialization 1: 0.39 0.61 | Initialization 2: 0.59 0.41 | Initialization 3: 0.38 0.62 | Initialization 4: 0.59 0.41 | Initialization 5: 0.67 0.33 | Initialization 6: 0.40 0.60 | Initialization 7: 0.36 0.64 | Initialization 8: 0.66 0.34 | Initialization 9: 0.56 0.44 | Initialization 10: 0.78 0.22 | \n",
            "Sample 54 Initialization 1: 0.39 0.61 | Initialization 2: 0.90 0.10 | Initialization 3: 0.38 0.62 | Initialization 4: 0.22 0.78 | Initialization 5: 0.69 0.31 | Initialization 6: 0.35 0.65 | Initialization 7: 0.61 0.39 | Initialization 8: 0.71 0.29 | Initialization 9: 0.55 0.45 | Initialization 10: 0.68 0.32 | \n",
            "Sample 55 Initialization 1: 0.58 0.42 | Initialization 2: 0.84 0.16 | Initialization 3: 0.39 0.61 | Initialization 4: 0.61 0.39 | Initialization 5: 0.69 0.31 | Initialization 6: 0.58 0.42 | Initialization 7: 0.55 0.45 | Initialization 8: 0.63 0.37 | Initialization 9: 0.61 0.39 | Initialization 10: 0.83 0.17 | \n",
            "Sample 56 Initialization 1: 0.66 0.34 | Initialization 2: 0.91 0.09 | Initialization 3: 0.61 0.39 | Initialization 4: 0.74 0.26 | Initialization 5: 0.74 0.26 | Initialization 6: 0.47 0.53 | Initialization 7: 0.56 0.44 | Initialization 8: 0.85 0.15 | Initialization 9: 0.71 0.29 | Initialization 10: 0.75 0.25 | \n",
            "Sample 57 Initialization 1: 0.66 0.34 | Initialization 2: 0.86 0.14 | Initialization 3: 0.50 0.50 | Initialization 4: 0.53 0.47 | Initialization 5: 0.68 0.32 | Initialization 6: 0.51 0.49 | Initialization 7: 0.59 0.41 | Initialization 8: 0.70 0.30 | Initialization 9: 0.58 0.42 | Initialization 10: 0.85 0.15 | \n",
            "Sample 58 Initialization 1: 0.15 0.85 | Initialization 2: 0.30 0.70 | Initialization 3: 0.16 0.84 | Initialization 4: 0.18 0.82 | Initialization 5: 0.38 0.62 | Initialization 6: 0.31 0.69 | Initialization 7: 0.18 0.82 | Initialization 8: 0.50 0.50 | Initialization 9: 0.45 0.55 | Initialization 10: 0.14 0.86 | \n",
            "Sample 59 Initialization 1: 0.50 0.50 | Initialization 2: 0.55 0.45 | Initialization 3: 0.43 0.57 | Initialization 4: 0.47 0.53 | Initialization 5: 0.62 0.38 | Initialization 6: 0.38 0.62 | Initialization 7: 0.34 0.66 | Initialization 8: 0.71 0.29 | Initialization 9: 0.58 0.42 | Initialization 10: 0.47 0.53 | \n",
            "Sample 60 Initialization 1: 0.69 0.31 | Initialization 2: 0.92 0.08 | Initialization 3: 0.52 0.48 | Initialization 4: 0.74 0.26 | Initialization 5: 0.81 0.19 | Initialization 6: 0.49 0.51 | Initialization 7: 0.62 0.38 | Initialization 8: 0.82 0.18 | Initialization 9: 0.71 0.29 | Initialization 10: 0.83 0.17 | \n",
            "Sample 61 Initialization 1: 0.55 0.45 | Initialization 2: 0.88 0.12 | Initialization 3: 0.53 0.47 | Initialization 4: 0.71 0.29 | Initialization 5: 0.78 0.22 | Initialization 6: 0.47 0.53 | Initialization 7: 0.58 0.42 | Initialization 8: 0.82 0.18 | Initialization 9: 0.68 0.32 | Initialization 10: 0.83 0.17 | \n",
            "Sample 62 Initialization 1: 0.18 0.82 | Initialization 2: 0.38 0.62 | Initialization 3: 0.17 0.83 | Initialization 4: 0.20 0.80 | Initialization 5: 0.55 0.45 | Initialization 6: 0.29 0.71 | Initialization 7: 0.21 0.79 | Initialization 8: 0.55 0.45 | Initialization 9: 0.53 0.47 | Initialization 10: 0.21 0.79 | \n",
            "Sample 63 Initialization 1: 0.11 0.89 | Initialization 2: 0.42 0.58 | Initialization 3: 0.15 0.85 | Initialization 4: 0.14 0.86 | Initialization 5: 0.50 0.50 | Initialization 6: 0.28 0.72 | Initialization 7: 0.23 0.77 | Initialization 8: 0.59 0.41 | Initialization 9: 0.61 0.39 | Initialization 10: 0.21 0.79 | \n",
            "Sample 64 Initialization 1: 0.50 0.50 | Initialization 2: 0.90 0.10 | Initialization 3: 0.52 0.48 | Initialization 4: 0.70 0.30 | Initialization 5: 0.71 0.29 | Initialization 6: 0.46 0.54 | Initialization 7: 0.61 0.39 | Initialization 8: 0.85 0.15 | Initialization 9: 0.71 0.29 | Initialization 10: 0.87 0.13 | \n",
            "Sample 65 Initialization 1: 0.62 0.38 | Initialization 2: 0.94 0.06 | Initialization 3: 0.62 0.38 | Initialization 4: 0.71 0.29 | Initialization 5: 0.79 0.21 | Initialization 6: 0.48 0.52 | Initialization 7: 0.70 0.30 | Initialization 8: 0.85 0.15 | Initialization 9: 0.70 0.30 | Initialization 10: 0.87 0.13 | \n",
            "Sample 66 Initialization 1: 0.60 0.40 | Initialization 2: 0.81 0.19 | Initialization 3: 0.59 0.41 | Initialization 4: 0.68 0.32 | Initialization 5: 0.64 0.36 | Initialization 6: 0.40 0.60 | Initialization 7: 0.46 0.54 | Initialization 8: 0.82 0.18 | Initialization 9: 0.67 0.33 | Initialization 10: 0.83 0.17 | \n",
            "Sample 67 Initialization 1: 0.20 0.80 | Initialization 2: 0.38 0.62 | Initialization 3: 0.22 0.78 | Initialization 4: 0.28 0.72 | Initialization 5: 0.39 0.61 | Initialization 6: 0.35 0.65 | Initialization 7: 0.20 0.80 | Initialization 8: 0.52 0.48 | Initialization 9: 0.45 0.55 | Initialization 10: 0.22 0.78 | \n",
            "Sample 68 Initialization 1: 0.13 0.87 | Initialization 2: 0.30 0.70 | Initialization 3: 0.13 0.87 | Initialization 4: 0.16 0.84 | Initialization 5: 0.49 0.51 | Initialization 6: 0.24 0.76 | Initialization 7: 0.22 0.78 | Initialization 8: 0.42 0.58 | Initialization 9: 0.39 0.61 | Initialization 10: 0.24 0.76 | \n",
            "Sample 69 Initialization 1: 0.62 0.38 | Initialization 2: 0.93 0.07 | Initialization 3: 0.54 0.46 | Initialization 4: 0.70 0.30 | Initialization 5: 0.85 0.15 | Initialization 6: 0.48 0.52 | Initialization 7: 0.70 0.30 | Initialization 8: 0.82 0.18 | Initialization 9: 0.64 0.36 | Initialization 10: 0.87 0.13 | \n",
            "Sample 70 Initialization 1: 0.63 0.37 | Initialization 2: 0.78 0.22 | Initialization 3: 0.44 0.56 | Initialization 4: 0.66 0.34 | Initialization 5: 0.73 0.27 | Initialization 6: 0.41 0.59 | Initialization 7: 0.40 0.60 | Initialization 8: 0.70 0.30 | Initialization 9: 0.63 0.37 | Initialization 10: 0.79 0.21 | \n",
            "Sample 71 Initialization 1: 0.26 0.74 | Initialization 2: 0.54 0.46 | Initialization 3: 0.25 0.75 | Initialization 4: 0.23 0.77 | Initialization 5: 0.62 0.38 | Initialization 6: 0.34 0.66 | Initialization 7: 0.50 0.50 | Initialization 8: 0.53 0.47 | Initialization 9: 0.48 0.52 | Initialization 10: 0.38 0.62 | \n",
            "Sample 72 Initialization 1: 0.32 0.68 | Initialization 2: 0.51 0.49 | Initialization 3: 0.33 0.67 | Initialization 4: 0.46 0.54 | Initialization 5: 0.53 0.47 | Initialization 6: 0.32 0.68 | Initialization 7: 0.22 0.78 | Initialization 8: 0.62 0.38 | Initialization 9: 0.57 0.43 | Initialization 10: 0.45 0.55 | \n",
            "Sample 73 Initialization 1: 0.61 0.39 | Initialization 2: 0.94 0.06 | Initialization 3: 0.57 0.43 | Initialization 4: 0.69 0.31 | Initialization 5: 0.75 0.25 | Initialization 6: 0.46 0.54 | Initialization 7: 0.64 0.36 | Initialization 8: 0.85 0.15 | Initialization 9: 0.68 0.32 | Initialization 10: 0.87 0.13 | \n",
            "Sample 74 Initialization 1: 0.26 0.74 | Initialization 2: 0.50 0.50 | Initialization 3: 0.30 0.70 | Initialization 4: 0.30 0.70 | Initialization 5: 0.48 0.52 | Initialization 6: 0.30 0.70 | Initialization 7: 0.23 0.77 | Initialization 8: 0.59 0.41 | Initialization 9: 0.50 0.50 | Initialization 10: 0.37 0.63 | \n",
            "Sample 75 Initialization 1: 0.64 0.36 | Initialization 2: 0.90 0.10 | Initialization 3: 0.62 0.38 | Initialization 4: 0.74 0.26 | Initialization 5: 0.76 0.24 | Initialization 6: 0.55 0.45 | Initialization 7: 0.74 0.26 | Initialization 8: 0.78 0.22 | Initialization 9: 0.70 0.30 | Initialization 10: 0.87 0.13 | \n",
            "Sample 76 Initialization 1: 0.62 0.38 | Initialization 2: 0.82 0.18 | Initialization 3: 0.44 0.56 | Initialization 4: 0.70 0.30 | Initialization 5: 0.77 0.23 | Initialization 6: 0.47 0.53 | Initialization 7: 0.50 0.50 | Initialization 8: 0.78 0.22 | Initialization 9: 0.65 0.35 | Initialization 10: 0.74 0.26 | \n",
            "Sample 77 Initialization 1: 0.51 0.49 | Initialization 2: 0.90 0.10 | Initialization 3: 0.56 0.44 | Initialization 4: 0.63 0.37 | Initialization 5: 0.75 0.25 | Initialization 6: 0.49 0.51 | Initialization 7: 0.67 0.33 | Initialization 8: 0.85 0.15 | Initialization 9: 0.68 0.32 | Initialization 10: 0.79 0.21 | \n",
            "Sample 78 Initialization 1: 0.39 0.61 | Initialization 2: 0.74 0.26 | Initialization 3: 0.35 0.65 | Initialization 4: 0.55 0.45 | Initialization 5: 0.75 0.25 | Initialization 6: 0.39 0.61 | Initialization 7: 0.40 0.60 | Initialization 8: 0.68 0.32 | Initialization 9: 0.60 0.40 | Initialization 10: 0.55 0.45 | \n",
            "Sample 79 Initialization 1: 0.68 0.32 | Initialization 2: 0.95 0.05 | Initialization 3: 0.66 0.34 | Initialization 4: 0.78 0.22 | Initialization 5: 0.78 0.22 | Initialization 6: 0.51 0.49 | Initialization 7: 0.63 0.37 | Initialization 8: 0.85 0.15 | Initialization 9: 0.74 0.26 | Initialization 10: 0.92 0.08 | \n",
            "Sample 80 Initialization 1: 0.50 0.50 | Initialization 2: 0.78 0.22 | Initialization 3: 0.40 0.60 | Initialization 4: 0.61 0.39 | Initialization 5: 0.76 0.24 | Initialization 6: 0.43 0.57 | Initialization 7: 0.56 0.44 | Initialization 8: 0.70 0.30 | Initialization 9: 0.62 0.38 | Initialization 10: 0.70 0.30 | \n",
            "Sample 81 Initialization 1: 0.20 0.80 | Initialization 2: 0.65 0.35 | Initialization 3: 0.31 0.69 | Initialization 4: 0.40 0.60 | Initialization 5: 0.66 0.34 | Initialization 6: 0.30 0.70 | Initialization 7: 0.33 0.67 | Initialization 8: 0.71 0.29 | Initialization 9: 0.60 0.40 | Initialization 10: 0.52 0.48 | \n",
            "Sample 82 Initialization 1: 0.67 0.33 | Initialization 2: 0.95 0.05 | Initialization 3: 0.60 0.40 | Initialization 4: 0.75 0.25 | Initialization 5: 0.81 0.19 | Initialization 6: 0.49 0.51 | Initialization 7: 0.67 0.33 | Initialization 8: 0.85 0.15 | Initialization 9: 0.74 0.26 | Initialization 10: 0.90 0.10 | \n",
            "Sample 83 Initialization 1: 0.30 0.70 | Initialization 2: 0.80 0.20 | Initialization 3: 0.39 0.61 | Initialization 4: 0.27 0.73 | Initialization 5: 0.64 0.36 | Initialization 6: 0.29 0.71 | Initialization 7: 0.45 0.55 | Initialization 8: 0.68 0.32 | Initialization 9: 0.54 0.46 | Initialization 10: 0.63 0.37 | \n",
            "Sample 84 Initialization 1: 0.16 0.84 | Initialization 2: 0.36 0.64 | Initialization 3: 0.18 0.82 | Initialization 4: 0.16 0.84 | Initialization 5: 0.42 0.58 | Initialization 6: 0.24 0.76 | Initialization 7: 0.19 0.81 | Initialization 8: 0.56 0.44 | Initialization 9: 0.56 0.44 | Initialization 10: 0.36 0.64 | \n",
            "Sample 85 Initialization 1: 0.27 0.73 | Initialization 2: 0.77 0.23 | Initialization 3: 0.36 0.64 | Initialization 4: 0.40 0.60 | Initialization 5: 0.64 0.36 | Initialization 6: 0.41 0.59 | Initialization 7: 0.48 0.52 | Initialization 8: 0.81 0.19 | Initialization 9: 0.64 0.36 | Initialization 10: 0.51 0.49 | \n",
            "Sample 86 Initialization 1: 0.17 0.83 | Initialization 2: 0.42 0.58 | Initialization 3: 0.17 0.83 | Initialization 4: 0.25 0.75 | Initialization 5: 0.51 0.49 | Initialization 6: 0.38 0.62 | Initialization 7: 0.27 0.73 | Initialization 8: 0.50 0.50 | Initialization 9: 0.49 0.51 | Initialization 10: 0.23 0.77 | \n",
            "Sample 87 Initialization 1: 0.20 0.80 | Initialization 2: 0.36 0.64 | Initialization 3: 0.34 0.66 | Initialization 4: 0.33 0.67 | Initialization 5: 0.30 0.70 | Initialization 6: 0.40 0.60 | Initialization 7: 0.19 0.81 | Initialization 8: 0.52 0.48 | Initialization 9: 0.41 0.59 | Initialization 10: 0.41 0.59 | \n",
            "Sample 88 Initialization 1: 0.18 0.82 | Initialization 2: 0.51 0.49 | Initialization 3: 0.26 0.74 | Initialization 4: 0.21 0.79 | Initialization 5: 0.28 0.72 | Initialization 6: 0.40 0.60 | Initialization 7: 0.22 0.78 | Initialization 8: 0.56 0.44 | Initialization 9: 0.55 0.45 | Initialization 10: 0.43 0.57 | \n",
            "Sample 89 Initialization 1: 0.67 0.33 | Initialization 2: 0.87 0.13 | Initialization 3: 0.61 0.39 | Initialization 4: 0.63 0.37 | Initialization 5: 0.50 0.50 | Initialization 6: 0.52 0.48 | Initialization 7: 0.47 0.53 | Initialization 8: 0.81 0.19 | Initialization 9: 0.64 0.36 | Initialization 10: 0.89 0.11 | \n",
            "Sample 90 Initialization 1: 0.61 0.39 | Initialization 2: 0.87 0.13 | Initialization 3: 0.49 0.51 | Initialization 4: 0.67 0.33 | Initialization 5: 0.75 0.25 | Initialization 6: 0.51 0.49 | Initialization 7: 0.57 0.43 | Initialization 8: 0.81 0.19 | Initialization 9: 0.65 0.35 | Initialization 10: 0.71 0.29 | \n",
            "Sample 91 Initialization 1: 0.53 0.47 | Initialization 2: 0.90 0.10 | Initialization 3: 0.53 0.47 | Initialization 4: 0.70 0.30 | Initialization 5: 0.77 0.23 | Initialization 6: 0.38 0.62 | Initialization 7: 0.56 0.44 | Initialization 8: 0.80 0.20 | Initialization 9: 0.66 0.34 | Initialization 10: 0.80 0.20 | \n",
            "Sample 92 Initialization 1: 0.51 0.49 | Initialization 2: 0.73 0.27 | Initialization 3: 0.46 0.54 | Initialization 4: 0.69 0.31 | Initialization 5: 0.81 0.19 | Initialization 6: 0.42 0.58 | Initialization 7: 0.46 0.54 | Initialization 8: 0.78 0.22 | Initialization 9: 0.63 0.37 | Initialization 10: 0.65 0.35 | \n",
            "Sample 93 Initialization 1: 0.43 0.57 | Initialization 2: 0.85 0.15 | Initialization 3: 0.42 0.58 | Initialization 4: 0.62 0.38 | Initialization 5: 0.65 0.35 | Initialization 6: 0.49 0.51 | Initialization 7: 0.48 0.52 | Initialization 8: 0.80 0.20 | Initialization 9: 0.66 0.34 | Initialization 10: 0.77 0.23 | \n",
            "Sample 94 Initialization 1: 0.68 0.32 | Initialization 2: 0.95 0.05 | Initialization 3: 0.69 0.31 | Initialization 4: 0.74 0.26 | Initialization 5: 0.75 0.25 | Initialization 6: 0.54 0.46 | Initialization 7: 0.71 0.29 | Initialization 8: 0.87 0.13 | Initialization 9: 0.66 0.34 | Initialization 10: 0.96 0.04 | \n",
            "Sample 95 Initialization 1: 0.60 0.40 | Initialization 2: 0.91 0.09 | Initialization 3: 0.45 0.55 | Initialization 4: 0.69 0.31 | Initialization 5: 0.87 0.13 | Initialization 6: 0.52 0.48 | Initialization 7: 0.73 0.27 | Initialization 8: 0.70 0.30 | Initialization 9: 0.60 0.40 | Initialization 10: 0.85 0.15 | \n",
            "Sample 96 Initialization 1: 0.63 0.37 | Initialization 2: 0.89 0.11 | Initialization 3: 0.50 0.50 | Initialization 4: 0.68 0.32 | Initialization 5: 0.78 0.22 | Initialization 6: 0.59 0.41 | Initialization 7: 0.67 0.33 | Initialization 8: 0.66 0.34 | Initialization 9: 0.60 0.40 | Initialization 10: 0.85 0.15 | \n",
            "Sample 97 Initialization 1: 0.12 0.88 | Initialization 2: 0.58 0.42 | Initialization 3: 0.27 0.73 | Initialization 4: 0.20 0.80 | Initialization 5: 0.54 0.46 | Initialization 6: 0.33 0.67 | Initialization 7: 0.35 0.65 | Initialization 8: 0.70 0.30 | Initialization 9: 0.56 0.44 | Initialization 10: 0.47 0.53 | \n",
            "Sample 98 Initialization 1: 0.17 0.83 | Initialization 2: 0.41 0.59 | Initialization 3: 0.23 0.77 | Initialization 4: 0.30 0.70 | Initialization 5: 0.49 0.51 | Initialization 6: 0.35 0.65 | Initialization 7: 0.26 0.74 | Initialization 8: 0.54 0.46 | Initialization 9: 0.46 0.54 | Initialization 10: 0.29 0.71 | \n",
            "Sample 99 Initialization 1: 0.68 0.32 | Initialization 2: 0.94 0.06 | Initialization 3: 0.51 0.49 | Initialization 4: 0.72 0.28 | Initialization 5: 0.85 0.15 | Initialization 6: 0.48 0.52 | Initialization 7: 0.67 0.33 | Initialization 8: 0.79 0.21 | Initialization 9: 0.70 0.30 | Initialization 10: 0.85 0.15 | \n",
            "Sample 100 Initialization 1: 0.23 0.77 | Initialization 2: 0.59 0.41 | Initialization 3: 0.37 0.63 | Initialization 4: 0.30 0.70 | Initialization 5: 0.54 0.46 | Initialization 6: 0.31 0.69 | Initialization 7: 0.27 0.73 | Initialization 8: 0.68 0.32 | Initialization 9: 0.53 0.47 | Initialization 10: 0.40 0.60 | \n",
            "Sample 101 Initialization 1: 0.31 0.69 | Initialization 2: 0.66 0.34 | Initialization 3: 0.40 0.60 | Initialization 4: 0.55 0.45 | Initialization 5: 0.65 0.35 | Initialization 6: 0.35 0.65 | Initialization 7: 0.32 0.68 | Initialization 8: 0.77 0.23 | Initialization 9: 0.64 0.36 | Initialization 10: 0.48 0.52 | \n",
            "Sample 102 Initialization 1: 0.75 0.25 | Initialization 2: 0.95 0.05 | Initialization 3: 0.54 0.46 | Initialization 4: 0.76 0.24 | Initialization 5: 0.84 0.16 | Initialization 6: 0.52 0.48 | Initialization 7: 0.66 0.34 | Initialization 8: 0.83 0.17 | Initialization 9: 0.75 0.25 | Initialization 10: 0.91 0.09 | \n",
            "Sample 103 Initialization 1: 0.19 0.81 | Initialization 2: 0.41 0.59 | Initialization 3: 0.20 0.80 | Initialization 4: 0.25 0.75 | Initialization 5: 0.45 0.55 | Initialization 6: 0.32 0.68 | Initialization 7: 0.19 0.81 | Initialization 8: 0.48 0.52 | Initialization 9: 0.55 0.45 | Initialization 10: 0.25 0.75 | \n",
            "Sample 104 Initialization 1: 0.21 0.79 | Initialization 2: 0.54 0.46 | Initialization 3: 0.23 0.77 | Initialization 4: 0.29 0.71 | Initialization 5: 0.63 0.37 | Initialization 6: 0.32 0.68 | Initialization 7: 0.28 0.72 | Initialization 8: 0.62 0.38 | Initialization 9: 0.54 0.46 | Initialization 10: 0.31 0.69 | \n",
            "Sample 105 Initialization 1: 0.56 0.44 | Initialization 2: 0.91 0.09 | Initialization 3: 0.59 0.41 | Initialization 4: 0.71 0.29 | Initialization 5: 0.75 0.25 | Initialization 6: 0.50 0.50 | Initialization 7: 0.67 0.33 | Initialization 8: 0.81 0.19 | Initialization 9: 0.67 0.33 | Initialization 10: 0.90 0.10 | \n",
            "Sample 106 Initialization 1: 0.51 0.49 | Initialization 2: 0.87 0.13 | Initialization 3: 0.51 0.49 | Initialization 4: 0.65 0.35 | Initialization 5: 0.72 0.28 | Initialization 6: 0.52 0.48 | Initialization 7: 0.64 0.36 | Initialization 8: 0.80 0.20 | Initialization 9: 0.65 0.35 | Initialization 10: 0.82 0.18 | \n",
            "Sample 107 Initialization 1: 0.52 0.48 | Initialization 2: 0.81 0.19 | Initialization 3: 0.51 0.49 | Initialization 4: 0.48 0.52 | Initialization 5: 0.69 0.31 | Initialization 6: 0.52 0.48 | Initialization 7: 0.64 0.36 | Initialization 8: 0.74 0.26 | Initialization 9: 0.57 0.43 | Initialization 10: 0.80 0.20 | \n",
            "Sample 108 Initialization 1: 0.15 0.85 | Initialization 2: 0.41 0.59 | Initialization 3: 0.17 0.83 | Initialization 4: 0.19 0.81 | Initialization 5: 0.53 0.47 | Initialization 6: 0.35 0.65 | Initialization 7: 0.18 0.82 | Initialization 8: 0.60 0.40 | Initialization 9: 0.63 0.37 | Initialization 10: 0.23 0.77 | \n",
            "Sample 109 Initialization 1: 0.53 0.47 | Initialization 2: 0.83 0.17 | Initialization 3: 0.41 0.59 | Initialization 4: 0.59 0.41 | Initialization 5: 0.72 0.28 | Initialization 6: 0.38 0.62 | Initialization 7: 0.43 0.57 | Initialization 8: 0.76 0.24 | Initialization 9: 0.64 0.36 | Initialization 10: 0.59 0.41 | \n",
            "Sample 110 Initialization 1: 0.54 0.46 | Initialization 2: 0.91 0.09 | Initialization 3: 0.56 0.44 | Initialization 4: 0.74 0.26 | Initialization 5: 0.75 0.25 | Initialization 6: 0.44 0.56 | Initialization 7: 0.50 0.50 | Initialization 8: 0.85 0.15 | Initialization 9: 0.71 0.29 | Initialization 10: 0.77 0.23 | \n",
            "Sample 111 Initialization 1: 0.31 0.69 | Initialization 2: 0.74 0.26 | Initialization 3: 0.33 0.67 | Initialization 4: 0.33 0.67 | Initialization 5: 0.71 0.29 | Initialization 6: 0.31 0.69 | Initialization 7: 0.43 0.57 | Initialization 8: 0.65 0.35 | Initialization 9: 0.63 0.37 | Initialization 10: 0.33 0.67 | \n",
            "Sample 112 Initialization 1: 0.62 0.38 | Initialization 2: 0.91 0.09 | Initialization 3: 0.53 0.47 | Initialization 4: 0.69 0.31 | Initialization 5: 0.78 0.22 | Initialization 6: 0.46 0.54 | Initialization 7: 0.62 0.38 | Initialization 8: 0.81 0.19 | Initialization 9: 0.69 0.31 | Initialization 10: 0.82 0.18 | \n",
            "Sample 113 Initialization 1: 0.30 0.70 | Initialization 2: 0.80 0.20 | Initialization 3: 0.46 0.54 | Initialization 4: 0.42 0.58 | Initialization 5: 0.53 0.47 | Initialization 6: 0.46 0.54 | Initialization 7: 0.52 0.48 | Initialization 8: 0.82 0.18 | Initialization 9: 0.61 0.39 | Initialization 10: 0.81 0.19 | \n",
            "Sample 114 Initialization 1: 0.11 0.89 | Initialization 2: 0.35 0.65 | Initialization 3: 0.28 0.72 | Initialization 4: 0.21 0.79 | Initialization 5: 0.51 0.49 | Initialization 6: 0.31 0.69 | Initialization 7: 0.22 0.78 | Initialization 8: 0.72 0.28 | Initialization 9: 0.63 0.37 | Initialization 10: 0.21 0.79 | \n",
            "\n",
            "Learning rate = 0.01\n",
            "Sample 1 Initialization 1: 0.81 0.19 | Initialization 2: 0.77 0.23 | Initialization 3: 0.80 0.20 | Initialization 4: 0.77 0.23 | Initialization 5: 0.88 0.12 | Initialization 6: 0.78 0.22 | Initialization 7: 0.83 0.17 | Initialization 8: 0.80 0.20 | Initialization 9: 0.79 0.21 | Initialization 10: 0.80 0.20 | \n",
            "Sample 2 Initialization 1: 0.19 0.81 | Initialization 2: 0.10 0.90 | Initialization 3: 0.23 0.77 | Initialization 4: 0.23 0.77 | Initialization 5: 0.05 0.95 | Initialization 6: 0.09 0.91 | Initialization 7: 0.13 0.87 | Initialization 8: 0.06 0.94 | Initialization 9: 0.15 0.85 | Initialization 10: 0.14 0.86 | \n",
            "Sample 3 Initialization 1: 0.29 0.71 | Initialization 2: 0.15 0.85 | Initialization 3: 0.36 0.64 | Initialization 4: 0.25 0.75 | Initialization 5: 0.15 0.85 | Initialization 6: 0.17 0.83 | Initialization 7: 0.21 0.79 | Initialization 8: 0.15 0.85 | Initialization 9: 0.31 0.69 | Initialization 10: 0.22 0.78 | \n",
            "Sample 4 Initialization 1: 0.91 0.09 | Initialization 2: 0.90 0.10 | Initialization 3: 0.91 0.09 | Initialization 4: 0.87 0.13 | Initialization 5: 0.95 0.05 | Initialization 6: 0.93 0.07 | Initialization 7: 0.92 0.08 | Initialization 8: 0.91 0.09 | Initialization 9: 0.87 0.13 | Initialization 10: 0.88 0.12 | \n",
            "Sample 5 Initialization 1: 0.94 0.06 | Initialization 2: 0.93 0.07 | Initialization 3: 0.92 0.08 | Initialization 4: 0.89 0.11 | Initialization 5: 0.97 0.03 | Initialization 6: 0.96 0.04 | Initialization 7: 0.95 0.05 | Initialization 8: 0.93 0.07 | Initialization 9: 0.92 0.08 | Initialization 10: 0.93 0.07 | \n",
            "Sample 6 Initialization 1: 0.13 0.87 | Initialization 2: 0.09 0.91 | Initialization 3: 0.20 0.80 | Initialization 4: 0.17 0.83 | Initialization 5: 0.02 0.98 | Initialization 6: 0.06 0.94 | Initialization 7: 0.09 0.91 | Initialization 8: 0.04 0.96 | Initialization 9: 0.09 0.91 | Initialization 10: 0.07 0.93 | \n",
            "Sample 7 Initialization 1: 0.14 0.86 | Initialization 2: 0.08 0.92 | Initialization 3: 0.20 0.80 | Initialization 4: 0.19 0.81 | Initialization 5: 0.03 0.97 | Initialization 6: 0.07 0.93 | Initialization 7: 0.10 0.90 | Initialization 8: 0.04 0.96 | Initialization 9: 0.11 0.89 | Initialization 10: 0.09 0.91 | \n",
            "Sample 8 Initialization 1: 0.32 0.68 | Initialization 2: 0.26 0.74 | Initialization 3: 0.35 0.65 | Initialization 4: 0.35 0.65 | Initialization 5: 0.18 0.82 | Initialization 6: 0.32 0.68 | Initialization 7: 0.25 0.75 | Initialization 8: 0.21 0.79 | Initialization 9: 0.24 0.76 | Initialization 10: 0.26 0.74 | \n",
            "Sample 9 Initialization 1: 0.45 0.55 | Initialization 2: 0.55 0.45 | Initialization 3: 0.49 0.51 | Initialization 4: 0.48 0.52 | Initialization 5: 0.40 0.60 | Initialization 6: 0.61 0.39 | Initialization 7: 0.53 0.47 | Initialization 8: 0.66 0.34 | Initialization 9: 0.47 0.53 | Initialization 10: 0.50 0.50 | \n",
            "Sample 10 Initialization 1: 0.94 0.06 | Initialization 2: 0.91 0.09 | Initialization 3: 0.91 0.09 | Initialization 4: 0.89 0.11 | Initialization 5: 0.95 0.05 | Initialization 6: 0.95 0.05 | Initialization 7: 0.93 0.07 | Initialization 8: 0.95 0.05 | Initialization 9: 0.89 0.11 | Initialization 10: 0.92 0.08 | \n",
            "Sample 11 Initialization 1: 0.93 0.07 | Initialization 2: 0.86 0.14 | Initialization 3: 0.91 0.09 | Initialization 4: 0.83 0.17 | Initialization 5: 0.90 0.10 | Initialization 6: 0.86 0.14 | Initialization 7: 0.86 0.14 | Initialization 8: 0.88 0.12 | Initialization 9: 0.88 0.12 | Initialization 10: 0.85 0.15 | \n",
            "Sample 12 Initialization 1: 0.34 0.66 | Initialization 2: 0.24 0.76 | Initialization 3: 0.32 0.68 | Initialization 4: 0.32 0.68 | Initialization 5: 0.23 0.77 | Initialization 6: 0.21 0.79 | Initialization 7: 0.25 0.75 | Initialization 8: 0.16 0.84 | Initialization 9: 0.24 0.76 | Initialization 10: 0.26 0.74 | \n",
            "Sample 13 Initialization 1: 0.92 0.08 | Initialization 2: 0.89 0.11 | Initialization 3: 0.91 0.09 | Initialization 4: 0.87 0.13 | Initialization 5: 0.94 0.06 | Initialization 6: 0.94 0.06 | Initialization 7: 0.90 0.10 | Initialization 8: 0.92 0.08 | Initialization 9: 0.88 0.12 | Initialization 10: 0.88 0.12 | \n",
            "Sample 14 Initialization 1: 0.43 0.57 | Initialization 2: 0.23 0.77 | Initialization 3: 0.55 0.45 | Initialization 4: 0.36 0.64 | Initialization 5: 0.25 0.75 | Initialization 6: 0.36 0.64 | Initialization 7: 0.24 0.76 | Initialization 8: 0.16 0.84 | Initialization 9: 0.28 0.72 | Initialization 10: 0.25 0.75 | \n",
            "Sample 15 Initialization 1: 0.92 0.08 | Initialization 2: 0.92 0.08 | Initialization 3: 0.91 0.09 | Initialization 4: 0.88 0.12 | Initialization 5: 0.96 0.04 | Initialization 6: 0.96 0.04 | Initialization 7: 0.94 0.06 | Initialization 8: 0.95 0.05 | Initialization 9: 0.91 0.09 | Initialization 10: 0.94 0.06 | \n",
            "Sample 16 Initialization 1: 0.24 0.76 | Initialization 2: 0.12 0.88 | Initialization 3: 0.24 0.76 | Initialization 4: 0.23 0.77 | Initialization 5: 0.03 0.97 | Initialization 6: 0.13 0.87 | Initialization 7: 0.12 0.88 | Initialization 8: 0.06 0.94 | Initialization 9: 0.13 0.87 | Initialization 10: 0.09 0.91 | \n",
            "Sample 17 Initialization 1: 0.93 0.07 | Initialization 2: 0.91 0.09 | Initialization 3: 0.91 0.09 | Initialization 4: 0.87 0.13 | Initialization 5: 0.97 0.03 | Initialization 6: 0.94 0.06 | Initialization 7: 0.93 0.07 | Initialization 8: 0.93 0.07 | Initialization 9: 0.90 0.10 | Initialization 10: 0.90 0.10 | \n",
            "Sample 18 Initialization 1: 0.95 0.05 | Initialization 2: 0.95 0.05 | Initialization 3: 0.95 0.05 | Initialization 4: 0.91 0.09 | Initialization 5: 0.99 0.01 | Initialization 6: 0.97 0.03 | Initialization 7: 0.96 0.04 | Initialization 8: 0.96 0.04 | Initialization 9: 0.93 0.07 | Initialization 10: 0.95 0.05 | \n",
            "Sample 19 Initialization 1: 0.96 0.04 | Initialization 2: 0.95 0.05 | Initialization 3: 0.95 0.05 | Initialization 4: 0.92 0.08 | Initialization 5: 0.99 0.01 | Initialization 6: 0.97 0.03 | Initialization 7: 0.97 0.03 | Initialization 8: 0.97 0.03 | Initialization 9: 0.94 0.06 | Initialization 10: 0.96 0.04 | \n",
            "Sample 20 Initialization 1: 0.15 0.85 | Initialization 2: 0.09 0.91 | Initialization 3: 0.21 0.79 | Initialization 4: 0.19 0.81 | Initialization 5: 0.03 0.97 | Initialization 6: 0.07 0.93 | Initialization 7: 0.11 0.89 | Initialization 8: 0.05 0.95 | Initialization 9: 0.12 0.88 | Initialization 10: 0.09 0.91 | \n",
            "Sample 21 Initialization 1: 0.68 0.32 | Initialization 2: 0.70 0.30 | Initialization 3: 0.77 0.23 | Initialization 4: 0.68 0.32 | Initialization 5: 0.69 0.31 | Initialization 6: 0.78 0.22 | Initialization 7: 0.81 0.19 | Initialization 8: 0.81 0.19 | Initialization 9: 0.68 0.32 | Initialization 10: 0.73 0.27 | \n",
            "Sample 22 Initialization 1: 0.90 0.10 | Initialization 2: 0.87 0.13 | Initialization 3: 0.88 0.12 | Initialization 4: 0.85 0.15 | Initialization 5: 0.95 0.05 | Initialization 6: 0.93 0.07 | Initialization 7: 0.91 0.09 | Initialization 8: 0.92 0.08 | Initialization 9: 0.87 0.13 | Initialization 10: 0.89 0.11 | \n",
            "Sample 23 Initialization 1: 0.13 0.87 | Initialization 2: 0.08 0.92 | Initialization 3: 0.20 0.80 | Initialization 4: 0.18 0.82 | Initialization 5: 0.03 0.97 | Initialization 6: 0.07 0.93 | Initialization 7: 0.10 0.90 | Initialization 8: 0.05 0.95 | Initialization 9: 0.10 0.90 | Initialization 10: 0.07 0.93 | \n",
            "Sample 24 Initialization 1: 0.96 0.04 | Initialization 2: 0.95 0.05 | Initialization 3: 0.94 0.06 | Initialization 4: 0.91 0.09 | Initialization 5: 0.98 0.02 | Initialization 6: 0.97 0.03 | Initialization 7: 0.96 0.04 | Initialization 8: 0.96 0.04 | Initialization 9: 0.92 0.08 | Initialization 10: 0.95 0.05 | \n",
            "Sample 25 Initialization 1: 0.95 0.05 | Initialization 2: 0.93 0.07 | Initialization 3: 0.91 0.09 | Initialization 4: 0.90 0.10 | Initialization 5: 0.96 0.04 | Initialization 6: 0.95 0.05 | Initialization 7: 0.93 0.07 | Initialization 8: 0.94 0.06 | Initialization 9: 0.88 0.12 | Initialization 10: 0.90 0.10 | \n",
            "Sample 26 Initialization 1: 0.87 0.13 | Initialization 2: 0.91 0.09 | Initialization 3: 0.89 0.11 | Initialization 4: 0.87 0.13 | Initialization 5: 0.96 0.04 | Initialization 6: 0.94 0.06 | Initialization 7: 0.82 0.18 | Initialization 8: 0.87 0.13 | Initialization 9: 0.75 0.25 | Initialization 10: 0.82 0.18 | \n",
            "Sample 27 Initialization 1: 0.91 0.09 | Initialization 2: 0.90 0.10 | Initialization 3: 0.90 0.10 | Initialization 4: 0.87 0.13 | Initialization 5: 0.95 0.05 | Initialization 6: 0.95 0.05 | Initialization 7: 0.93 0.07 | Initialization 8: 0.94 0.06 | Initialization 9: 0.90 0.10 | Initialization 10: 0.92 0.08 | \n",
            "Sample 28 Initialization 1: 0.93 0.07 | Initialization 2: 0.91 0.09 | Initialization 3: 0.92 0.08 | Initialization 4: 0.86 0.14 | Initialization 5: 0.93 0.07 | Initialization 6: 0.89 0.11 | Initialization 7: 0.92 0.08 | Initialization 8: 0.91 0.09 | Initialization 9: 0.90 0.10 | Initialization 10: 0.88 0.12 | \n",
            "Sample 29 Initialization 1: 0.92 0.08 | Initialization 2: 0.91 0.09 | Initialization 3: 0.90 0.10 | Initialization 4: 0.87 0.13 | Initialization 5: 0.96 0.04 | Initialization 6: 0.93 0.07 | Initialization 7: 0.92 0.08 | Initialization 8: 0.93 0.07 | Initialization 9: 0.89 0.11 | Initialization 10: 0.90 0.10 | \n",
            "Sample 30 Initialization 1: 0.18 0.82 | Initialization 2: 0.10 0.90 | Initialization 3: 0.22 0.78 | Initialization 4: 0.22 0.78 | Initialization 5: 0.03 0.97 | Initialization 6: 0.08 0.92 | Initialization 7: 0.10 0.90 | Initialization 8: 0.06 0.94 | Initialization 9: 0.11 0.89 | Initialization 10: 0.09 0.91 | \n",
            "Sample 31 Initialization 1: 0.93 0.07 | Initialization 2: 0.93 0.07 | Initialization 3: 0.91 0.09 | Initialization 4: 0.90 0.10 | Initialization 5: 0.96 0.04 | Initialization 6: 0.94 0.06 | Initialization 7: 0.92 0.08 | Initialization 8: 0.94 0.06 | Initialization 9: 0.88 0.12 | Initialization 10: 0.90 0.10 | \n",
            "Sample 32 Initialization 1: 0.95 0.05 | Initialization 2: 0.93 0.07 | Initialization 3: 0.93 0.07 | Initialization 4: 0.90 0.10 | Initialization 5: 0.98 0.02 | Initialization 6: 0.97 0.03 | Initialization 7: 0.96 0.04 | Initialization 8: 0.96 0.04 | Initialization 9: 0.93 0.07 | Initialization 10: 0.95 0.05 | \n",
            "Sample 33 Initialization 1: 0.91 0.09 | Initialization 2: 0.94 0.06 | Initialization 3: 0.90 0.10 | Initialization 4: 0.90 0.10 | Initialization 5: 0.97 0.03 | Initialization 6: 0.93 0.07 | Initialization 7: 0.93 0.07 | Initialization 8: 0.94 0.06 | Initialization 9: 0.82 0.18 | Initialization 10: 0.91 0.09 | \n",
            "Sample 34 Initialization 1: 0.91 0.09 | Initialization 2: 0.90 0.10 | Initialization 3: 0.90 0.10 | Initialization 4: 0.87 0.13 | Initialization 5: 0.94 0.06 | Initialization 6: 0.94 0.06 | Initialization 7: 0.92 0.08 | Initialization 8: 0.92 0.08 | Initialization 9: 0.88 0.12 | Initialization 10: 0.91 0.09 | \n",
            "Sample 35 Initialization 1: 0.94 0.06 | Initialization 2: 0.94 0.06 | Initialization 3: 0.94 0.06 | Initialization 4: 0.90 0.10 | Initialization 5: 0.98 0.02 | Initialization 6: 0.96 0.04 | Initialization 7: 0.95 0.05 | Initialization 8: 0.95 0.05 | Initialization 9: 0.92 0.08 | Initialization 10: 0.94 0.06 | \n",
            "Sample 36 Initialization 1: 0.93 0.07 | Initialization 2: 0.92 0.08 | Initialization 3: 0.93 0.07 | Initialization 4: 0.90 0.10 | Initialization 5: 0.98 0.02 | Initialization 6: 0.95 0.05 | Initialization 7: 0.94 0.06 | Initialization 8: 0.93 0.07 | Initialization 9: 0.90 0.10 | Initialization 10: 0.92 0.08 | \n",
            "Sample 37 Initialization 1: 0.68 0.32 | Initialization 2: 0.45 0.55 | Initialization 3: 0.66 0.34 | Initialization 4: 0.53 0.47 | Initialization 5: 0.36 0.64 | Initialization 6: 0.49 0.51 | Initialization 7: 0.53 0.47 | Initialization 8: 0.48 0.52 | Initialization 9: 0.50 0.50 | Initialization 10: 0.63 0.37 | \n",
            "Sample 38 Initialization 1: 0.92 0.08 | Initialization 2: 0.92 0.08 | Initialization 3: 0.88 0.12 | Initialization 4: 0.89 0.11 | Initialization 5: 0.97 0.03 | Initialization 6: 0.92 0.08 | Initialization 7: 0.94 0.06 | Initialization 8: 0.93 0.07 | Initialization 9: 0.89 0.11 | Initialization 10: 0.91 0.09 | \n",
            "Sample 39 Initialization 1: 0.19 0.81 | Initialization 2: 0.11 0.89 | Initialization 3: 0.25 0.75 | Initialization 4: 0.22 0.78 | Initialization 5: 0.06 0.94 | Initialization 6: 0.11 0.89 | Initialization 7: 0.13 0.87 | Initialization 8: 0.08 0.92 | Initialization 9: 0.16 0.84 | Initialization 10: 0.13 0.87 | \n",
            "Sample 40 Initialization 1: 0.88 0.12 | Initialization 2: 0.85 0.15 | Initialization 3: 0.86 0.14 | Initialization 4: 0.83 0.17 | Initialization 5: 0.84 0.16 | Initialization 6: 0.81 0.19 | Initialization 7: 0.82 0.18 | Initialization 8: 0.81 0.19 | Initialization 9: 0.78 0.22 | Initialization 10: 0.80 0.20 | \n",
            "Sample 41 Initialization 1: 0.95 0.05 | Initialization 2: 0.94 0.06 | Initialization 3: 0.93 0.07 | Initialization 4: 0.90 0.10 | Initialization 5: 0.98 0.02 | Initialization 6: 0.96 0.04 | Initialization 7: 0.94 0.06 | Initialization 8: 0.96 0.04 | Initialization 9: 0.90 0.10 | Initialization 10: 0.93 0.07 | \n",
            "Sample 42 Initialization 1: 0.31 0.69 | Initialization 2: 0.18 0.82 | Initialization 3: 0.38 0.62 | Initialization 4: 0.26 0.74 | Initialization 5: 0.13 0.87 | Initialization 6: 0.12 0.88 | Initialization 7: 0.23 0.77 | Initialization 8: 0.09 0.91 | Initialization 9: 0.25 0.75 | Initialization 10: 0.18 0.82 | \n",
            "Sample 43 Initialization 1: 0.89 0.11 | Initialization 2: 0.89 0.11 | Initialization 3: 0.89 0.11 | Initialization 4: 0.85 0.15 | Initialization 5: 0.96 0.04 | Initialization 6: 0.91 0.09 | Initialization 7: 0.92 0.08 | Initialization 8: 0.92 0.08 | Initialization 9: 0.90 0.10 | Initialization 10: 0.90 0.10 | \n",
            "Sample 44 Initialization 1: 0.95 0.05 | Initialization 2: 0.93 0.07 | Initialization 3: 0.93 0.07 | Initialization 4: 0.89 0.11 | Initialization 5: 0.97 0.03 | Initialization 6: 0.95 0.05 | Initialization 7: 0.94 0.06 | Initialization 8: 0.95 0.05 | Initialization 9: 0.91 0.09 | Initialization 10: 0.93 0.07 | \n",
            "Sample 45 Initialization 1: 0.84 0.16 | Initialization 2: 0.84 0.16 | Initialization 3: 0.83 0.17 | Initialization 4: 0.85 0.15 | Initialization 5: 0.96 0.04 | Initialization 6: 0.88 0.12 | Initialization 7: 0.91 0.09 | Initialization 8: 0.89 0.11 | Initialization 9: 0.81 0.19 | Initialization 10: 0.82 0.18 | \n",
            "Sample 46 Initialization 1: 0.93 0.07 | Initialization 2: 0.88 0.12 | Initialization 3: 0.91 0.09 | Initialization 4: 0.82 0.18 | Initialization 5: 0.88 0.12 | Initialization 6: 0.89 0.11 | Initialization 7: 0.90 0.10 | Initialization 8: 0.89 0.11 | Initialization 9: 0.89 0.11 | Initialization 10: 0.89 0.11 | \n",
            "Sample 47 Initialization 1: 0.87 0.13 | Initialization 2: 0.93 0.07 | Initialization 3: 0.93 0.07 | Initialization 4: 0.90 0.10 | Initialization 5: 0.98 0.02 | Initialization 6: 0.95 0.05 | Initialization 7: 0.94 0.06 | Initialization 8: 0.93 0.07 | Initialization 9: 0.88 0.12 | Initialization 10: 0.90 0.10 | \n",
            "Sample 48 Initialization 1: 0.95 0.05 | Initialization 2: 0.94 0.06 | Initialization 3: 0.94 0.06 | Initialization 4: 0.91 0.09 | Initialization 5: 0.98 0.02 | Initialization 6: 0.96 0.04 | Initialization 7: 0.95 0.05 | Initialization 8: 0.95 0.05 | Initialization 9: 0.88 0.12 | Initialization 10: 0.93 0.07 | \n",
            "Sample 49 Initialization 1: 0.89 0.11 | Initialization 2: 0.87 0.13 | Initialization 3: 0.88 0.12 | Initialization 4: 0.83 0.17 | Initialization 5: 0.90 0.10 | Initialization 6: 0.89 0.11 | Initialization 7: 0.86 0.14 | Initialization 8: 0.91 0.09 | Initialization 9: 0.86 0.14 | Initialization 10: 0.87 0.13 | \n",
            "Sample 50 Initialization 1: 0.90 0.10 | Initialization 2: 0.87 0.13 | Initialization 3: 0.88 0.12 | Initialization 4: 0.86 0.14 | Initialization 5: 0.93 0.07 | Initialization 6: 0.93 0.07 | Initialization 7: 0.94 0.06 | Initialization 8: 0.88 0.12 | Initialization 9: 0.90 0.10 | Initialization 10: 0.88 0.12 | \n",
            "Sample 51 Initialization 1: 0.21 0.79 | Initialization 2: 0.14 0.86 | Initialization 3: 0.27 0.73 | Initialization 4: 0.23 0.77 | Initialization 5: 0.07 0.93 | Initialization 6: 0.12 0.88 | Initialization 7: 0.13 0.87 | Initialization 8: 0.12 0.88 | Initialization 9: 0.19 0.81 | Initialization 10: 0.14 0.86 | \n",
            "Sample 52 Initialization 1: 0.15 0.85 | Initialization 2: 0.09 0.91 | Initialization 3: 0.22 0.78 | Initialization 4: 0.19 0.81 | Initialization 5: 0.03 0.97 | Initialization 6: 0.07 0.93 | Initialization 7: 0.10 0.90 | Initialization 8: 0.05 0.95 | Initialization 9: 0.10 0.90 | Initialization 10: 0.07 0.93 | \n",
            "Sample 53 Initialization 1: 0.66 0.34 | Initialization 2: 0.77 0.23 | Initialization 3: 0.66 0.34 | Initialization 4: 0.72 0.28 | Initialization 5: 0.33 0.67 | Initialization 6: 0.76 0.24 | Initialization 7: 0.68 0.32 | Initialization 8: 0.77 0.23 | Initialization 9: 0.55 0.45 | Initialization 10: 0.67 0.33 | \n",
            "Sample 54 Initialization 1: 0.65 0.35 | Initialization 2: 0.74 0.26 | Initialization 3: 0.71 0.29 | Initialization 4: 0.80 0.20 | Initialization 5: 0.96 0.04 | Initialization 6: 0.79 0.21 | Initialization 7: 0.84 0.16 | Initialization 8: 0.78 0.22 | Initialization 9: 0.67 0.33 | Initialization 10: 0.57 0.43 | \n",
            "Sample 55 Initialization 1: 0.92 0.08 | Initialization 2: 0.93 0.07 | Initialization 3: 0.93 0.07 | Initialization 4: 0.90 0.10 | Initialization 5: 0.97 0.03 | Initialization 6: 0.94 0.06 | Initialization 7: 0.93 0.07 | Initialization 8: 0.93 0.07 | Initialization 9: 0.85 0.15 | Initialization 10: 0.92 0.08 | \n",
            "Sample 56 Initialization 1: 0.93 0.07 | Initialization 2: 0.89 0.11 | Initialization 3: 0.91 0.09 | Initialization 4: 0.84 0.16 | Initialization 5: 0.92 0.08 | Initialization 6: 0.92 0.08 | Initialization 7: 0.93 0.07 | Initialization 8: 0.91 0.09 | Initialization 9: 0.90 0.10 | Initialization 10: 0.91 0.09 | \n",
            "Sample 57 Initialization 1: 0.92 0.08 | Initialization 2: 0.93 0.07 | Initialization 3: 0.93 0.07 | Initialization 4: 0.89 0.11 | Initialization 5: 0.97 0.03 | Initialization 6: 0.95 0.05 | Initialization 7: 0.95 0.05 | Initialization 8: 0.94 0.06 | Initialization 9: 0.90 0.10 | Initialization 10: 0.93 0.07 | \n",
            "Sample 58 Initialization 1: 0.15 0.85 | Initialization 2: 0.08 0.92 | Initialization 3: 0.20 0.80 | Initialization 4: 0.16 0.84 | Initialization 5: 0.02 0.98 | Initialization 6: 0.06 0.94 | Initialization 7: 0.10 0.90 | Initialization 8: 0.04 0.96 | Initialization 9: 0.10 0.90 | Initialization 10: 0.07 0.93 | \n",
            "Sample 59 Initialization 1: 0.39 0.61 | Initialization 2: 0.36 0.64 | Initialization 3: 0.51 0.49 | Initialization 4: 0.31 0.69 | Initialization 5: 0.25 0.75 | Initialization 6: 0.30 0.70 | Initialization 7: 0.44 0.56 | Initialization 8: 0.44 0.56 | Initialization 9: 0.49 0.51 | Initialization 10: 0.45 0.55 | \n",
            "Sample 60 Initialization 1: 0.94 0.06 | Initialization 2: 0.93 0.07 | Initialization 3: 0.93 0.07 | Initialization 4: 0.89 0.11 | Initialization 5: 0.98 0.02 | Initialization 6: 0.96 0.04 | Initialization 7: 0.96 0.04 | Initialization 8: 0.96 0.04 | Initialization 9: 0.93 0.07 | Initialization 10: 0.95 0.05 | \n",
            "Sample 61 Initialization 1: 0.90 0.10 | Initialization 2: 0.89 0.11 | Initialization 3: 0.89 0.11 | Initialization 4: 0.86 0.14 | Initialization 5: 0.95 0.05 | Initialization 6: 0.93 0.07 | Initialization 7: 0.91 0.09 | Initialization 8: 0.92 0.08 | Initialization 9: 0.88 0.12 | Initialization 10: 0.90 0.10 | \n",
            "Sample 62 Initialization 1: 0.14 0.86 | Initialization 2: 0.09 0.91 | Initialization 3: 0.20 0.80 | Initialization 4: 0.20 0.80 | Initialization 5: 0.03 0.97 | Initialization 6: 0.07 0.93 | Initialization 7: 0.10 0.90 | Initialization 8: 0.05 0.95 | Initialization 9: 0.12 0.88 | Initialization 10: 0.08 0.92 | \n",
            "Sample 63 Initialization 1: 0.14 0.86 | Initialization 2: 0.09 0.91 | Initialization 3: 0.21 0.79 | Initialization 4: 0.19 0.81 | Initialization 5: 0.03 0.97 | Initialization 6: 0.07 0.93 | Initialization 7: 0.11 0.89 | Initialization 8: 0.05 0.95 | Initialization 9: 0.14 0.86 | Initialization 10: 0.10 0.90 | \n",
            "Sample 64 Initialization 1: 0.93 0.07 | Initialization 2: 0.88 0.12 | Initialization 3: 0.90 0.10 | Initialization 4: 0.84 0.16 | Initialization 5: 0.83 0.17 | Initialization 6: 0.83 0.17 | Initialization 7: 0.86 0.14 | Initialization 8: 0.87 0.13 | Initialization 9: 0.86 0.14 | Initialization 10: 0.82 0.18 | \n",
            "Sample 65 Initialization 1: 0.95 0.05 | Initialization 2: 0.93 0.07 | Initialization 3: 0.94 0.06 | Initialization 4: 0.90 0.10 | Initialization 5: 0.98 0.02 | Initialization 6: 0.96 0.04 | Initialization 7: 0.95 0.05 | Initialization 8: 0.95 0.05 | Initialization 9: 0.91 0.09 | Initialization 10: 0.93 0.07 | \n",
            "Sample 66 Initialization 1: 0.84 0.16 | Initialization 2: 0.83 0.17 | Initialization 3: 0.85 0.15 | Initialization 4: 0.78 0.22 | Initialization 5: 0.75 0.25 | Initialization 6: 0.82 0.18 | Initialization 7: 0.86 0.14 | Initialization 8: 0.87 0.13 | Initialization 9: 0.84 0.16 | Initialization 10: 0.83 0.17 | \n",
            "Sample 67 Initialization 1: 0.17 0.83 | Initialization 2: 0.10 0.90 | Initialization 3: 0.23 0.77 | Initialization 4: 0.20 0.80 | Initialization 5: 0.04 0.96 | Initialization 6: 0.08 0.92 | Initialization 7: 0.12 0.88 | Initialization 8: 0.06 0.94 | Initialization 9: 0.14 0.86 | Initialization 10: 0.12 0.88 | \n",
            "Sample 68 Initialization 1: 0.13 0.87 | Initialization 2: 0.08 0.92 | Initialization 3: 0.22 0.78 | Initialization 4: 0.17 0.83 | Initialization 5: 0.02 0.98 | Initialization 6: 0.06 0.94 | Initialization 7: 0.10 0.90 | Initialization 8: 0.04 0.96 | Initialization 9: 0.12 0.88 | Initialization 10: 0.07 0.93 | \n",
            "Sample 69 Initialization 1: 0.92 0.08 | Initialization 2: 0.91 0.09 | Initialization 3: 0.93 0.07 | Initialization 4: 0.89 0.11 | Initialization 5: 0.98 0.02 | Initialization 6: 0.96 0.04 | Initialization 7: 0.94 0.06 | Initialization 8: 0.95 0.05 | Initialization 9: 0.91 0.09 | Initialization 10: 0.93 0.07 | \n",
            "Sample 70 Initialization 1: 0.81 0.19 | Initialization 2: 0.86 0.14 | Initialization 3: 0.83 0.17 | Initialization 4: 0.85 0.15 | Initialization 5: 0.85 0.15 | Initialization 6: 0.87 0.13 | Initialization 7: 0.86 0.14 | Initialization 8: 0.86 0.14 | Initialization 9: 0.79 0.21 | Initialization 10: 0.81 0.19 | \n",
            "Sample 71 Initialization 1: 0.32 0.68 | Initialization 2: 0.22 0.78 | Initialization 3: 0.37 0.63 | Initialization 4: 0.28 0.72 | Initialization 5: 0.30 0.70 | Initialization 6: 0.17 0.83 | Initialization 7: 0.28 0.72 | Initialization 8: 0.13 0.87 | Initialization 9: 0.27 0.73 | Initialization 10: 0.20 0.80 | \n",
            "Sample 72 Initialization 1: 0.23 0.77 | Initialization 2: 0.18 0.82 | Initialization 3: 0.27 0.73 | Initialization 4: 0.28 0.72 | Initialization 5: 0.08 0.92 | Initialization 6: 0.18 0.82 | Initialization 7: 0.18 0.82 | Initialization 8: 0.17 0.83 | Initialization 9: 0.16 0.84 | Initialization 10: 0.18 0.82 | \n",
            "Sample 73 Initialization 1: 0.92 0.08 | Initialization 2: 0.90 0.10 | Initialization 3: 0.90 0.10 | Initialization 4: 0.86 0.14 | Initialization 5: 0.95 0.05 | Initialization 6: 0.95 0.05 | Initialization 7: 0.93 0.07 | Initialization 8: 0.94 0.06 | Initialization 9: 0.88 0.12 | Initialization 10: 0.90 0.10 | \n",
            "Sample 74 Initialization 1: 0.24 0.76 | Initialization 2: 0.15 0.85 | Initialization 3: 0.27 0.73 | Initialization 4: 0.22 0.78 | Initialization 5: 0.07 0.93 | Initialization 6: 0.14 0.86 | Initialization 7: 0.17 0.83 | Initialization 8: 0.13 0.87 | Initialization 9: 0.21 0.79 | Initialization 10: 0.15 0.85 | \n",
            "Sample 75 Initialization 1: 0.96 0.04 | Initialization 2: 0.95 0.05 | Initialization 3: 0.95 0.05 | Initialization 4: 0.92 0.08 | Initialization 5: 0.98 0.02 | Initialization 6: 0.97 0.03 | Initialization 7: 0.96 0.04 | Initialization 8: 0.96 0.04 | Initialization 9: 0.90 0.10 | Initialization 10: 0.95 0.05 | \n",
            "Sample 76 Initialization 1: 0.81 0.19 | Initialization 2: 0.86 0.14 | Initialization 3: 0.84 0.16 | Initialization 4: 0.83 0.17 | Initialization 5: 0.87 0.13 | Initialization 6: 0.92 0.08 | Initialization 7: 0.88 0.12 | Initialization 8: 0.90 0.10 | Initialization 9: 0.80 0.20 | Initialization 10: 0.85 0.15 | \n",
            "Sample 77 Initialization 1: 0.91 0.09 | Initialization 2: 0.88 0.12 | Initialization 3: 0.90 0.10 | Initialization 4: 0.84 0.16 | Initialization 5: 0.93 0.07 | Initialization 6: 0.90 0.10 | Initialization 7: 0.90 0.10 | Initialization 8: 0.89 0.11 | Initialization 9: 0.86 0.14 | Initialization 10: 0.86 0.14 | \n",
            "Sample 78 Initialization 1: 0.54 0.46 | Initialization 2: 0.52 0.48 | Initialization 3: 0.58 0.42 | Initialization 4: 0.54 0.46 | Initialization 5: 0.63 0.37 | Initialization 6: 0.52 0.48 | Initialization 7: 0.57 0.43 | Initialization 8: 0.50 0.50 | Initialization 9: 0.58 0.42 | Initialization 10: 0.50 0.50 | \n",
            "Sample 79 Initialization 1: 0.96 0.04 | Initialization 2: 0.94 0.06 | Initialization 3: 0.94 0.06 | Initialization 4: 0.91 0.09 | Initialization 5: 0.98 0.02 | Initialization 6: 0.97 0.03 | Initialization 7: 0.96 0.04 | Initialization 8: 0.97 0.03 | Initialization 9: 0.93 0.07 | Initialization 10: 0.96 0.04 | \n",
            "Sample 80 Initialization 1: 0.79 0.21 | Initialization 2: 0.85 0.15 | Initialization 3: 0.85 0.15 | Initialization 4: 0.82 0.18 | Initialization 5: 0.93 0.07 | Initialization 6: 0.92 0.08 | Initialization 7: 0.87 0.13 | Initialization 8: 0.90 0.10 | Initialization 9: 0.83 0.17 | Initialization 10: 0.86 0.14 | \n",
            "Sample 81 Initialization 1: 0.30 0.70 | Initialization 2: 0.18 0.82 | Initialization 3: 0.30 0.70 | Initialization 4: 0.28 0.72 | Initialization 5: 0.14 0.86 | Initialization 6: 0.19 0.81 | Initialization 7: 0.17 0.83 | Initialization 8: 0.16 0.84 | Initialization 9: 0.23 0.77 | Initialization 10: 0.19 0.81 | \n",
            "Sample 82 Initialization 1: 0.96 0.04 | Initialization 2: 0.94 0.06 | Initialization 3: 0.94 0.06 | Initialization 4: 0.91 0.09 | Initialization 5: 0.98 0.02 | Initialization 6: 0.97 0.03 | Initialization 7: 0.96 0.04 | Initialization 8: 0.97 0.03 | Initialization 9: 0.92 0.08 | Initialization 10: 0.95 0.05 | \n",
            "Sample 83 Initialization 1: 0.55 0.45 | Initialization 2: 0.35 0.65 | Initialization 3: 0.45 0.55 | Initialization 4: 0.43 0.57 | Initialization 5: 0.64 0.36 | Initialization 6: 0.38 0.62 | Initialization 7: 0.46 0.54 | Initialization 8: 0.35 0.65 | Initialization 9: 0.50 0.50 | Initialization 10: 0.35 0.65 | \n",
            "Sample 84 Initialization 1: 0.14 0.86 | Initialization 2: 0.08 0.92 | Initialization 3: 0.21 0.79 | Initialization 4: 0.19 0.81 | Initialization 5: 0.03 0.97 | Initialization 6: 0.06 0.94 | Initialization 7: 0.11 0.89 | Initialization 8: 0.05 0.95 | Initialization 9: 0.12 0.88 | Initialization 10: 0.09 0.91 | \n",
            "Sample 85 Initialization 1: 0.44 0.56 | Initialization 2: 0.20 0.80 | Initialization 3: 0.47 0.53 | Initialization 4: 0.30 0.70 | Initialization 5: 0.15 0.85 | Initialization 6: 0.23 0.77 | Initialization 7: 0.23 0.77 | Initialization 8: 0.22 0.78 | Initialization 9: 0.27 0.73 | Initialization 10: 0.30 0.70 | \n",
            "Sample 86 Initialization 1: 0.20 0.80 | Initialization 2: 0.12 0.88 | Initialization 3: 0.23 0.77 | Initialization 4: 0.23 0.77 | Initialization 5: 0.07 0.93 | Initialization 6: 0.10 0.90 | Initialization 7: 0.16 0.84 | Initialization 8: 0.07 0.93 | Initialization 9: 0.16 0.84 | Initialization 10: 0.17 0.83 | \n",
            "Sample 87 Initialization 1: 0.29 0.71 | Initialization 2: 0.12 0.88 | Initialization 3: 0.25 0.75 | Initialization 4: 0.23 0.77 | Initialization 5: 0.04 0.96 | Initialization 6: 0.11 0.89 | Initialization 7: 0.16 0.84 | Initialization 8: 0.23 0.77 | Initialization 9: 0.12 0.88 | Initialization 10: 0.28 0.72 | \n",
            "Sample 88 Initialization 1: 0.25 0.75 | Initialization 2: 0.12 0.88 | Initialization 3: 0.21 0.79 | Initialization 4: 0.23 0.77 | Initialization 5: 0.04 0.96 | Initialization 6: 0.08 0.92 | Initialization 7: 0.15 0.85 | Initialization 8: 0.06 0.94 | Initialization 9: 0.13 0.87 | Initialization 10: 0.10 0.90 | \n",
            "Sample 89 Initialization 1: 0.92 0.08 | Initialization 2: 0.91 0.09 | Initialization 3: 0.92 0.08 | Initialization 4: 0.88 0.12 | Initialization 5: 0.86 0.14 | Initialization 6: 0.93 0.07 | Initialization 7: 0.94 0.06 | Initialization 8: 0.91 0.09 | Initialization 9: 0.87 0.13 | Initialization 10: 0.90 0.10 | \n",
            "Sample 90 Initialization 1: 0.86 0.14 | Initialization 2: 0.88 0.12 | Initialization 3: 0.90 0.10 | Initialization 4: 0.84 0.16 | Initialization 5: 0.95 0.05 | Initialization 6: 0.95 0.05 | Initialization 7: 0.91 0.09 | Initialization 8: 0.93 0.07 | Initialization 9: 0.89 0.11 | Initialization 10: 0.90 0.10 | \n",
            "Sample 91 Initialization 1: 0.90 0.10 | Initialization 2: 0.89 0.11 | Initialization 3: 0.89 0.11 | Initialization 4: 0.86 0.14 | Initialization 5: 0.96 0.04 | Initialization 6: 0.93 0.07 | Initialization 7: 0.90 0.10 | Initialization 8: 0.94 0.06 | Initialization 9: 0.87 0.13 | Initialization 10: 0.90 0.10 | \n",
            "Sample 92 Initialization 1: 0.67 0.33 | Initialization 2: 0.71 0.29 | Initialization 3: 0.71 0.29 | Initialization 4: 0.67 0.33 | Initialization 5: 0.62 0.38 | Initialization 6: 0.75 0.25 | Initialization 7: 0.71 0.29 | Initialization 8: 0.76 0.24 | Initialization 9: 0.73 0.27 | Initialization 10: 0.72 0.28 | \n",
            "Sample 93 Initialization 1: 0.77 0.23 | Initialization 2: 0.76 0.24 | Initialization 3: 0.79 0.21 | Initialization 4: 0.78 0.22 | Initialization 5: 0.73 0.27 | Initialization 6: 0.81 0.19 | Initialization 7: 0.83 0.17 | Initialization 8: 0.80 0.20 | Initialization 9: 0.72 0.28 | Initialization 10: 0.79 0.21 | \n",
            "Sample 94 Initialization 1: 0.94 0.06 | Initialization 2: 0.93 0.07 | Initialization 3: 0.94 0.06 | Initialization 4: 0.89 0.11 | Initialization 5: 0.95 0.05 | Initialization 6: 0.97 0.03 | Initialization 7: 0.95 0.05 | Initialization 8: 0.95 0.05 | Initialization 9: 0.90 0.10 | Initialization 10: 0.87 0.13 | \n",
            "Sample 95 Initialization 1: 0.91 0.09 | Initialization 2: 0.93 0.07 | Initialization 3: 0.94 0.06 | Initialization 4: 0.90 0.10 | Initialization 5: 0.98 0.02 | Initialization 6: 0.96 0.04 | Initialization 7: 0.95 0.05 | Initialization 8: 0.95 0.05 | Initialization 9: 0.89 0.11 | Initialization 10: 0.93 0.07 | \n",
            "Sample 96 Initialization 1: 0.95 0.05 | Initialization 2: 0.94 0.06 | Initialization 3: 0.94 0.06 | Initialization 4: 0.91 0.09 | Initialization 5: 0.98 0.02 | Initialization 6: 0.96 0.04 | Initialization 7: 0.96 0.04 | Initialization 8: 0.95 0.05 | Initialization 9: 0.92 0.08 | Initialization 10: 0.95 0.05 | \n",
            "Sample 97 Initialization 1: 0.19 0.81 | Initialization 2: 0.10 0.90 | Initialization 3: 0.23 0.77 | Initialization 4: 0.21 0.79 | Initialization 5: 0.03 0.97 | Initialization 6: 0.08 0.92 | Initialization 7: 0.12 0.88 | Initialization 8: 0.05 0.95 | Initialization 9: 0.12 0.88 | Initialization 10: 0.10 0.90 | \n",
            "Sample 98 Initialization 1: 0.17 0.83 | Initialization 2: 0.12 0.88 | Initialization 3: 0.24 0.76 | Initialization 4: 0.22 0.78 | Initialization 5: 0.04 0.96 | Initialization 6: 0.09 0.91 | Initialization 7: 0.12 0.88 | Initialization 8: 0.06 0.94 | Initialization 9: 0.12 0.88 | Initialization 10: 0.13 0.87 | \n",
            "Sample 99 Initialization 1: 0.95 0.05 | Initialization 2: 0.94 0.06 | Initialization 3: 0.94 0.06 | Initialization 4: 0.90 0.10 | Initialization 5: 0.98 0.02 | Initialization 6: 0.97 0.03 | Initialization 7: 0.96 0.04 | Initialization 8: 0.96 0.04 | Initialization 9: 0.93 0.07 | Initialization 10: 0.95 0.05 | \n",
            "Sample 100 Initialization 1: 0.27 0.73 | Initialization 2: 0.14 0.86 | Initialization 3: 0.30 0.70 | Initialization 4: 0.22 0.78 | Initialization 5: 0.08 0.92 | Initialization 6: 0.12 0.88 | Initialization 7: 0.16 0.84 | Initialization 8: 0.10 0.90 | Initialization 9: 0.19 0.81 | Initialization 10: 0.13 0.87 | \n",
            "Sample 101 Initialization 1: 0.32 0.68 | Initialization 2: 0.22 0.78 | Initialization 3: 0.41 0.59 | Initialization 4: 0.31 0.69 | Initialization 5: 0.16 0.84 | Initialization 6: 0.29 0.71 | Initialization 7: 0.23 0.77 | Initialization 8: 0.29 0.71 | Initialization 9: 0.28 0.72 | Initialization 10: 0.31 0.69 | \n",
            "Sample 102 Initialization 1: 0.96 0.04 | Initialization 2: 0.95 0.05 | Initialization 3: 0.95 0.05 | Initialization 4: 0.92 0.08 | Initialization 5: 0.99 0.01 | Initialization 6: 0.97 0.03 | Initialization 7: 0.96 0.04 | Initialization 8: 0.97 0.03 | Initialization 9: 0.94 0.06 | Initialization 10: 0.96 0.04 | \n",
            "Sample 103 Initialization 1: 0.15 0.85 | Initialization 2: 0.11 0.89 | Initialization 3: 0.20 0.80 | Initialization 4: 0.23 0.77 | Initialization 5: 0.03 0.97 | Initialization 6: 0.08 0.92 | Initialization 7: 0.13 0.87 | Initialization 8: 0.06 0.94 | Initialization 9: 0.12 0.88 | Initialization 10: 0.12 0.88 | \n",
            "Sample 104 Initialization 1: 0.20 0.80 | Initialization 2: 0.15 0.85 | Initialization 3: 0.24 0.76 | Initialization 4: 0.23 0.77 | Initialization 5: 0.06 0.94 | Initialization 6: 0.11 0.89 | Initialization 7: 0.13 0.87 | Initialization 8: 0.09 0.91 | Initialization 9: 0.17 0.83 | Initialization 10: 0.12 0.88 | \n",
            "Sample 105 Initialization 1: 0.94 0.06 | Initialization 2: 0.92 0.08 | Initialization 3: 0.90 0.10 | Initialization 4: 0.89 0.11 | Initialization 5: 0.96 0.04 | Initialization 6: 0.92 0.08 | Initialization 7: 0.92 0.08 | Initialization 8: 0.92 0.08 | Initialization 9: 0.85 0.15 | Initialization 10: 0.91 0.09 | \n",
            "Sample 106 Initialization 1: 0.86 0.14 | Initialization 2: 0.85 0.15 | Initialization 3: 0.86 0.14 | Initialization 4: 0.83 0.17 | Initialization 5: 0.94 0.06 | Initialization 6: 0.91 0.09 | Initialization 7: 0.91 0.09 | Initialization 8: 0.90 0.10 | Initialization 9: 0.85 0.15 | Initialization 10: 0.90 0.10 | \n",
            "Sample 107 Initialization 1: 0.91 0.09 | Initialization 2: 0.90 0.10 | Initialization 3: 0.92 0.08 | Initialization 4: 0.85 0.15 | Initialization 5: 0.93 0.07 | Initialization 6: 0.92 0.08 | Initialization 7: 0.91 0.09 | Initialization 8: 0.90 0.10 | Initialization 9: 0.87 0.13 | Initialization 10: 0.87 0.13 | \n",
            "Sample 108 Initialization 1: 0.13 0.87 | Initialization 2: 0.07 0.93 | Initialization 3: 0.20 0.80 | Initialization 4: 0.20 0.80 | Initialization 5: 0.03 0.97 | Initialization 6: 0.06 0.94 | Initialization 7: 0.11 0.89 | Initialization 8: 0.05 0.95 | Initialization 9: 0.11 0.89 | Initialization 10: 0.11 0.89 | \n",
            "Sample 109 Initialization 1: 0.64 0.36 | Initialization 2: 0.71 0.29 | Initialization 3: 0.72 0.28 | Initialization 4: 0.67 0.33 | Initialization 5: 0.79 0.21 | Initialization 6: 0.75 0.25 | Initialization 7: 0.77 0.23 | Initialization 8: 0.75 0.25 | Initialization 9: 0.74 0.26 | Initialization 10: 0.68 0.32 | \n",
            "Sample 110 Initialization 1: 0.89 0.11 | Initialization 2: 0.81 0.19 | Initialization 3: 0.88 0.12 | Initialization 4: 0.80 0.20 | Initialization 5: 0.88 0.12 | Initialization 6: 0.92 0.08 | Initialization 7: 0.87 0.13 | Initialization 8: 0.90 0.10 | Initialization 9: 0.87 0.13 | Initialization 10: 0.86 0.14 | \n",
            "Sample 111 Initialization 1: 0.34 0.66 | Initialization 2: 0.22 0.78 | Initialization 3: 0.32 0.68 | Initialization 4: 0.28 0.72 | Initialization 5: 0.20 0.80 | Initialization 6: 0.15 0.85 | Initialization 7: 0.27 0.73 | Initialization 8: 0.11 0.89 | Initialization 9: 0.28 0.72 | Initialization 10: 0.21 0.79 | \n",
            "Sample 112 Initialization 1: 0.92 0.08 | Initialization 2: 0.91 0.09 | Initialization 3: 0.91 0.09 | Initialization 4: 0.87 0.13 | Initialization 5: 0.97 0.03 | Initialization 6: 0.95 0.05 | Initialization 7: 0.94 0.06 | Initialization 8: 0.94 0.06 | Initialization 9: 0.91 0.09 | Initialization 10: 0.93 0.07 | \n",
            "Sample 113 Initialization 1: 0.77 0.23 | Initialization 2: 0.51 0.49 | Initialization 3: 0.78 0.22 | Initialization 4: 0.54 0.46 | Initialization 5: 0.48 0.52 | Initialization 6: 0.66 0.34 | Initialization 7: 0.69 0.31 | Initialization 8: 0.64 0.36 | Initialization 9: 0.76 0.24 | Initialization 10: 0.72 0.28 | \n",
            "Sample 114 Initialization 1: 0.16 0.84 | Initialization 2: 0.08 0.92 | Initialization 3: 0.21 0.79 | Initialization 4: 0.22 0.78 | Initialization 5: 0.03 0.97 | Initialization 6: 0.07 0.93 | Initialization 7: 0.10 0.90 | Initialization 8: 0.04 0.96 | Initialization 9: 0.10 0.90 | Initialization 10: 0.09 0.91 | \n",
            "\n",
            "Learning rate = 0.05\n",
            "Sample 1 Initialization 1: 0.92 0.08 | Initialization 2: 0.93 0.07 | Initialization 3: 0.93 0.07 | Initialization 4: 0.94 0.06 | Initialization 5: 0.91 0.09 | Initialization 6: 0.92 0.08 | Initialization 7: 0.93 0.07 | Initialization 8: 0.92 0.08 | Initialization 9: 0.93 0.07 | Initialization 10: 0.93 0.07 | \n",
            "Sample 2 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 3 Initialization 1: 0.02 0.98 | Initialization 2: 0.03 0.97 | Initialization 3: 0.02 0.98 | Initialization 4: 0.02 0.98 | Initialization 5: 0.02 0.98 | Initialization 6: 0.03 0.97 | Initialization 7: 0.03 0.97 | Initialization 8: 0.02 0.98 | Initialization 9: 0.03 0.97 | Initialization 10: 0.02 0.98 | \n",
            "Sample 4 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 5 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 6 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 7 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 8 Initialization 1: 0.05 0.95 | Initialization 2: 0.06 0.94 | Initialization 3: 0.06 0.94 | Initialization 4: 0.06 0.94 | Initialization 5: 0.07 0.93 | Initialization 6: 0.07 0.93 | Initialization 7: 0.06 0.94 | Initialization 8: 0.05 0.95 | Initialization 9: 0.09 0.91 | Initialization 10: 0.07 0.93 | \n",
            "Sample 9 Initialization 1: 0.62 0.38 | Initialization 2: 0.71 0.29 | Initialization 3: 0.54 0.46 | Initialization 4: 0.59 0.41 | Initialization 5: 0.67 0.33 | Initialization 6: 0.66 0.34 | Initialization 7: 0.64 0.36 | Initialization 8: 0.72 0.28 | Initialization 9: 0.56 0.44 | Initialization 10: 0.62 0.38 | \n",
            "Sample 10 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 11 Initialization 1: 0.96 0.04 | Initialization 2: 0.95 0.05 | Initialization 3: 0.96 0.04 | Initialization 4: 0.95 0.05 | Initialization 5: 0.94 0.06 | Initialization 6: 0.95 0.05 | Initialization 7: 0.94 0.06 | Initialization 8: 0.93 0.07 | Initialization 9: 0.95 0.05 | Initialization 10: 0.95 0.05 | \n",
            "Sample 12 Initialization 1: 0.04 0.96 | Initialization 2: 0.04 0.96 | Initialization 3: 0.03 0.97 | Initialization 4: 0.04 0.96 | Initialization 5: 0.05 0.95 | Initialization 6: 0.05 0.95 | Initialization 7: 0.04 0.96 | Initialization 8: 0.04 0.96 | Initialization 9: 0.06 0.94 | Initialization 10: 0.04 0.96 | \n",
            "Sample 13 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.97 0.03 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 14 Initialization 1: 0.09 0.91 | Initialization 2: 0.16 0.84 | Initialization 3: 0.23 0.77 | Initialization 4: 0.06 0.94 | Initialization 5: 0.18 0.82 | Initialization 6: 0.16 0.84 | Initialization 7: 0.21 0.79 | Initialization 8: 0.05 0.95 | Initialization 9: 0.20 0.80 | Initialization 10: 0.08 0.92 | \n",
            "Sample 15 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 16 Initialization 1: 0.01 0.99 | Initialization 2: 0.02 0.98 | Initialization 3: 0.02 0.98 | Initialization 4: 0.01 0.99 | Initialization 5: 0.02 0.98 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 17 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 18 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 19 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 20 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 21 Initialization 1: 0.89 0.11 | Initialization 2: 0.88 0.12 | Initialization 3: 0.91 0.09 | Initialization 4: 0.89 0.11 | Initialization 5: 0.89 0.11 | Initialization 6: 0.86 0.14 | Initialization 7: 0.88 0.12 | Initialization 8: 0.89 0.11 | Initialization 9: 0.91 0.09 | Initialization 10: 0.91 0.09 | \n",
            "Sample 22 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 23 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 24 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 25 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 26 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.98 0.02 | Initialization 7: 0.99 0.01 | Initialization 8: 0.98 0.02 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 27 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 28 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.98 0.02 | Initialization 5: 0.99 0.01 | Initialization 6: 0.98 0.02 | Initialization 7: 0.97 0.03 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 29 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 30 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 31 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 32 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 33 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 34 Initialization 1: 0.98 0.02 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 35 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 36 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 37 Initialization 1: 0.12 0.88 | Initialization 2: 0.10 0.90 | Initialization 3: 0.21 0.79 | Initialization 4: 0.13 0.87 | Initialization 5: 0.14 0.86 | Initialization 6: 0.14 0.86 | Initialization 7: 0.10 0.90 | Initialization 8: 0.10 0.90 | Initialization 9: 0.19 0.81 | Initialization 10: 0.15 0.85 | \n",
            "Sample 38 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 39 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 40 Initialization 1: 0.94 0.06 | Initialization 2: 0.92 0.08 | Initialization 3: 0.94 0.06 | Initialization 4: 0.91 0.09 | Initialization 5: 0.90 0.10 | Initialization 6: 0.93 0.07 | Initialization 7: 0.92 0.08 | Initialization 8: 0.91 0.09 | Initialization 9: 0.90 0.10 | Initialization 10: 0.92 0.08 | \n",
            "Sample 41 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 42 Initialization 1: 0.02 0.98 | Initialization 2: 0.02 0.98 | Initialization 3: 0.02 0.98 | Initialization 4: 0.01 0.99 | Initialization 5: 0.02 0.98 | Initialization 6: 0.02 0.98 | Initialization 7: 0.02 0.98 | Initialization 8: 0.02 0.98 | Initialization 9: 0.03 0.97 | Initialization 10: 0.02 0.98 | \n",
            "Sample 43 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 44 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 45 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.95 0.05 | Initialization 4: 0.97 0.03 | Initialization 5: 0.99 0.01 | Initialization 6: 0.97 0.03 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.97 0.03 | Initialization 10: 0.98 0.02 | \n",
            "Sample 46 Initialization 1: 0.93 0.07 | Initialization 2: 0.97 0.03 | Initialization 3: 0.97 0.03 | Initialization 4: 0.96 0.04 | Initialization 5: 0.97 0.03 | Initialization 6: 0.97 0.03 | Initialization 7: 0.97 0.03 | Initialization 8: 0.95 0.05 | Initialization 9: 0.98 0.02 | Initialization 10: 0.94 0.06 | \n",
            "Sample 47 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 48 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 49 Initialization 1: 0.96 0.04 | Initialization 2: 0.96 0.04 | Initialization 3: 0.96 0.04 | Initialization 4: 0.97 0.03 | Initialization 5: 0.95 0.05 | Initialization 6: 0.97 0.03 | Initialization 7: 0.97 0.03 | Initialization 8: 0.96 0.04 | Initialization 9: 0.96 0.04 | Initialization 10: 0.95 0.05 | \n",
            "Sample 50 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.98 0.02 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 51 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.02 0.98 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 52 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 53 Initialization 1: 0.68 0.32 | Initialization 2: 0.77 0.23 | Initialization 3: 0.71 0.29 | Initialization 4: 0.61 0.39 | Initialization 5: 0.49 0.51 | Initialization 6: 0.66 0.34 | Initialization 7: 0.61 0.39 | Initialization 8: 0.77 0.23 | Initialization 9: 0.62 0.38 | Initialization 10: 0.65 0.35 | \n",
            "Sample 54 Initialization 1: 0.97 0.03 | Initialization 2: 0.98 0.02 | Initialization 3: 0.97 0.03 | Initialization 4: 0.98 0.02 | Initialization 5: 0.99 0.01 | Initialization 6: 0.95 0.05 | Initialization 7: 0.96 0.04 | Initialization 8: 0.97 0.03 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 55 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 56 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.99 0.01 | Initialization 10: 0.98 0.02 | \n",
            "Sample 57 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 58 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 59 Initialization 1: 0.16 0.84 | Initialization 2: 0.15 0.85 | Initialization 3: 0.15 0.85 | Initialization 4: 0.12 0.88 | Initialization 5: 0.17 0.83 | Initialization 6: 0.14 0.86 | Initialization 7: 0.31 0.69 | Initialization 8: 0.16 0.84 | Initialization 9: 0.22 0.78 | Initialization 10: 0.11 0.89 | \n",
            "Sample 60 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 61 Initialization 1: 0.99 0.01 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.99 0.01 | Initialization 10: 0.98 0.02 | \n",
            "Sample 62 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 63 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 64 Initialization 1: 0.95 0.05 | Initialization 2: 0.95 0.05 | Initialization 3: 0.95 0.05 | Initialization 4: 0.96 0.04 | Initialization 5: 0.88 0.12 | Initialization 6: 0.94 0.06 | Initialization 7: 0.94 0.06 | Initialization 8: 0.92 0.08 | Initialization 9: 0.93 0.07 | Initialization 10: 0.95 0.05 | \n",
            "Sample 65 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 66 Initialization 1: 0.89 0.11 | Initialization 2: 0.93 0.07 | Initialization 3: 0.90 0.10 | Initialization 4: 0.89 0.11 | Initialization 5: 0.92 0.08 | Initialization 6: 0.91 0.09 | Initialization 7: 0.92 0.08 | Initialization 8: 0.92 0.08 | Initialization 9: 0.90 0.10 | Initialization 10: 0.88 0.12 | \n",
            "Sample 67 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.02 0.98 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 68 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.02 0.98 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 69 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 70 Initialization 1: 0.97 0.03 | Initialization 2: 0.97 0.03 | Initialization 3: 0.97 0.03 | Initialization 4: 0.97 0.03 | Initialization 5: 0.97 0.03 | Initialization 6: 0.97 0.03 | Initialization 7: 0.98 0.02 | Initialization 8: 0.96 0.04 | Initialization 9: 0.97 0.03 | Initialization 10: 0.95 0.05 | \n",
            "Sample 71 Initialization 1: 0.02 0.98 | Initialization 2: 0.03 0.97 | Initialization 3: 0.02 0.98 | Initialization 4: 0.02 0.98 | Initialization 5: 0.03 0.97 | Initialization 6: 0.03 0.97 | Initialization 7: 0.04 0.96 | Initialization 8: 0.03 0.97 | Initialization 9: 0.04 0.96 | Initialization 10: 0.02 0.98 | \n",
            "Sample 72 Initialization 1: 0.02 0.98 | Initialization 2: 0.02 0.98 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.03 0.97 | Initialization 6: 0.03 0.97 | Initialization 7: 0.02 0.98 | Initialization 8: 0.02 0.98 | Initialization 9: 0.02 0.98 | Initialization 10: 0.02 0.98 | \n",
            "Sample 73 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.98 0.02 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 74 Initialization 1: 0.02 0.98 | Initialization 2: 0.03 0.97 | Initialization 3: 0.02 0.98 | Initialization 4: 0.02 0.98 | Initialization 5: 0.02 0.98 | Initialization 6: 0.03 0.97 | Initialization 7: 0.02 0.98 | Initialization 8: 0.02 0.98 | Initialization 9: 0.03 0.97 | Initialization 10: 0.02 0.98 | \n",
            "Sample 75 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 76 Initialization 1: 0.97 0.03 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.97 0.03 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 77 Initialization 1: 0.97 0.03 | Initialization 2: 0.97 0.03 | Initialization 3: 0.97 0.03 | Initialization 4: 0.97 0.03 | Initialization 5: 0.98 0.02 | Initialization 6: 0.97 0.03 | Initialization 7: 0.95 0.05 | Initialization 8: 0.97 0.03 | Initialization 9: 0.97 0.03 | Initialization 10: 0.97 0.03 | \n",
            "Sample 78 Initialization 1: 0.48 0.52 | Initialization 2: 0.46 0.54 | Initialization 3: 0.43 0.57 | Initialization 4: 0.49 0.51 | Initialization 5: 0.49 0.51 | Initialization 6: 0.45 0.55 | Initialization 7: 0.58 0.42 | Initialization 8: 0.42 0.58 | Initialization 9: 0.42 0.58 | Initialization 10: 0.37 0.63 | \n",
            "Sample 79 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 80 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.99 0.01 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 81 Initialization 1: 0.02 0.98 | Initialization 2: 0.03 0.97 | Initialization 3: 0.02 0.98 | Initialization 4: 0.02 0.98 | Initialization 5: 0.02 0.98 | Initialization 6: 0.03 0.97 | Initialization 7: 0.02 0.98 | Initialization 8: 0.02 0.98 | Initialization 9: 0.03 0.97 | Initialization 10: 0.02 0.98 | \n",
            "Sample 82 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 83 Initialization 1: 0.31 0.69 | Initialization 2: 0.26 0.74 | Initialization 3: 0.22 0.78 | Initialization 4: 0.31 0.69 | Initialization 5: 0.31 0.69 | Initialization 6: 0.21 0.79 | Initialization 7: 0.22 0.78 | Initialization 8: 0.29 0.71 | Initialization 9: 0.32 0.68 | Initialization 10: 0.30 0.70 | \n",
            "Sample 84 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 85 Initialization 1: 0.03 0.97 | Initialization 2: 0.02 0.98 | Initialization 3: 0.03 0.97 | Initialization 4: 0.02 0.98 | Initialization 5: 0.02 0.98 | Initialization 6: 0.03 0.97 | Initialization 7: 0.02 0.98 | Initialization 8: 0.02 0.98 | Initialization 9: 0.04 0.96 | Initialization 10: 0.03 0.97 | \n",
            "Sample 86 Initialization 1: 0.02 0.98 | Initialization 2: 0.03 0.97 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.02 0.98 | Initialization 6: 0.03 0.97 | Initialization 7: 0.03 0.97 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.02 0.98 | \n",
            "Sample 87 Initialization 1: 0.01 0.99 | Initialization 2: 0.02 0.98 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.02 0.98 | Initialization 6: 0.02 0.98 | Initialization 7: 0.03 0.97 | Initialization 8: 0.02 0.98 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 88 Initialization 1: 0.01 0.99 | Initialization 2: 0.02 0.98 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 89 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.98 0.02 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 90 Initialization 1: 0.98 0.02 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 91 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.98 0.02 | Initialization 9: 0.99 0.01 | Initialization 10: 0.98 0.02 | \n",
            "Sample 92 Initialization 1: 0.64 0.36 | Initialization 2: 0.63 0.37 | Initialization 3: 0.68 0.32 | Initialization 4: 0.69 0.31 | Initialization 5: 0.60 0.40 | Initialization 6: 0.68 0.32 | Initialization 7: 0.74 0.26 | Initialization 8: 0.66 0.34 | Initialization 9: 0.62 0.38 | Initialization 10: 0.55 0.45 | \n",
            "Sample 93 Initialization 1: 0.95 0.05 | Initialization 2: 0.96 0.04 | Initialization 3: 0.95 0.05 | Initialization 4: 0.95 0.05 | Initialization 5: 0.89 0.11 | Initialization 6: 0.94 0.06 | Initialization 7: 0.92 0.08 | Initialization 8: 0.95 0.05 | Initialization 9: 0.96 0.04 | Initialization 10: 0.97 0.03 | \n",
            "Sample 94 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.97 0.03 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.97 0.03 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 95 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 96 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 97 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 98 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.03 0.97 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 99 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 100 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.02 0.98 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 101 Initialization 1: 0.02 0.98 | Initialization 2: 0.02 0.98 | Initialization 3: 0.02 0.98 | Initialization 4: 0.02 0.98 | Initialization 5: 0.04 0.96 | Initialization 6: 0.03 0.97 | Initialization 7: 0.03 0.97 | Initialization 8: 0.02 0.98 | Initialization 9: 0.03 0.97 | Initialization 10: 0.02 0.98 | \n",
            "Sample 102 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 103 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.02 0.98 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 104 Initialization 1: 0.01 0.99 | Initialization 2: 0.02 0.98 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.02 0.98 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 105 Initialization 1: 0.98 0.02 | Initialization 2: 0.99 0.01 | Initialization 3: 0.98 0.02 | Initialization 4: 0.99 0.01 | Initialization 5: 0.98 0.02 | Initialization 6: 0.99 0.01 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 106 Initialization 1: 0.97 0.03 | Initialization 2: 0.97 0.03 | Initialization 3: 0.97 0.03 | Initialization 4: 0.97 0.03 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.96 0.04 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.97 0.03 | \n",
            "Sample 107 Initialization 1: 0.98 0.02 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.98 0.02 | Initialization 5: 0.99 0.01 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.99 0.01 | Initialization 10: 0.98 0.02 | \n",
            "Sample 108 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 109 Initialization 1: 0.89 0.11 | Initialization 2: 0.89 0.11 | Initialization 3: 0.85 0.15 | Initialization 4: 0.90 0.10 | Initialization 5: 0.92 0.08 | Initialization 6: 0.88 0.12 | Initialization 7: 0.89 0.11 | Initialization 8: 0.92 0.08 | Initialization 9: 0.89 0.11 | Initialization 10: 0.89 0.11 | \n",
            "Sample 110 Initialization 1: 0.97 0.03 | Initialization 2: 0.96 0.04 | Initialization 3: 0.95 0.05 | Initialization 4: 0.96 0.04 | Initialization 5: 0.97 0.03 | Initialization 6: 0.96 0.04 | Initialization 7: 0.94 0.06 | Initialization 8: 0.96 0.04 | Initialization 9: 0.95 0.05 | Initialization 10: 0.95 0.05 | \n",
            "Sample 111 Initialization 1: 0.03 0.97 | Initialization 2: 0.02 0.98 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.02 0.98 | Initialization 6: 0.02 0.98 | Initialization 7: 0.03 0.97 | Initialization 8: 0.02 0.98 | Initialization 9: 0.03 0.97 | Initialization 10: 0.02 0.98 | \n",
            "Sample 112 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 113 Initialization 1: 0.53 0.47 | Initialization 2: 0.57 0.43 | Initialization 3: 0.50 0.50 | Initialization 4: 0.38 0.62 | Initialization 5: 0.51 0.49 | Initialization 6: 0.54 0.46 | Initialization 7: 0.36 0.64 | Initialization 8: 0.60 0.40 | Initialization 9: 0.52 0.48 | Initialization 10: 0.59 0.41 | \n",
            "Sample 114 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "\n",
            "Learning rate = 0.1\n",
            "Sample 1 Initialization 1: 0.95 0.05 | Initialization 2: 0.95 0.05 | Initialization 3: 0.94 0.06 | Initialization 4: 0.94 0.06 | Initialization 5: 0.96 0.04 | Initialization 6: 0.95 0.05 | Initialization 7: 0.93 0.07 | Initialization 8: 0.94 0.06 | Initialization 9: 0.94 0.06 | Initialization 10: 0.95 0.05 | \n",
            "Sample 2 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 3 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.03 0.97 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 4 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 5 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 6 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 7 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 8 Initialization 1: 0.03 0.97 | Initialization 2: 0.03 0.97 | Initialization 3: 0.04 0.96 | Initialization 4: 0.07 0.93 | Initialization 5: 0.02 0.98 | Initialization 6: 0.03 0.97 | Initialization 7: 0.03 0.97 | Initialization 8: 0.04 0.96 | Initialization 9: 0.03 0.97 | Initialization 10: 0.03 0.97 | \n",
            "Sample 9 Initialization 1: 0.68 0.32 | Initialization 2: 0.72 0.28 | Initialization 3: 0.73 0.27 | Initialization 4: 0.66 0.34 | Initialization 5: 0.76 0.24 | Initialization 6: 0.78 0.22 | Initialization 7: 0.65 0.35 | Initialization 8: 0.73 0.27 | Initialization 9: 0.69 0.31 | Initialization 10: 0.67 0.33 | \n",
            "Sample 10 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 11 Initialization 1: 0.97 0.03 | Initialization 2: 0.97 0.03 | Initialization 3: 0.96 0.04 | Initialization 4: 0.96 0.04 | Initialization 5: 0.97 0.03 | Initialization 6: 0.97 0.03 | Initialization 7: 0.97 0.03 | Initialization 8: 0.96 0.04 | Initialization 9: 0.96 0.04 | Initialization 10: 0.97 0.03 | \n",
            "Sample 12 Initialization 1: 0.02 0.98 | Initialization 2: 0.02 0.98 | Initialization 3: 0.02 0.98 | Initialization 4: 0.05 0.95 | Initialization 5: 0.01 0.99 | Initialization 6: 0.02 0.98 | Initialization 7: 0.02 0.98 | Initialization 8: 0.02 0.98 | Initialization 9: 0.02 0.98 | Initialization 10: 0.02 0.98 | \n",
            "Sample 13 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 14 Initialization 1: 0.07 0.93 | Initialization 2: 0.14 0.86 | Initialization 3: 0.15 0.85 | Initialization 4: 0.11 0.89 | Initialization 5: 0.04 0.96 | Initialization 6: 0.19 0.81 | Initialization 7: 0.06 0.94 | Initialization 8: 0.07 0.93 | Initialization 9: 0.06 0.94 | Initialization 10: 0.09 0.91 | \n",
            "Sample 15 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 16 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 17 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 18 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 19 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 20 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 21 Initialization 1: 0.94 0.06 | Initialization 2: 0.95 0.05 | Initialization 3: 0.95 0.05 | Initialization 4: 0.93 0.07 | Initialization 5: 0.95 0.05 | Initialization 6: 0.95 0.05 | Initialization 7: 0.92 0.08 | Initialization 8: 0.94 0.06 | Initialization 9: 0.95 0.05 | Initialization 10: 0.95 0.05 | \n",
            "Sample 22 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 23 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 24 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 25 Initialization 1: 1.00 0.00 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 26 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 1.00 0.00 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 27 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 28 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 29 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 30 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 31 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 32 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 33 Initialization 1: 1.00 0.00 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 1.00 0.00 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 34 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 35 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 36 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 37 Initialization 1: 0.06 0.94 | Initialization 2: 0.04 0.96 | Initialization 3: 0.06 0.94 | Initialization 4: 0.11 0.89 | Initialization 5: 0.05 0.95 | Initialization 6: 0.05 0.95 | Initialization 7: 0.06 0.94 | Initialization 8: 0.06 0.94 | Initialization 9: 0.08 0.92 | Initialization 10: 0.06 0.94 | \n",
            "Sample 38 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 39 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 40 Initialization 1: 0.96 0.04 | Initialization 2: 0.96 0.04 | Initialization 3: 0.97 0.03 | Initialization 4: 0.95 0.05 | Initialization 5: 0.97 0.03 | Initialization 6: 0.97 0.03 | Initialization 7: 0.95 0.05 | Initialization 8: 0.95 0.05 | Initialization 9: 0.96 0.04 | Initialization 10: 0.96 0.04 | \n",
            "Sample 41 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 42 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.00 1.00 | \n",
            "Sample 43 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 1.00 0.00 | \n",
            "Sample 44 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 45 Initialization 1: 0.98 0.02 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.97 0.03 | Initialization 6: 0.99 0.01 | Initialization 7: 0.97 0.03 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.98 0.02 | \n",
            "Sample 46 Initialization 1: 0.96 0.04 | Initialization 2: 0.97 0.03 | Initialization 3: 0.97 0.03 | Initialization 4: 0.97 0.03 | Initialization 5: 0.97 0.03 | Initialization 6: 0.97 0.03 | Initialization 7: 0.97 0.03 | Initialization 8: 0.97 0.03 | Initialization 9: 0.96 0.04 | Initialization 10: 0.97 0.03 | \n",
            "Sample 47 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 0.98 0.02 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 48 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 49 Initialization 1: 0.97 0.03 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.97 0.03 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.97 0.03 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 50 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 51 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 52 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 53 Initialization 1: 0.80 0.20 | Initialization 2: 0.56 0.44 | Initialization 3: 0.79 0.21 | Initialization 4: 0.73 0.27 | Initialization 5: 0.73 0.27 | Initialization 6: 0.78 0.22 | Initialization 7: 0.79 0.21 | Initialization 8: 0.78 0.22 | Initialization 9: 0.76 0.24 | Initialization 10: 0.80 0.20 | \n",
            "Sample 54 Initialization 1: 0.98 0.02 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.98 0.02 | Initialization 5: 0.97 0.03 | Initialization 6: 0.98 0.02 | Initialization 7: 0.97 0.03 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.98 0.02 | \n",
            "Sample 55 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 56 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 57 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 58 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 59 Initialization 1: 0.09 0.91 | Initialization 2: 0.12 0.88 | Initialization 3: 0.11 0.89 | Initialization 4: 0.13 0.87 | Initialization 5: 0.13 0.87 | Initialization 6: 0.10 0.90 | Initialization 7: 0.11 0.89 | Initialization 8: 0.13 0.87 | Initialization 9: 0.10 0.90 | Initialization 10: 0.07 0.93 | \n",
            "Sample 60 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 61 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 62 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 63 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 64 Initialization 1: 0.98 0.02 | Initialization 2: 0.97 0.03 | Initialization 3: 0.96 0.04 | Initialization 4: 0.97 0.03 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.97 0.03 | Initialization 8: 0.96 0.04 | Initialization 9: 0.96 0.04 | Initialization 10: 0.98 0.02 | \n",
            "Sample 65 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 66 Initialization 1: 0.94 0.06 | Initialization 2: 0.95 0.05 | Initialization 3: 0.95 0.05 | Initialization 4: 0.94 0.06 | Initialization 5: 0.96 0.04 | Initialization 6: 0.95 0.05 | Initialization 7: 0.91 0.09 | Initialization 8: 0.94 0.06 | Initialization 9: 0.94 0.06 | Initialization 10: 0.94 0.06 | \n",
            "Sample 67 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 68 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 69 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 70 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.99 0.01 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.99 0.01 | \n",
            "Sample 71 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.03 0.97 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 72 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.03 0.97 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 73 Initialization 1: 1.00 0.00 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 74 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.03 0.97 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 75 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 76 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 77 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.99 0.01 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.99 0.01 | Initialization 10: 0.98 0.02 | \n",
            "Sample 78 Initialization 1: 0.33 0.67 | Initialization 2: 0.51 0.49 | Initialization 3: 0.51 0.49 | Initialization 4: 0.42 0.58 | Initialization 5: 0.48 0.52 | Initialization 6: 0.47 0.53 | Initialization 7: 0.37 0.63 | Initialization 8: 0.41 0.59 | Initialization 9: 0.42 0.58 | Initialization 10: 0.47 0.53 | \n",
            "Sample 79 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 80 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.98 0.02 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 81 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.03 0.97 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 82 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 83 Initialization 1: 0.24 0.76 | Initialization 2: 0.21 0.79 | Initialization 3: 0.12 0.88 | Initialization 4: 0.19 0.81 | Initialization 5: 0.17 0.83 | Initialization 6: 0.15 0.85 | Initialization 7: 0.17 0.83 | Initialization 8: 0.21 0.79 | Initialization 9: 0.16 0.84 | Initialization 10: 0.17 0.83 | \n",
            "Sample 84 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 85 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.03 0.97 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 86 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 87 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.00 1.00 | \n",
            "Sample 88 Initialization 1: 0.01 0.99 | Initialization 2: 0.00 1.00 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.00 1.00 | \n",
            "Sample 89 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 90 Initialization 1: 0.99 0.01 | Initialization 2: 1.00 0.00 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 0.99 0.01 | \n",
            "Sample 91 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 92 Initialization 1: 0.50 0.50 | Initialization 2: 0.54 0.46 | Initialization 3: 0.70 0.30 | Initialization 4: 0.53 0.47 | Initialization 5: 0.59 0.41 | Initialization 6: 0.49 0.51 | Initialization 7: 0.69 0.31 | Initialization 8: 0.63 0.37 | Initialization 9: 0.55 0.45 | Initialization 10: 0.51 0.49 | \n",
            "Sample 93 Initialization 1: 0.99 0.01 | Initialization 2: 0.97 0.03 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.97 0.03 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 94 Initialization 1: 1.00 0.00 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 95 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 96 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 97 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 98 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 99 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 100 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 101 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.03 0.97 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 102 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 103 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 104 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 105 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 106 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.97 0.03 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 107 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 108 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 109 Initialization 1: 0.93 0.07 | Initialization 2: 0.92 0.08 | Initialization 3: 0.93 0.07 | Initialization 4: 0.91 0.09 | Initialization 5: 0.93 0.07 | Initialization 6: 0.92 0.08 | Initialization 7: 0.94 0.06 | Initialization 8: 0.94 0.06 | Initialization 9: 0.92 0.08 | Initialization 10: 0.92 0.08 | \n",
            "Sample 110 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.97 0.03 | Initialization 5: 0.97 0.03 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 111 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.03 0.97 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 112 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 0.99 0.01 | Initialization 5: 1.00 0.00 | Initialization 6: 0.99 0.01 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 113 Initialization 1: 0.70 0.30 | Initialization 2: 0.51 0.49 | Initialization 3: 0.58 0.42 | Initialization 4: 0.59 0.41 | Initialization 5: 0.61 0.39 | Initialization 6: 0.69 0.31 | Initialization 7: 0.54 0.46 | Initialization 8: 0.61 0.39 | Initialization 9: 0.56 0.44 | Initialization 10: 0.69 0.31 | \n",
            "Sample 114 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.02 0.98 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "\n",
            "Learning rate = 0.2\n",
            "Sample 1 Initialization 1: 0.95 0.05 | Initialization 2: 0.96 0.04 | Initialization 3: 0.96 0.04 | Initialization 4: 0.96 0.04 | Initialization 5: 0.94 0.06 | Initialization 6: 0.95 0.05 | Initialization 7: 0.95 0.05 | Initialization 8: 0.96 0.04 | Initialization 9: 0.95 0.05 | Initialization 10: 0.96 0.04 | \n",
            "Sample 2 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 3 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 4 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 5 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 6 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 7 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 8 Initialization 1: 0.02 0.98 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.02 0.98 | Initialization 5: 0.02 0.98 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.02 0.98 | Initialization 10: 0.01 0.99 | \n",
            "Sample 9 Initialization 1: 0.77 0.23 | Initialization 2: 0.81 0.19 | Initialization 3: 0.67 0.33 | Initialization 4: 0.71 0.29 | Initialization 5: 0.69 0.31 | Initialization 6: 0.77 0.23 | Initialization 7: 0.76 0.24 | Initialization 8: 0.76 0.24 | Initialization 9: 0.77 0.23 | Initialization 10: 0.76 0.24 | \n",
            "Sample 10 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 11 Initialization 1: 0.97 0.03 | Initialization 2: 0.97 0.03 | Initialization 3: 0.98 0.02 | Initialization 4: 0.97 0.03 | Initialization 5: 0.97 0.03 | Initialization 6: 0.97 0.03 | Initialization 7: 0.98 0.02 | Initialization 8: 0.98 0.02 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 12 Initialization 1: 0.01 0.99 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.01 0.99 | Initialization 6: 0.01 0.99 | Initialization 7: 0.01 0.99 | Initialization 8: 0.01 0.99 | Initialization 9: 0.01 0.99 | Initialization 10: 0.01 0.99 | \n",
            "Sample 13 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 1.00 0.00 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 14 Initialization 1: 0.05 0.95 | Initialization 2: 0.07 0.93 | Initialization 3: 0.10 0.90 | Initialization 4: 0.09 0.91 | Initialization 5: 0.07 0.93 | Initialization 6: 0.08 0.92 | Initialization 7: 0.05 0.95 | Initialization 8: 0.09 0.91 | Initialization 9: 0.08 0.92 | Initialization 10: 0.08 0.92 | \n",
            "Sample 15 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 16 Initialization 1: 0.00 1.00 | Initialization 2: 0.01 0.99 | Initialization 3: 0.01 0.99 | Initialization 4: 0.01 0.99 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.01 0.99 | \n",
            "Sample 17 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 18 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 19 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 20 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 21 Initialization 1: 0.96 0.04 | Initialization 2: 0.96 0.04 | Initialization 3: 0.97 0.03 | Initialization 4: 0.96 0.04 | Initialization 5: 0.97 0.03 | Initialization 6: 0.98 0.02 | Initialization 7: 0.97 0.03 | Initialization 8: 0.97 0.03 | Initialization 9: 0.97 0.03 | Initialization 10: 0.97 0.03 | \n",
            "Sample 22 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 23 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 24 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 25 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 26 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 27 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 28 Initialization 1: 1.00 0.00 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 29 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 30 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 31 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 32 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 33 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 34 Initialization 1: 1.00 0.00 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 1.00 0.00 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 35 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 36 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 37 Initialization 1: 0.02 0.98 | Initialization 2: 0.02 0.98 | Initialization 3: 0.02 0.98 | Initialization 4: 0.02 0.98 | Initialization 5: 0.02 0.98 | Initialization 6: 0.02 0.98 | Initialization 7: 0.02 0.98 | Initialization 8: 0.02 0.98 | Initialization 9: 0.02 0.98 | Initialization 10: 0.03 0.97 | \n",
            "Sample 38 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 39 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 40 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.99 0.01 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 41 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 42 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 43 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 44 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 45 Initialization 1: 0.98 0.02 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.98 0.02 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.98 0.02 | Initialization 10: 0.99 0.01 | \n",
            "Sample 46 Initialization 1: 0.96 0.04 | Initialization 2: 0.94 0.06 | Initialization 3: 0.96 0.04 | Initialization 4: 0.93 0.07 | Initialization 5: 0.95 0.05 | Initialization 6: 0.95 0.05 | Initialization 7: 0.95 0.05 | Initialization 8: 0.95 0.05 | Initialization 9: 0.97 0.03 | Initialization 10: 0.96 0.04 | \n",
            "Sample 47 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 48 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 49 Initialization 1: 0.98 0.02 | Initialization 2: 0.97 0.03 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 50 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 51 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 52 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 53 Initialization 1: 0.79 0.21 | Initialization 2: 0.76 0.24 | Initialization 3: 0.72 0.28 | Initialization 4: 0.75 0.25 | Initialization 5: 0.85 0.15 | Initialization 6: 0.83 0.17 | Initialization 7: 0.86 0.14 | Initialization 8: 0.83 0.17 | Initialization 9: 0.77 0.23 | Initialization 10: 0.72 0.28 | \n",
            "Sample 54 Initialization 1: 0.98 0.02 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 55 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 56 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 57 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 58 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 59 Initialization 1: 0.06 0.94 | Initialization 2: 0.05 0.95 | Initialization 3: 0.05 0.95 | Initialization 4: 0.05 0.95 | Initialization 5: 0.09 0.91 | Initialization 6: 0.08 0.92 | Initialization 7: 0.05 0.95 | Initialization 8: 0.04 0.96 | Initialization 9: 0.09 0.91 | Initialization 10: 0.06 0.94 | \n",
            "Sample 60 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 61 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 62 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 63 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 64 Initialization 1: 0.98 0.02 | Initialization 2: 0.97 0.03 | Initialization 3: 0.98 0.02 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.98 0.02 | Initialization 7: 0.98 0.02 | Initialization 8: 0.99 0.01 | Initialization 9: 0.98 0.02 | Initialization 10: 0.98 0.02 | \n",
            "Sample 65 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 66 Initialization 1: 0.94 0.06 | Initialization 2: 0.95 0.05 | Initialization 3: 0.94 0.06 | Initialization 4: 0.94 0.06 | Initialization 5: 0.95 0.05 | Initialization 6: 0.96 0.04 | Initialization 7: 0.95 0.05 | Initialization 8: 0.95 0.05 | Initialization 9: 0.96 0.04 | Initialization 10: 0.96 0.04 | \n",
            "Sample 67 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 68 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 69 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 70 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 71 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.01 0.99 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 72 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 73 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 74 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 75 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 76 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 77 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 78 Initialization 1: 0.42 0.58 | Initialization 2: 0.41 0.59 | Initialization 3: 0.37 0.63 | Initialization 4: 0.35 0.65 | Initialization 5: 0.44 0.56 | Initialization 6: 0.37 0.63 | Initialization 7: 0.32 0.68 | Initialization 8: 0.44 0.56 | Initialization 9: 0.44 0.56 | Initialization 10: 0.36 0.64 | \n",
            "Sample 79 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 80 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 81 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 82 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 83 Initialization 1: 0.14 0.86 | Initialization 2: 0.19 0.81 | Initialization 3: 0.19 0.81 | Initialization 4: 0.16 0.84 | Initialization 5: 0.13 0.87 | Initialization 6: 0.17 0.83 | Initialization 7: 0.14 0.86 | Initialization 8: 0.11 0.89 | Initialization 9: 0.09 0.91 | Initialization 10: 0.14 0.86 | \n",
            "Sample 84 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 85 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 86 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 87 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 88 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 89 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 90 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 91 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 92 Initialization 1: 0.48 0.52 | Initialization 2: 0.43 0.57 | Initialization 3: 0.42 0.58 | Initialization 4: 0.37 0.63 | Initialization 5: 0.60 0.40 | Initialization 6: 0.41 0.59 | Initialization 7: 0.48 0.52 | Initialization 8: 0.52 0.48 | Initialization 9: 0.62 0.38 | Initialization 10: 0.38 0.62 | \n",
            "Sample 93 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 94 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 95 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 96 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 97 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 98 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 99 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 100 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 101 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 102 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 103 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 104 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 105 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 0.99 0.01 | Initialization 6: 1.00 0.00 | Initialization 7: 0.99 0.01 | Initialization 8: 1.00 0.00 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 106 Initialization 1: 0.99 0.01 | Initialization 2: 0.99 0.01 | Initialization 3: 0.99 0.01 | Initialization 4: 0.99 0.01 | Initialization 5: 0.99 0.01 | Initialization 6: 0.99 0.01 | Initialization 7: 0.99 0.01 | Initialization 8: 0.99 0.01 | Initialization 9: 0.99 0.01 | Initialization 10: 0.99 0.01 | \n",
            "Sample 107 Initialization 1: 1.00 0.00 | Initialization 2: 0.99 0.01 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 108 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 109 Initialization 1: 0.96 0.04 | Initialization 2: 0.96 0.04 | Initialization 3: 0.95 0.05 | Initialization 4: 0.95 0.05 | Initialization 5: 0.95 0.05 | Initialization 6: 0.96 0.04 | Initialization 7: 0.96 0.04 | Initialization 8: 0.93 0.07 | Initialization 9: 0.94 0.06 | Initialization 10: 0.94 0.06 | \n",
            "Sample 110 Initialization 1: 0.98 0.02 | Initialization 2: 0.98 0.02 | Initialization 3: 0.99 0.01 | Initialization 4: 0.98 0.02 | Initialization 5: 0.98 0.02 | Initialization 6: 0.99 0.01 | Initialization 7: 0.98 0.02 | Initialization 8: 0.99 0.01 | Initialization 9: 0.98 0.02 | Initialization 10: 0.99 0.01 | \n",
            "Sample 111 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "Sample 112 Initialization 1: 1.00 0.00 | Initialization 2: 1.00 0.00 | Initialization 3: 1.00 0.00 | Initialization 4: 1.00 0.00 | Initialization 5: 1.00 0.00 | Initialization 6: 1.00 0.00 | Initialization 7: 1.00 0.00 | Initialization 8: 1.00 0.00 | Initialization 9: 1.00 0.00 | Initialization 10: 1.00 0.00 | \n",
            "Sample 113 Initialization 1: 0.72 0.28 | Initialization 2: 0.78 0.22 | Initialization 3: 0.69 0.31 | Initialization 4: 0.70 0.30 | Initialization 5: 0.64 0.36 | Initialization 6: 0.77 0.23 | Initialization 7: 0.68 0.32 | Initialization 8: 0.70 0.30 | Initialization 9: 0.61 0.39 | Initialization 10: 0.64 0.36 | \n",
            "Sample 114 Initialization 1: 0.00 1.00 | Initialization 2: 0.00 1.00 | Initialization 3: 0.00 1.00 | Initialization 4: 0.00 1.00 | Initialization 5: 0.00 1.00 | Initialization 6: 0.00 1.00 | Initialization 7: 0.00 1.00 | Initialization 8: 0.00 1.00 | Initialization 9: 0.00 1.00 | Initialization 10: 0.00 1.00 | \n",
            "\n"
          ]
        }
      ],
      "source": [
        "class_probabilities = np.zeros((5, 10, X_test.shape[0], output_size))\n",
        "\n",
        "# Extract weights and biases and compute class probabilities\n",
        "for lr_idx in range(5):\n",
        "    for init_idx in range(10):\n",
        "        # Retrieve weights and biases\n",
        "        W1 = updated_weights_biases[lr_idx, init_idx, :input_size * hidden_size].reshape(input_size, hidden_size)\n",
        "        b1 = updated_weights_biases[lr_idx, init_idx, input_size * hidden_size:input_size * hidden_size + hidden_size].reshape(1, hidden_size)\n",
        "        W2 = updated_weights_biases[lr_idx, init_idx, input_size * hidden_size + hidden_size:input_size * hidden_size + hidden_size + hidden_size * output_size].reshape(hidden_size, output_size)\n",
        "        b2 = updated_weights_biases[lr_idx, init_idx, input_size * hidden_size + hidden_size + hidden_size * output_size:].reshape(1, output_size)\n",
        "\n",
        "        # Compute class probabilities for test samples\n",
        "        _, probs = forward_propagation(X_test, W1, b1, W2, b2)\n",
        "        class_probabilities[lr_idx, init_idx, :, :] = probs\n",
        "\n",
        "# Print class probabilities\n",
        "for lr_idx in range(5):\n",
        "    print(f\"Learning rate = {learning_rates[lr_idx]}\")\n",
        "    for sample_idx in range(X_test.shape[0]):\n",
        "        print(f\"Sample {sample_idx + 1}\", end=\" \")\n",
        "        for init_idx in range(10):\n",
        "            print(f\"Initialization {init_idx + 1}: \", end=\"\")\n",
        "            print(\" \".join(f\"{prob:.2f}\" for prob in class_probabilities[lr_idx, init_idx, sample_idx]), end=\" | \")\n",
        "        print()  # Newline for each sample\n",
        "    print()  # Newline for each learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbzXUSQEFncC",
        "outputId": "e9138d8f-91c2-455b-e3d7-33cb7c1d33c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate = 0.001\n",
            "Sample 1: Class 1: (0.40, 0.82) | Class 2: (0.18, 0.60) | \n",
            "Sample 2: Class 1: (0.23, 0.72) | Class 2: (0.28, 0.77) | \n",
            "Sample 3: Class 1: (0.21, 0.68) | Class 2: (0.32, 0.79) | \n",
            "Sample 4: Class 1: (0.42, 0.81) | Class 2: (0.19, 0.58) | \n",
            "Sample 5: Class 1: (0.44, 0.90) | Class 2: (0.10, 0.56) | \n",
            "Sample 6: Class 1: (0.16, 0.49) | Class 2: (0.51, 0.84) | \n",
            "Sample 7: Class 1: (0.14, 0.59) | Class 2: (0.41, 0.86) | \n",
            "Sample 8: Class 1: (0.27, 0.74) | Class 2: (0.26, 0.73) | \n",
            "Sample 9: Class 1: (0.30, 0.63) | Class 2: (0.37, 0.70) | \n",
            "Sample 10: Class 1: (0.44, 0.93) | Class 2: (0.07, 0.56) | \n",
            "Sample 11: Class 1: (0.45, 0.89) | Class 2: (0.11, 0.55) | \n",
            "Sample 12: Class 1: (0.30, 0.80) | Class 2: (0.20, 0.70) | \n",
            "Sample 13: Class 1: (0.44, 0.93) | Class 2: (0.07, 0.56) | \n",
            "Sample 14: Class 1: (0.19, 0.68) | Class 2: (0.32, 0.81) | \n",
            "Sample 15: Class 1: (0.44, 0.91) | Class 2: (0.09, 0.56) | \n",
            "Sample 16: Class 1: (0.11, 0.58) | Class 2: (0.42, 0.89) | \n",
            "Sample 17: Class 1: (0.43, 0.93) | Class 2: (0.07, 0.57) | \n",
            "Sample 18: Class 1: (0.58, 0.94) | Class 2: (0.06, 0.42) | \n",
            "Sample 19: Class 1: (0.53, 0.93) | Class 2: (0.07, 0.47) | \n",
            "Sample 20: Class 1: (0.15, 0.58) | Class 2: (0.42, 0.85) | \n",
            "Sample 21: Class 1: (0.41, 0.78) | Class 2: (0.22, 0.59) | \n",
            "Sample 22: Class 1: (0.45, 0.88) | Class 2: (0.12, 0.55) | \n",
            "Sample 23: Class 1: (0.16, 0.51) | Class 2: (0.49, 0.84) | \n",
            "Sample 24: Class 1: (0.52, 0.94) | Class 2: (0.06, 0.48) | \n",
            "Sample 25: Class 1: (0.48, 0.93) | Class 2: (0.07, 0.52) | \n",
            "Sample 26: Class 1: (0.31, 0.88) | Class 2: (0.12, 0.69) | \n",
            "Sample 27: Class 1: (0.43, 0.91) | Class 2: (0.09, 0.57) | \n",
            "Sample 28: Class 1: (0.41, 0.85) | Class 2: (0.15, 0.59) | \n",
            "Sample 29: Class 1: (0.48, 0.90) | Class 2: (0.10, 0.52) | \n",
            "Sample 30: Class 1: (0.19, 0.56) | Class 2: (0.44, 0.81) | \n",
            "Sample 31: Class 1: (0.47, 0.93) | Class 2: (0.07, 0.53) | \n",
            "Sample 32: Class 1: (0.50, 0.94) | Class 2: (0.06, 0.50) | \n",
            "Sample 33: Class 1: (0.43, 0.89) | Class 2: (0.11, 0.57) | \n",
            "Sample 34: Class 1: (0.47, 0.91) | Class 2: (0.09, 0.53) | \n",
            "Sample 35: Class 1: (0.51, 0.90) | Class 2: (0.10, 0.49) | \n",
            "Sample 36: Class 1: (0.45, 0.91) | Class 2: (0.09, 0.55) | \n",
            "Sample 37: Class 1: (0.38, 0.84) | Class 2: (0.16, 0.62) | \n",
            "Sample 38: Class 1: (0.40, 0.95) | Class 2: (0.05, 0.60) | \n",
            "Sample 39: Class 1: (0.23, 0.71) | Class 2: (0.29, 0.77) | \n",
            "Sample 40: Class 1: (0.44, 0.88) | Class 2: (0.12, 0.56) | \n",
            "Sample 41: Class 1: (0.50, 0.92) | Class 2: (0.08, 0.50) | \n",
            "Sample 42: Class 1: (0.23, 0.63) | Class 2: (0.37, 0.77) | \n",
            "Sample 43: Class 1: (0.45, 0.87) | Class 2: (0.13, 0.55) | \n",
            "Sample 44: Class 1: (0.50, 0.91) | Class 2: (0.09, 0.50) | \n",
            "Sample 45: Class 1: (0.35, 0.90) | Class 2: (0.10, 0.65) | \n",
            "Sample 46: Class 1: (0.45, 0.83) | Class 2: (0.17, 0.55) | \n",
            "Sample 47: Class 1: (0.37, 0.81) | Class 2: (0.19, 0.63) | \n",
            "Sample 48: Class 1: (0.56, 0.93) | Class 2: (0.07, 0.44) | \n",
            "Sample 49: Class 1: (0.42, 0.81) | Class 2: (0.19, 0.58) | \n",
            "Sample 50: Class 1: (0.41, 0.91) | Class 2: (0.09, 0.59) | \n",
            "Sample 51: Class 1: (0.25, 0.74) | Class 2: (0.26, 0.75) | \n",
            "Sample 52: Class 1: (0.17, 0.54) | Class 2: (0.46, 0.83) | \n",
            "Sample 53: Class 1: (0.36, 0.78) | Class 2: (0.22, 0.64) | \n",
            "Sample 54: Class 1: (0.22, 0.90) | Class 2: (0.10, 0.78) | \n",
            "Sample 55: Class 1: (0.39, 0.84) | Class 2: (0.16, 0.61) | \n",
            "Sample 56: Class 1: (0.47, 0.91) | Class 2: (0.09, 0.53) | \n",
            "Sample 57: Class 1: (0.50, 0.86) | Class 2: (0.14, 0.50) | \n",
            "Sample 58: Class 1: (0.14, 0.50) | Class 2: (0.50, 0.86) | \n",
            "Sample 59: Class 1: (0.34, 0.71) | Class 2: (0.29, 0.66) | \n",
            "Sample 60: Class 1: (0.49, 0.92) | Class 2: (0.08, 0.51) | \n",
            "Sample 61: Class 1: (0.47, 0.88) | Class 2: (0.12, 0.53) | \n",
            "Sample 62: Class 1: (0.17, 0.55) | Class 2: (0.45, 0.83) | \n",
            "Sample 63: Class 1: (0.11, 0.61) | Class 2: (0.39, 0.89) | \n",
            "Sample 64: Class 1: (0.46, 0.90) | Class 2: (0.10, 0.54) | \n",
            "Sample 65: Class 1: (0.48, 0.94) | Class 2: (0.06, 0.52) | \n",
            "Sample 66: Class 1: (0.40, 0.83) | Class 2: (0.17, 0.60) | \n",
            "Sample 67: Class 1: (0.20, 0.52) | Class 2: (0.48, 0.80) | \n",
            "Sample 68: Class 1: (0.13, 0.49) | Class 2: (0.51, 0.87) | \n",
            "Sample 69: Class 1: (0.48, 0.93) | Class 2: (0.07, 0.52) | \n",
            "Sample 70: Class 1: (0.40, 0.79) | Class 2: (0.21, 0.60) | \n",
            "Sample 71: Class 1: (0.23, 0.62) | Class 2: (0.38, 0.77) | \n",
            "Sample 72: Class 1: (0.22, 0.62) | Class 2: (0.38, 0.78) | \n",
            "Sample 73: Class 1: (0.46, 0.94) | Class 2: (0.06, 0.54) | \n",
            "Sample 74: Class 1: (0.23, 0.59) | Class 2: (0.41, 0.77) | \n",
            "Sample 75: Class 1: (0.55, 0.90) | Class 2: (0.10, 0.45) | \n",
            "Sample 76: Class 1: (0.44, 0.82) | Class 2: (0.18, 0.56) | \n",
            "Sample 77: Class 1: (0.49, 0.90) | Class 2: (0.10, 0.51) | \n",
            "Sample 78: Class 1: (0.35, 0.75) | Class 2: (0.25, 0.65) | \n",
            "Sample 79: Class 1: (0.51, 0.95) | Class 2: (0.05, 0.49) | \n",
            "Sample 80: Class 1: (0.40, 0.78) | Class 2: (0.22, 0.60) | \n",
            "Sample 81: Class 1: (0.20, 0.71) | Class 2: (0.29, 0.80) | \n",
            "Sample 82: Class 1: (0.49, 0.95) | Class 2: (0.05, 0.51) | \n",
            "Sample 83: Class 1: (0.27, 0.80) | Class 2: (0.20, 0.73) | \n",
            "Sample 84: Class 1: (0.16, 0.56) | Class 2: (0.44, 0.84) | \n",
            "Sample 85: Class 1: (0.27, 0.81) | Class 2: (0.19, 0.73) | \n",
            "Sample 86: Class 1: (0.17, 0.51) | Class 2: (0.49, 0.83) | \n",
            "Sample 87: Class 1: (0.19, 0.52) | Class 2: (0.48, 0.81) | \n",
            "Sample 88: Class 1: (0.18, 0.56) | Class 2: (0.44, 0.82) | \n",
            "Sample 89: Class 1: (0.47, 0.89) | Class 2: (0.11, 0.53) | \n",
            "Sample 90: Class 1: (0.49, 0.87) | Class 2: (0.13, 0.51) | \n",
            "Sample 91: Class 1: (0.38, 0.90) | Class 2: (0.10, 0.62) | \n",
            "Sample 92: Class 1: (0.42, 0.81) | Class 2: (0.19, 0.58) | \n",
            "Sample 93: Class 1: (0.42, 0.85) | Class 2: (0.15, 0.58) | \n",
            "Sample 94: Class 1: (0.54, 0.96) | Class 2: (0.04, 0.46) | \n",
            "Sample 95: Class 1: (0.45, 0.91) | Class 2: (0.09, 0.55) | \n",
            "Sample 96: Class 1: (0.50, 0.89) | Class 2: (0.11, 0.50) | \n",
            "Sample 97: Class 1: (0.12, 0.70) | Class 2: (0.30, 0.88) | \n",
            "Sample 98: Class 1: (0.17, 0.54) | Class 2: (0.46, 0.83) | \n",
            "Sample 99: Class 1: (0.48, 0.94) | Class 2: (0.06, 0.52) | \n",
            "Sample 100: Class 1: (0.23, 0.68) | Class 2: (0.32, 0.77) | \n",
            "Sample 101: Class 1: (0.31, 0.77) | Class 2: (0.23, 0.69) | \n",
            "Sample 102: Class 1: (0.52, 0.95) | Class 2: (0.05, 0.48) | \n",
            "Sample 103: Class 1: (0.19, 0.55) | Class 2: (0.45, 0.81) | \n",
            "Sample 104: Class 1: (0.21, 0.63) | Class 2: (0.37, 0.79) | \n",
            "Sample 105: Class 1: (0.50, 0.91) | Class 2: (0.09, 0.50) | \n",
            "Sample 106: Class 1: (0.51, 0.87) | Class 2: (0.13, 0.49) | \n",
            "Sample 107: Class 1: (0.48, 0.81) | Class 2: (0.19, 0.52) | \n",
            "Sample 108: Class 1: (0.15, 0.63) | Class 2: (0.37, 0.85) | \n",
            "Sample 109: Class 1: (0.38, 0.83) | Class 2: (0.17, 0.62) | \n",
            "Sample 110: Class 1: (0.44, 0.91) | Class 2: (0.09, 0.56) | \n",
            "Sample 111: Class 1: (0.31, 0.74) | Class 2: (0.26, 0.69) | \n",
            "Sample 112: Class 1: (0.46, 0.91) | Class 2: (0.09, 0.54) | \n",
            "Sample 113: Class 1: (0.30, 0.82) | Class 2: (0.18, 0.70) | \n",
            "Sample 114: Class 1: (0.11, 0.72) | Class 2: (0.28, 0.89) | \n",
            "\n",
            "Learning rate = 0.01\n",
            "Sample 1: Class 1: (0.77, 0.88) | Class 2: (0.12, 0.23) | \n",
            "Sample 2: Class 1: (0.05, 0.23) | Class 2: (0.77, 0.95) | \n",
            "Sample 3: Class 1: (0.15, 0.36) | Class 2: (0.64, 0.85) | \n",
            "Sample 4: Class 1: (0.87, 0.95) | Class 2: (0.05, 0.13) | \n",
            "Sample 5: Class 1: (0.89, 0.97) | Class 2: (0.03, 0.11) | \n",
            "Sample 6: Class 1: (0.02, 0.20) | Class 2: (0.80, 0.98) | \n",
            "Sample 7: Class 1: (0.03, 0.20) | Class 2: (0.80, 0.97) | \n",
            "Sample 8: Class 1: (0.18, 0.35) | Class 2: (0.65, 0.82) | \n",
            "Sample 9: Class 1: (0.40, 0.66) | Class 2: (0.34, 0.60) | \n",
            "Sample 10: Class 1: (0.89, 0.95) | Class 2: (0.05, 0.11) | \n",
            "Sample 11: Class 1: (0.83, 0.93) | Class 2: (0.07, 0.17) | \n",
            "Sample 12: Class 1: (0.16, 0.34) | Class 2: (0.66, 0.84) | \n",
            "Sample 13: Class 1: (0.87, 0.94) | Class 2: (0.06, 0.13) | \n",
            "Sample 14: Class 1: (0.16, 0.55) | Class 2: (0.45, 0.84) | \n",
            "Sample 15: Class 1: (0.88, 0.96) | Class 2: (0.04, 0.12) | \n",
            "Sample 16: Class 1: (0.03, 0.24) | Class 2: (0.76, 0.97) | \n",
            "Sample 17: Class 1: (0.87, 0.97) | Class 2: (0.03, 0.13) | \n",
            "Sample 18: Class 1: (0.91, 0.99) | Class 2: (0.01, 0.09) | \n",
            "Sample 19: Class 1: (0.92, 0.99) | Class 2: (0.01, 0.08) | \n",
            "Sample 20: Class 1: (0.03, 0.21) | Class 2: (0.79, 0.97) | \n",
            "Sample 21: Class 1: (0.68, 0.81) | Class 2: (0.19, 0.32) | \n",
            "Sample 22: Class 1: (0.85, 0.95) | Class 2: (0.05, 0.15) | \n",
            "Sample 23: Class 1: (0.03, 0.20) | Class 2: (0.80, 0.97) | \n",
            "Sample 24: Class 1: (0.91, 0.98) | Class 2: (0.02, 0.09) | \n",
            "Sample 25: Class 1: (0.88, 0.96) | Class 2: (0.04, 0.12) | \n",
            "Sample 26: Class 1: (0.75, 0.96) | Class 2: (0.04, 0.25) | \n",
            "Sample 27: Class 1: (0.87, 0.95) | Class 2: (0.05, 0.13) | \n",
            "Sample 28: Class 1: (0.86, 0.93) | Class 2: (0.07, 0.14) | \n",
            "Sample 29: Class 1: (0.87, 0.96) | Class 2: (0.04, 0.13) | \n",
            "Sample 30: Class 1: (0.03, 0.22) | Class 2: (0.78, 0.97) | \n",
            "Sample 31: Class 1: (0.88, 0.96) | Class 2: (0.04, 0.12) | \n",
            "Sample 32: Class 1: (0.90, 0.98) | Class 2: (0.02, 0.10) | \n",
            "Sample 33: Class 1: (0.82, 0.97) | Class 2: (0.03, 0.18) | \n",
            "Sample 34: Class 1: (0.87, 0.94) | Class 2: (0.06, 0.13) | \n",
            "Sample 35: Class 1: (0.90, 0.98) | Class 2: (0.02, 0.10) | \n",
            "Sample 36: Class 1: (0.90, 0.98) | Class 2: (0.02, 0.10) | \n",
            "Sample 37: Class 1: (0.36, 0.68) | Class 2: (0.32, 0.64) | \n",
            "Sample 38: Class 1: (0.88, 0.97) | Class 2: (0.03, 0.12) | \n",
            "Sample 39: Class 1: (0.06, 0.25) | Class 2: (0.75, 0.94) | \n",
            "Sample 40: Class 1: (0.78, 0.88) | Class 2: (0.12, 0.22) | \n",
            "Sample 41: Class 1: (0.90, 0.98) | Class 2: (0.02, 0.10) | \n",
            "Sample 42: Class 1: (0.09, 0.38) | Class 2: (0.62, 0.91) | \n",
            "Sample 43: Class 1: (0.85, 0.96) | Class 2: (0.04, 0.15) | \n",
            "Sample 44: Class 1: (0.89, 0.97) | Class 2: (0.03, 0.11) | \n",
            "Sample 45: Class 1: (0.81, 0.96) | Class 2: (0.04, 0.19) | \n",
            "Sample 46: Class 1: (0.82, 0.93) | Class 2: (0.07, 0.18) | \n",
            "Sample 47: Class 1: (0.87, 0.98) | Class 2: (0.02, 0.13) | \n",
            "Sample 48: Class 1: (0.88, 0.98) | Class 2: (0.02, 0.12) | \n",
            "Sample 49: Class 1: (0.83, 0.91) | Class 2: (0.09, 0.17) | \n",
            "Sample 50: Class 1: (0.86, 0.94) | Class 2: (0.06, 0.14) | \n",
            "Sample 51: Class 1: (0.07, 0.27) | Class 2: (0.73, 0.93) | \n",
            "Sample 52: Class 1: (0.03, 0.22) | Class 2: (0.78, 0.97) | \n",
            "Sample 53: Class 1: (0.33, 0.77) | Class 2: (0.23, 0.67) | \n",
            "Sample 54: Class 1: (0.57, 0.96) | Class 2: (0.04, 0.43) | \n",
            "Sample 55: Class 1: (0.85, 0.97) | Class 2: (0.03, 0.15) | \n",
            "Sample 56: Class 1: (0.84, 0.93) | Class 2: (0.07, 0.16) | \n",
            "Sample 57: Class 1: (0.89, 0.97) | Class 2: (0.03, 0.11) | \n",
            "Sample 58: Class 1: (0.02, 0.20) | Class 2: (0.80, 0.98) | \n",
            "Sample 59: Class 1: (0.25, 0.51) | Class 2: (0.49, 0.75) | \n",
            "Sample 60: Class 1: (0.89, 0.98) | Class 2: (0.02, 0.11) | \n",
            "Sample 61: Class 1: (0.86, 0.95) | Class 2: (0.05, 0.14) | \n",
            "Sample 62: Class 1: (0.03, 0.20) | Class 2: (0.80, 0.97) | \n",
            "Sample 63: Class 1: (0.03, 0.21) | Class 2: (0.79, 0.97) | \n",
            "Sample 64: Class 1: (0.82, 0.93) | Class 2: (0.07, 0.18) | \n",
            "Sample 65: Class 1: (0.90, 0.98) | Class 2: (0.02, 0.10) | \n",
            "Sample 66: Class 1: (0.75, 0.87) | Class 2: (0.13, 0.25) | \n",
            "Sample 67: Class 1: (0.04, 0.23) | Class 2: (0.77, 0.96) | \n",
            "Sample 68: Class 1: (0.02, 0.22) | Class 2: (0.78, 0.98) | \n",
            "Sample 69: Class 1: (0.89, 0.98) | Class 2: (0.02, 0.11) | \n",
            "Sample 70: Class 1: (0.79, 0.87) | Class 2: (0.13, 0.21) | \n",
            "Sample 71: Class 1: (0.13, 0.37) | Class 2: (0.63, 0.87) | \n",
            "Sample 72: Class 1: (0.08, 0.28) | Class 2: (0.72, 0.92) | \n",
            "Sample 73: Class 1: (0.86, 0.95) | Class 2: (0.05, 0.14) | \n",
            "Sample 74: Class 1: (0.07, 0.27) | Class 2: (0.73, 0.93) | \n",
            "Sample 75: Class 1: (0.90, 0.98) | Class 2: (0.02, 0.10) | \n",
            "Sample 76: Class 1: (0.80, 0.92) | Class 2: (0.08, 0.20) | \n",
            "Sample 77: Class 1: (0.84, 0.93) | Class 2: (0.07, 0.16) | \n",
            "Sample 78: Class 1: (0.50, 0.63) | Class 2: (0.37, 0.50) | \n",
            "Sample 79: Class 1: (0.91, 0.98) | Class 2: (0.02, 0.09) | \n",
            "Sample 80: Class 1: (0.79, 0.93) | Class 2: (0.07, 0.21) | \n",
            "Sample 81: Class 1: (0.14, 0.30) | Class 2: (0.70, 0.86) | \n",
            "Sample 82: Class 1: (0.91, 0.98) | Class 2: (0.02, 0.09) | \n",
            "Sample 83: Class 1: (0.35, 0.64) | Class 2: (0.36, 0.65) | \n",
            "Sample 84: Class 1: (0.03, 0.21) | Class 2: (0.79, 0.97) | \n",
            "Sample 85: Class 1: (0.15, 0.47) | Class 2: (0.53, 0.85) | \n",
            "Sample 86: Class 1: (0.07, 0.23) | Class 2: (0.77, 0.93) | \n",
            "Sample 87: Class 1: (0.04, 0.29) | Class 2: (0.71, 0.96) | \n",
            "Sample 88: Class 1: (0.04, 0.25) | Class 2: (0.75, 0.96) | \n",
            "Sample 89: Class 1: (0.86, 0.94) | Class 2: (0.06, 0.14) | \n",
            "Sample 90: Class 1: (0.84, 0.95) | Class 2: (0.05, 0.16) | \n",
            "Sample 91: Class 1: (0.86, 0.96) | Class 2: (0.04, 0.14) | \n",
            "Sample 92: Class 1: (0.62, 0.76) | Class 2: (0.24, 0.38) | \n",
            "Sample 93: Class 1: (0.72, 0.83) | Class 2: (0.17, 0.28) | \n",
            "Sample 94: Class 1: (0.87, 0.97) | Class 2: (0.03, 0.13) | \n",
            "Sample 95: Class 1: (0.89, 0.98) | Class 2: (0.02, 0.11) | \n",
            "Sample 96: Class 1: (0.91, 0.98) | Class 2: (0.02, 0.09) | \n",
            "Sample 97: Class 1: (0.03, 0.23) | Class 2: (0.77, 0.97) | \n",
            "Sample 98: Class 1: (0.04, 0.24) | Class 2: (0.76, 0.96) | \n",
            "Sample 99: Class 1: (0.90, 0.98) | Class 2: (0.02, 0.10) | \n",
            "Sample 100: Class 1: (0.08, 0.30) | Class 2: (0.70, 0.92) | \n",
            "Sample 101: Class 1: (0.16, 0.41) | Class 2: (0.59, 0.84) | \n",
            "Sample 102: Class 1: (0.92, 0.99) | Class 2: (0.01, 0.08) | \n",
            "Sample 103: Class 1: (0.03, 0.23) | Class 2: (0.77, 0.97) | \n",
            "Sample 104: Class 1: (0.06, 0.24) | Class 2: (0.76, 0.94) | \n",
            "Sample 105: Class 1: (0.85, 0.96) | Class 2: (0.04, 0.15) | \n",
            "Sample 106: Class 1: (0.83, 0.94) | Class 2: (0.06, 0.17) | \n",
            "Sample 107: Class 1: (0.85, 0.93) | Class 2: (0.07, 0.15) | \n",
            "Sample 108: Class 1: (0.03, 0.20) | Class 2: (0.80, 0.97) | \n",
            "Sample 109: Class 1: (0.64, 0.79) | Class 2: (0.21, 0.36) | \n",
            "Sample 110: Class 1: (0.80, 0.92) | Class 2: (0.08, 0.20) | \n",
            "Sample 111: Class 1: (0.11, 0.34) | Class 2: (0.66, 0.89) | \n",
            "Sample 112: Class 1: (0.87, 0.97) | Class 2: (0.03, 0.13) | \n",
            "Sample 113: Class 1: (0.48, 0.78) | Class 2: (0.22, 0.52) | \n",
            "Sample 114: Class 1: (0.03, 0.22) | Class 2: (0.78, 0.97) | \n",
            "\n",
            "Learning rate = 0.05\n",
            "Sample 1: Class 1: (0.91, 0.94) | Class 2: (0.06, 0.09) | \n",
            "Sample 2: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 3: Class 1: (0.02, 0.03) | Class 2: (0.97, 0.98) | \n",
            "Sample 4: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 5: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 6: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 7: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 8: Class 1: (0.05, 0.09) | Class 2: (0.91, 0.95) | \n",
            "Sample 9: Class 1: (0.54, 0.72) | Class 2: (0.28, 0.46) | \n",
            "Sample 10: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 11: Class 1: (0.93, 0.96) | Class 2: (0.04, 0.07) | \n",
            "Sample 12: Class 1: (0.03, 0.06) | Class 2: (0.94, 0.97) | \n",
            "Sample 13: Class 1: (0.97, 0.98) | Class 2: (0.02, 0.03) | \n",
            "Sample 14: Class 1: (0.05, 0.23) | Class 2: (0.77, 0.95) | \n",
            "Sample 15: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 16: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 17: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 18: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 19: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 20: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 21: Class 1: (0.86, 0.91) | Class 2: (0.09, 0.14) | \n",
            "Sample 22: Class 1: (0.98, 0.98) | Class 2: (0.02, 0.02) | \n",
            "Sample 23: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 24: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 25: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 26: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 27: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 28: Class 1: (0.97, 0.99) | Class 2: (0.01, 0.03) | \n",
            "Sample 29: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 30: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 31: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 32: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 33: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 34: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 35: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 36: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 37: Class 1: (0.10, 0.21) | Class 2: (0.79, 0.90) | \n",
            "Sample 38: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 39: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 40: Class 1: (0.90, 0.94) | Class 2: (0.06, 0.10) | \n",
            "Sample 41: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 42: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 43: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 44: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 45: Class 1: (0.95, 0.99) | Class 2: (0.01, 0.05) | \n",
            "Sample 46: Class 1: (0.93, 0.98) | Class 2: (0.02, 0.07) | \n",
            "Sample 47: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 48: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 49: Class 1: (0.95, 0.97) | Class 2: (0.03, 0.05) | \n",
            "Sample 50: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 51: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 52: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 53: Class 1: (0.49, 0.77) | Class 2: (0.23, 0.51) | \n",
            "Sample 54: Class 1: (0.95, 0.99) | Class 2: (0.01, 0.05) | \n",
            "Sample 55: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 56: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 57: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 58: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 59: Class 1: (0.11, 0.31) | Class 2: (0.69, 0.89) | \n",
            "Sample 60: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 61: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 62: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 63: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 64: Class 1: (0.88, 0.96) | Class 2: (0.04, 0.12) | \n",
            "Sample 65: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 66: Class 1: (0.88, 0.93) | Class 2: (0.07, 0.12) | \n",
            "Sample 67: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 68: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 69: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 70: Class 1: (0.95, 0.98) | Class 2: (0.02, 0.05) | \n",
            "Sample 71: Class 1: (0.02, 0.04) | Class 2: (0.96, 0.98) | \n",
            "Sample 72: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 73: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 74: Class 1: (0.02, 0.03) | Class 2: (0.97, 0.98) | \n",
            "Sample 75: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 76: Class 1: (0.97, 0.98) | Class 2: (0.02, 0.03) | \n",
            "Sample 77: Class 1: (0.95, 0.98) | Class 2: (0.02, 0.05) | \n",
            "Sample 78: Class 1: (0.37, 0.58) | Class 2: (0.42, 0.63) | \n",
            "Sample 79: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 80: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 81: Class 1: (0.02, 0.03) | Class 2: (0.97, 0.98) | \n",
            "Sample 82: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 83: Class 1: (0.21, 0.32) | Class 2: (0.68, 0.79) | \n",
            "Sample 84: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 85: Class 1: (0.02, 0.04) | Class 2: (0.96, 0.98) | \n",
            "Sample 86: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 87: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 88: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 89: Class 1: (0.98, 1.00) | Class 2: (0.00, 0.02) | \n",
            "Sample 90: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 91: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 92: Class 1: (0.55, 0.74) | Class 2: (0.26, 0.45) | \n",
            "Sample 93: Class 1: (0.89, 0.97) | Class 2: (0.03, 0.11) | \n",
            "Sample 94: Class 1: (0.97, 0.99) | Class 2: (0.01, 0.03) | \n",
            "Sample 95: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 96: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 97: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 98: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 99: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 100: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 101: Class 1: (0.02, 0.04) | Class 2: (0.96, 0.98) | \n",
            "Sample 102: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 103: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 104: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 105: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 106: Class 1: (0.96, 0.98) | Class 2: (0.02, 0.04) | \n",
            "Sample 107: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 108: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 109: Class 1: (0.85, 0.92) | Class 2: (0.08, 0.15) | \n",
            "Sample 110: Class 1: (0.94, 0.97) | Class 2: (0.03, 0.06) | \n",
            "Sample 111: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 112: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 113: Class 1: (0.36, 0.60) | Class 2: (0.40, 0.64) | \n",
            "Sample 114: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "\n",
            "Learning rate = 0.1\n",
            "Sample 1: Class 1: (0.93, 0.96) | Class 2: (0.04, 0.07) | \n",
            "Sample 2: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 3: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 4: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 5: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 6: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 7: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 8: Class 1: (0.02, 0.07) | Class 2: (0.93, 0.98) | \n",
            "Sample 9: Class 1: (0.65, 0.78) | Class 2: (0.22, 0.35) | \n",
            "Sample 10: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 11: Class 1: (0.96, 0.97) | Class 2: (0.03, 0.04) | \n",
            "Sample 12: Class 1: (0.01, 0.05) | Class 2: (0.95, 0.99) | \n",
            "Sample 13: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 14: Class 1: (0.04, 0.19) | Class 2: (0.81, 0.96) | \n",
            "Sample 15: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 16: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 17: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 18: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 19: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 20: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 21: Class 1: (0.92, 0.95) | Class 2: (0.05, 0.08) | \n",
            "Sample 22: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 23: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 24: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 25: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 26: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 27: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 28: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 29: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 30: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 31: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 32: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 33: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 34: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 35: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 36: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 37: Class 1: (0.04, 0.11) | Class 2: (0.89, 0.96) | \n",
            "Sample 38: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 39: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 40: Class 1: (0.95, 0.97) | Class 2: (0.03, 0.05) | \n",
            "Sample 41: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 42: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 43: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 44: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 45: Class 1: (0.97, 0.99) | Class 2: (0.01, 0.03) | \n",
            "Sample 46: Class 1: (0.96, 0.97) | Class 2: (0.03, 0.04) | \n",
            "Sample 47: Class 1: (0.98, 1.00) | Class 2: (0.00, 0.02) | \n",
            "Sample 48: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 49: Class 1: (0.97, 0.98) | Class 2: (0.02, 0.03) | \n",
            "Sample 50: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 51: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 52: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 53: Class 1: (0.56, 0.80) | Class 2: (0.20, 0.44) | \n",
            "Sample 54: Class 1: (0.97, 0.99) | Class 2: (0.01, 0.03) | \n",
            "Sample 55: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 56: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 57: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 58: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 59: Class 1: (0.07, 0.13) | Class 2: (0.87, 0.93) | \n",
            "Sample 60: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 61: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 62: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 63: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 64: Class 1: (0.96, 0.98) | Class 2: (0.02, 0.04) | \n",
            "Sample 65: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 66: Class 1: (0.91, 0.96) | Class 2: (0.04, 0.09) | \n",
            "Sample 67: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 68: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 69: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 70: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 71: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 72: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 73: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 74: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 75: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 76: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 77: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 78: Class 1: (0.33, 0.51) | Class 2: (0.49, 0.67) | \n",
            "Sample 79: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 80: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 81: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 82: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 83: Class 1: (0.12, 0.24) | Class 2: (0.76, 0.88) | \n",
            "Sample 84: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 85: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 86: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 87: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 88: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 89: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 90: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 91: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 92: Class 1: (0.49, 0.70) | Class 2: (0.30, 0.51) | \n",
            "Sample 93: Class 1: (0.97, 0.99) | Class 2: (0.01, 0.03) | \n",
            "Sample 94: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 95: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 96: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 97: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 98: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 99: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 100: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 101: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 102: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 103: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 104: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 105: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 106: Class 1: (0.97, 0.99) | Class 2: (0.01, 0.03) | \n",
            "Sample 107: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 108: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "Sample 109: Class 1: (0.91, 0.94) | Class 2: (0.06, 0.09) | \n",
            "Sample 110: Class 1: (0.97, 0.98) | Class 2: (0.02, 0.03) | \n",
            "Sample 111: Class 1: (0.01, 0.03) | Class 2: (0.97, 0.99) | \n",
            "Sample 112: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 113: Class 1: (0.51, 0.70) | Class 2: (0.30, 0.49) | \n",
            "Sample 114: Class 1: (0.00, 0.02) | Class 2: (0.98, 1.00) | \n",
            "\n",
            "Learning rate = 0.2\n",
            "Sample 1: Class 1: (0.94, 0.96) | Class 2: (0.04, 0.06) | \n",
            "Sample 2: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 3: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 4: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 5: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 6: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 7: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 8: Class 1: (0.01, 0.02) | Class 2: (0.98, 0.99) | \n",
            "Sample 9: Class 1: (0.67, 0.81) | Class 2: (0.19, 0.33) | \n",
            "Sample 10: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 11: Class 1: (0.97, 0.98) | Class 2: (0.02, 0.03) | \n",
            "Sample 12: Class 1: (0.01, 0.01) | Class 2: (0.99, 0.99) | \n",
            "Sample 13: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 14: Class 1: (0.05, 0.10) | Class 2: (0.90, 0.95) | \n",
            "Sample 15: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 16: Class 1: (0.00, 0.01) | Class 2: (0.99, 1.00) | \n",
            "Sample 17: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 18: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 19: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 20: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 21: Class 1: (0.96, 0.98) | Class 2: (0.02, 0.04) | \n",
            "Sample 22: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 23: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 24: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 25: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 26: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 27: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 28: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 29: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 30: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 31: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 32: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 33: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 34: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 35: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 36: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 37: Class 1: (0.02, 0.03) | Class 2: (0.97, 0.98) | \n",
            "Sample 38: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 39: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 40: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 41: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 42: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 43: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 44: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 45: Class 1: (0.98, 1.00) | Class 2: (0.00, 0.02) | \n",
            "Sample 46: Class 1: (0.93, 0.97) | Class 2: (0.03, 0.07) | \n",
            "Sample 47: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 48: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 49: Class 1: (0.97, 0.99) | Class 2: (0.01, 0.03) | \n",
            "Sample 50: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 51: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 52: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 53: Class 1: (0.72, 0.86) | Class 2: (0.14, 0.28) | \n",
            "Sample 54: Class 1: (0.98, 1.00) | Class 2: (0.00, 0.02) | \n",
            "Sample 55: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 56: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 57: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 58: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 59: Class 1: (0.04, 0.09) | Class 2: (0.91, 0.96) | \n",
            "Sample 60: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 61: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 62: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 63: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 64: Class 1: (0.97, 0.99) | Class 2: (0.01, 0.03) | \n",
            "Sample 65: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 66: Class 1: (0.94, 0.96) | Class 2: (0.04, 0.06) | \n",
            "Sample 67: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 68: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 69: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 70: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 71: Class 1: (0.00, 0.01) | Class 2: (0.99, 1.00) | \n",
            "Sample 72: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 73: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 74: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 75: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 76: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 77: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 78: Class 1: (0.32, 0.44) | Class 2: (0.56, 0.68) | \n",
            "Sample 79: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 80: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 81: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 82: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 83: Class 1: (0.09, 0.19) | Class 2: (0.81, 0.91) | \n",
            "Sample 84: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 85: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 86: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 87: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 88: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 89: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 90: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 91: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 92: Class 1: (0.37, 0.62) | Class 2: (0.38, 0.63) | \n",
            "Sample 93: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 94: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 95: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 96: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 97: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 98: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 99: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 100: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 101: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 102: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 103: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 104: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 105: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 106: Class 1: (0.99, 0.99) | Class 2: (0.01, 0.01) | \n",
            "Sample 107: Class 1: (0.99, 1.00) | Class 2: (0.00, 0.01) | \n",
            "Sample 108: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 109: Class 1: (0.93, 0.96) | Class 2: (0.04, 0.07) | \n",
            "Sample 110: Class 1: (0.98, 0.99) | Class 2: (0.01, 0.02) | \n",
            "Sample 111: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "Sample 112: Class 1: (1.00, 1.00) | Class 2: (0.00, 0.00) | \n",
            "Sample 113: Class 1: (0.61, 0.78) | Class 2: (0.22, 0.39) | \n",
            "Sample 114: Class 1: (0.00, 0.00) | Class 2: (1.00, 1.00) | \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate min and max probabilities\n",
        "for lr_idx in range(5):\n",
        "    print(f\"Learning rate = {learning_rates[lr_idx]}\")\n",
        "    for sample_idx in range(X_test.shape[0]):\n",
        "        min_probs = np.min(class_probabilities[lr_idx, :, sample_idx, :], axis=0)\n",
        "        max_probs = np.max(class_probabilities[lr_idx, :, sample_idx, :], axis=0)\n",
        "\n",
        "        print(f\"Sample {sample_idx + 1}: \", end=\"\")\n",
        "        for class_idx in range(output_size):\n",
        "            print(f\"Class {class_idx + 1}: ({min_probs[class_idx]:.2f}, {max_probs[class_idx]:.2f})\", end=\" | \")\n",
        "        print()  # Newline for each sample\n",
        "    print()  # Newline for each learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E9UGHS2GrHN",
        "outputId": "49bece82-e778-4847-f2d7-0b2e58ba6673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F and G values for Test Sample 1:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6092, 0.6092)\t(0.3908, 0.3908)\t\n",
            "0.0100\t(0.8231, 0.8231)\t(0.1769, 0.1769)\t\n",
            "0.0500\t(0.9209, 0.9209)\t(0.0791, 0.0791)\t\n",
            "0.1000\t(0.9459, 0.9459)\t(0.0541, 0.0541)\t\n",
            "0.2000\t(0.9543, 0.9543)\t(0.0457, 0.0457)\t\n",
            "\n",
            "F and G values for Test Sample 2:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4738, 0.4738)\t(0.5262, 0.5262)\t\n",
            "0.0100\t(0.1395, 0.1395)\t(0.8605, 0.8605)\t\n",
            "0.0500\t(0.0122, 0.0122)\t(0.9878, 0.9878)\t\n",
            "0.1000\t(0.0106, 0.0106)\t(0.9894, 0.9894)\t\n",
            "0.2000\t(0.0015, 0.0015)\t(0.9985, 0.9985)\t\n",
            "\n",
            "F and G values for Test Sample 3:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4460, 0.4460)\t(0.5540, 0.5540)\t\n",
            "0.0100\t(0.2524, 0.2524)\t(0.7476, 0.7476)\t\n",
            "0.0500\t(0.0266, 0.0266)\t(0.9734, 0.9734)\t\n",
            "0.1000\t(0.0162, 0.0162)\t(0.9838, 0.9838)\t\n",
            "0.2000\t(0.0028, 0.0028)\t(0.9972, 0.9972)\t\n",
            "\n",
            "F and G values for Test Sample 4:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6128, 0.6128)\t(0.3872, 0.3872)\t\n",
            "0.0100\t(0.9114, 0.9114)\t(0.0886, 0.0886)\t\n",
            "0.0500\t(0.9911, 0.9911)\t(0.0089, 0.0089)\t\n",
            "0.1000\t(0.9959, 0.9959)\t(0.0041, 0.0041)\t\n",
            "0.2000\t(0.9979, 0.9979)\t(0.0021, 0.0021)\t\n",
            "\n",
            "F and G values for Test Sample 5:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6693, 0.6693)\t(0.3307, 0.3307)\t\n",
            "0.0100\t(0.9335, 0.9335)\t(0.0665, 0.0665)\t\n",
            "0.0500\t(0.9939, 0.9939)\t(0.0061, 0.0061)\t\n",
            "0.1000\t(0.9972, 0.9972)\t(0.0028, 0.0028)\t\n",
            "0.2000\t(0.9983, 0.9983)\t(0.0017, 0.0017)\t\n",
            "\n",
            "F and G values for Test Sample 6:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3278, 0.3278)\t(0.6722, 0.6722)\t\n",
            "0.0100\t(0.1139, 0.1139)\t(0.8861, 0.8861)\t\n",
            "0.0500\t(0.0123, 0.0123)\t(0.9877, 0.9877)\t\n",
            "0.1000\t(0.0099, 0.0099)\t(0.9901, 0.9901)\t\n",
            "0.2000\t(0.0012, 0.0012)\t(0.9988, 0.9988)\t\n",
            "\n",
            "F and G values for Test Sample 7:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3645, 0.3645)\t(0.6355, 0.6355)\t\n",
            "0.0100\t(0.1138, 0.1138)\t(0.8862, 0.8862)\t\n",
            "0.0500\t(0.0116, 0.0116)\t(0.9884, 0.9884)\t\n",
            "0.1000\t(0.0099, 0.0099)\t(0.9901, 0.9901)\t\n",
            "0.2000\t(0.0013, 0.0013)\t(0.9987, 0.9987)\t\n",
            "\n",
            "F and G values for Test Sample 8:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5054, 0.5054)\t(0.4946, 0.4946)\t\n",
            "0.0100\t(0.2686, 0.2686)\t(0.7314, 0.7314)\t\n",
            "0.0500\t(0.0717, 0.0717)\t(0.9283, 0.9283)\t\n",
            "0.1000\t(0.0436, 0.0436)\t(0.9564, 0.9564)\t\n",
            "0.2000\t(0.0151, 0.0151)\t(0.9849, 0.9849)\t\n",
            "\n",
            "F and G values for Test Sample 9:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4654, 0.4654)\t(0.5346, 0.5346)\t\n",
            "0.0100\t(0.5309, 0.5309)\t(0.4691, 0.4691)\t\n",
            "0.0500\t(0.6312, 0.6312)\t(0.3688, 0.3688)\t\n",
            "0.1000\t(0.7144, 0.7144)\t(0.2856, 0.2856)\t\n",
            "0.2000\t(0.7362, 0.7362)\t(0.2638, 0.2638)\t\n",
            "\n",
            "F and G values for Test Sample 10:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6820, 0.6820)\t(0.3180, 0.3180)\t\n",
            "0.0100\t(0.9202, 0.9202)\t(0.0798, 0.0798)\t\n",
            "0.0500\t(0.9901, 0.9901)\t(0.0099, 0.0099)\t\n",
            "0.1000\t(0.9955, 0.9955)\t(0.0045, 0.0045)\t\n",
            "0.2000\t(0.9976, 0.9976)\t(0.0024, 0.0024)\t\n",
            "\n",
            "F and G values for Test Sample 11:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6698, 0.6698)\t(0.3302, 0.3302)\t\n",
            "0.0100\t(0.8787, 0.8787)\t(0.1213, 0.1213)\t\n",
            "0.0500\t(0.9445, 0.9445)\t(0.0555, 0.0555)\t\n",
            "0.1000\t(0.9641, 0.9641)\t(0.0359, 0.0359)\t\n",
            "0.2000\t(0.9743, 0.9743)\t(0.0257, 0.0257)\t\n",
            "\n",
            "F and G values for Test Sample 12:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5491, 0.5491)\t(0.4509, 0.4509)\t\n",
            "0.0100\t(0.2504, 0.2504)\t(0.7496, 0.7496)\t\n",
            "0.0500\t(0.0464, 0.0464)\t(0.9536, 0.9536)\t\n",
            "0.1000\t(0.0290, 0.0290)\t(0.9710, 0.9710)\t\n",
            "0.2000\t(0.0073, 0.0073)\t(0.9927, 0.9927)\t\n",
            "\n",
            "F and G values for Test Sample 13:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6822, 0.6822)\t(0.3178, 0.3178)\t\n",
            "0.0100\t(0.9032, 0.9032)\t(0.0968, 0.0968)\t\n",
            "0.0500\t(0.9784, 0.9784)\t(0.0216, 0.0216)\t\n",
            "0.1000\t(0.9899, 0.9899)\t(0.0101, 0.0101)\t\n",
            "0.2000\t(0.9950, 0.9950)\t(0.0050, 0.0050)\t\n",
            "\n",
            "F and G values for Test Sample 14:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4389, 0.4389)\t(0.5611, 0.5611)\t\n",
            "0.0100\t(0.3526, 0.3526)\t(0.6474, 0.6474)\t\n",
            "0.0500\t(0.1422, 0.1422)\t(0.8578, 0.8578)\t\n",
            "0.1000\t(0.1192, 0.1192)\t(0.8808, 0.8808)\t\n",
            "0.2000\t(0.0757, 0.0757)\t(0.9243, 0.9243)\t\n",
            "\n",
            "F and G values for Test Sample 15:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6758, 0.6758)\t(0.3242, 0.3242)\t\n",
            "0.0100\t(0.9213, 0.9213)\t(0.0787, 0.0787)\t\n",
            "0.0500\t(0.9906, 0.9906)\t(0.0094, 0.0094)\t\n",
            "0.1000\t(0.9952, 0.9952)\t(0.0048, 0.0048)\t\n",
            "0.2000\t(0.9973, 0.9973)\t(0.0027, 0.0027)\t\n",
            "\n",
            "F and G values for Test Sample 16:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3465, 0.3465)\t(0.6535, 0.6535)\t\n",
            "0.0100\t(0.1372, 0.1372)\t(0.8628, 0.8628)\t\n",
            "0.0500\t(0.0161, 0.0161)\t(0.9839, 0.9839)\t\n",
            "0.1000\t(0.0126, 0.0126)\t(0.9874, 0.9874)\t\n",
            "0.2000\t(0.0048, 0.0048)\t(0.9952, 0.9952)\t\n",
            "\n",
            "F and G values for Test Sample 17:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6811, 0.6811)\t(0.3189, 0.3189)\t\n",
            "0.0100\t(0.9209, 0.9209)\t(0.0791, 0.0791)\t\n",
            "0.0500\t(0.9887, 0.9887)\t(0.0113, 0.0113)\t\n",
            "0.1000\t(0.9946, 0.9946)\t(0.0054, 0.0054)\t\n",
            "0.2000\t(0.9971, 0.9971)\t(0.0029, 0.0029)\t\n",
            "\n",
            "F and G values for Test Sample 18:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7567, 0.7567)\t(0.2433, 0.2433)\t\n",
            "0.0100\t(0.9481, 0.9481)\t(0.0519, 0.0519)\t\n",
            "0.0500\t(0.9945, 0.9945)\t(0.0055, 0.0055)\t\n",
            "0.1000\t(0.9975, 0.9975)\t(0.0025, 0.0025)\t\n",
            "0.2000\t(0.9984, 0.9984)\t(0.0016, 0.0016)\t\n",
            "\n",
            "F and G values for Test Sample 19:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7323, 0.7323)\t(0.2677, 0.2677)\t\n",
            "0.0100\t(0.9516, 0.9516)\t(0.0484, 0.0484)\t\n",
            "0.0500\t(0.9951, 0.9951)\t(0.0049, 0.0049)\t\n",
            "0.1000\t(0.9976, 0.9976)\t(0.0024, 0.0024)\t\n",
            "0.2000\t(0.9985, 0.9985)\t(0.0015, 0.0015)\t\n",
            "\n",
            "F and G values for Test Sample 20:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3671, 0.3671)\t(0.6329, 0.6329)\t\n",
            "0.0100\t(0.1191, 0.1191)\t(0.8809, 0.8809)\t\n",
            "0.0500\t(0.0122, 0.0122)\t(0.9878, 0.9878)\t\n",
            "0.1000\t(0.0101, 0.0101)\t(0.9899, 0.9899)\t\n",
            "0.2000\t(0.0013, 0.0013)\t(0.9987, 0.9987)\t\n",
            "\n",
            "F and G values for Test Sample 21:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5916, 0.5916)\t(0.4084, 0.4084)\t\n",
            "0.0100\t(0.7451, 0.7451)\t(0.2549, 0.2549)\t\n",
            "0.0500\t(0.8878, 0.8878)\t(0.1122, 0.1122)\t\n",
            "0.1000\t(0.9378, 0.9378)\t(0.0622, 0.0622)\t\n",
            "0.2000\t(0.9695, 0.9695)\t(0.0305, 0.0305)\t\n",
            "\n",
            "F and G values for Test Sample 22:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6688, 0.6688)\t(0.3312, 0.3312)\t\n",
            "0.0100\t(0.8975, 0.8975)\t(0.1025, 0.1025)\t\n",
            "0.0500\t(0.9796, 0.9796)\t(0.0204, 0.0204)\t\n",
            "0.1000\t(0.9898, 0.9898)\t(0.0102, 0.0102)\t\n",
            "0.2000\t(0.9941, 0.9941)\t(0.0059, 0.0059)\t\n",
            "\n",
            "F and G values for Test Sample 23:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3351, 0.3351)\t(0.6649, 0.6649)\t\n",
            "0.0100\t(0.1143, 0.1143)\t(0.8857, 0.8857)\t\n",
            "0.0500\t(0.0120, 0.0120)\t(0.9880, 0.9880)\t\n",
            "0.1000\t(0.0099, 0.0099)\t(0.9901, 0.9901)\t\n",
            "0.2000\t(0.0013, 0.0013)\t(0.9987, 0.9987)\t\n",
            "\n",
            "F and G values for Test Sample 24:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7314, 0.7314)\t(0.2686, 0.2686)\t\n",
            "0.0100\t(0.9458, 0.9458)\t(0.0542, 0.0542)\t\n",
            "0.0500\t(0.9944, 0.9944)\t(0.0056, 0.0056)\t\n",
            "0.1000\t(0.9971, 0.9971)\t(0.0029, 0.0029)\t\n",
            "0.2000\t(0.9982, 0.9982)\t(0.0018, 0.0018)\t\n",
            "\n",
            "F and G values for Test Sample 25:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7062, 0.7062)\t(0.2938, 0.2938)\t\n",
            "0.0100\t(0.9169, 0.9169)\t(0.0831, 0.0831)\t\n",
            "0.0500\t(0.9898, 0.9898)\t(0.0102, 0.0102)\t\n",
            "0.1000\t(0.9959, 0.9959)\t(0.0041, 0.0041)\t\n",
            "0.2000\t(0.9974, 0.9974)\t(0.0026, 0.0026)\t\n",
            "\n",
            "F and G values for Test Sample 26:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5945, 0.5945)\t(0.4055, 0.4055)\t\n",
            "0.0100\t(0.8586, 0.8586)\t(0.1414, 0.1414)\t\n",
            "0.0500\t(0.9890, 0.9890)\t(0.0110, 0.0110)\t\n",
            "0.1000\t(0.9935, 0.9935)\t(0.0065, 0.0065)\t\n",
            "0.2000\t(0.9976, 0.9976)\t(0.0024, 0.0024)\t\n",
            "\n",
            "F and G values for Test Sample 27:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6705, 0.6705)\t(0.3295, 0.3295)\t\n",
            "0.0100\t(0.9089, 0.9089)\t(0.0911, 0.0911)\t\n",
            "0.0500\t(0.9903, 0.9903)\t(0.0097, 0.0097)\t\n",
            "0.1000\t(0.9952, 0.9952)\t(0.0048, 0.0048)\t\n",
            "0.2000\t(0.9974, 0.9974)\t(0.0026, 0.0026)\t\n",
            "\n",
            "F and G values for Test Sample 28:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6262, 0.6262)\t(0.3738, 0.3738)\t\n",
            "0.0100\t(0.8961, 0.8961)\t(0.1039, 0.1039)\t\n",
            "0.0500\t(0.9819, 0.9819)\t(0.0181, 0.0181)\t\n",
            "0.1000\t(0.9931, 0.9931)\t(0.0069, 0.0069)\t\n",
            "0.2000\t(0.9957, 0.9957)\t(0.0043, 0.0043)\t\n",
            "\n",
            "F and G values for Test Sample 29:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6924, 0.6924)\t(0.3076, 0.3076)\t\n",
            "0.0100\t(0.9162, 0.9162)\t(0.0838, 0.0838)\t\n",
            "0.0500\t(0.9873, 0.9873)\t(0.0127, 0.0127)\t\n",
            "0.1000\t(0.9942, 0.9942)\t(0.0058, 0.0058)\t\n",
            "0.2000\t(0.9967, 0.9967)\t(0.0033, 0.0033)\t\n",
            "\n",
            "F and G values for Test Sample 30:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3766, 0.3766)\t(0.6234, 0.6234)\t\n",
            "0.0100\t(0.1266, 0.1266)\t(0.8734, 0.8734)\t\n",
            "0.0500\t(0.0126, 0.0126)\t(0.9874, 0.9874)\t\n",
            "0.1000\t(0.0105, 0.0105)\t(0.9895, 0.9895)\t\n",
            "0.2000\t(0.0014, 0.0014)\t(0.9986, 0.9986)\t\n",
            "\n",
            "F and G values for Test Sample 31:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7016, 0.7016)\t(0.2984, 0.2984)\t\n",
            "0.0100\t(0.9209, 0.9209)\t(0.0791, 0.0791)\t\n",
            "0.0500\t(0.9918, 0.9918)\t(0.0082, 0.0082)\t\n",
            "0.1000\t(0.9962, 0.9962)\t(0.0038, 0.0038)\t\n",
            "0.2000\t(0.9978, 0.9978)\t(0.0022, 0.0022)\t\n",
            "\n",
            "F and G values for Test Sample 32:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7178, 0.7178)\t(0.2822, 0.2822)\t\n",
            "0.0100\t(0.9393, 0.9393)\t(0.0607, 0.0607)\t\n",
            "0.0500\t(0.9937, 0.9937)\t(0.0063, 0.0063)\t\n",
            "0.1000\t(0.9967, 0.9967)\t(0.0033, 0.0033)\t\n",
            "0.2000\t(0.9982, 0.9982)\t(0.0018, 0.0018)\t\n",
            "\n",
            "F and G values for Test Sample 33:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6615, 0.6615)\t(0.3385, 0.3385)\t\n",
            "0.0100\t(0.8956, 0.8956)\t(0.1044, 0.1044)\t\n",
            "0.0500\t(0.9899, 0.9899)\t(0.0101, 0.0101)\t\n",
            "0.1000\t(0.9945, 0.9945)\t(0.0055, 0.0055)\t\n",
            "0.2000\t(0.9966, 0.9966)\t(0.0034, 0.0034)\t\n",
            "\n",
            "F and G values for Test Sample 34:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6906, 0.6906)\t(0.3094, 0.3094)\t\n",
            "0.0100\t(0.9094, 0.9094)\t(0.0906, 0.0906)\t\n",
            "0.0500\t(0.9843, 0.9843)\t(0.0157, 0.0157)\t\n",
            "0.1000\t(0.9918, 0.9918)\t(0.0082, 0.0082)\t\n",
            "0.2000\t(0.9951, 0.9951)\t(0.0049, 0.0049)\t\n",
            "\n",
            "F and G values for Test Sample 35:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7063, 0.7063)\t(0.2937, 0.2937)\t\n",
            "0.0100\t(0.9420, 0.9420)\t(0.0580, 0.0580)\t\n",
            "0.0500\t(0.9940, 0.9940)\t(0.0060, 0.0060)\t\n",
            "0.1000\t(0.9970, 0.9970)\t(0.0030, 0.0030)\t\n",
            "0.2000\t(0.9982, 0.9982)\t(0.0018, 0.0018)\t\n",
            "\n",
            "F and G values for Test Sample 36:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6836, 0.6836)\t(0.3164, 0.3164)\t\n",
            "0.0100\t(0.9367, 0.9367)\t(0.0633, 0.0633)\t\n",
            "0.0500\t(0.9904, 0.9904)\t(0.0096, 0.0096)\t\n",
            "0.1000\t(0.9951, 0.9951)\t(0.0049, 0.0049)\t\n",
            "0.2000\t(0.9967, 0.9967)\t(0.0033, 0.0033)\t\n",
            "\n",
            "F and G values for Test Sample 37:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6081, 0.6081)\t(0.3919, 0.3919)\t\n",
            "0.0100\t(0.5220, 0.5220)\t(0.4780, 0.4780)\t\n",
            "0.0500\t(0.1550, 0.1550)\t(0.8450, 0.8450)\t\n",
            "0.1000\t(0.0744, 0.0744)\t(0.9256, 0.9256)\t\n",
            "0.2000\t(0.0230, 0.0230)\t(0.9770, 0.9770)\t\n",
            "\n",
            "F and G values for Test Sample 38:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6717, 0.6717)\t(0.3283, 0.3283)\t\n",
            "0.0100\t(0.9232, 0.9232)\t(0.0768, 0.0768)\t\n",
            "0.0500\t(0.9917, 0.9917)\t(0.0083, 0.0083)\t\n",
            "0.1000\t(0.9956, 0.9956)\t(0.0044, 0.0044)\t\n",
            "0.2000\t(0.9970, 0.9970)\t(0.0030, 0.0030)\t\n",
            "\n",
            "F and G values for Test Sample 39:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4720, 0.4720)\t(0.5280, 0.5280)\t\n",
            "0.0100\t(0.1522, 0.1522)\t(0.8478, 0.8478)\t\n",
            "0.0500\t(0.0138, 0.0138)\t(0.9862, 0.9862)\t\n",
            "0.1000\t(0.0112, 0.0112)\t(0.9888, 0.9888)\t\n",
            "0.2000\t(0.0016, 0.0016)\t(0.9984, 0.9984)\t\n",
            "\n",
            "F and G values for Test Sample 40:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6629, 0.6629)\t(0.3371, 0.3371)\t\n",
            "0.0100\t(0.8326, 0.8326)\t(0.1674, 0.1674)\t\n",
            "0.0500\t(0.9173, 0.9173)\t(0.0827, 0.0827)\t\n",
            "0.1000\t(0.9609, 0.9609)\t(0.0391, 0.0391)\t\n",
            "0.2000\t(0.9813, 0.9813)\t(0.0187, 0.0187)\t\n",
            "\n",
            "F and G values for Test Sample 41:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7092, 0.7092)\t(0.2908, 0.2908)\t\n",
            "0.0100\t(0.9409, 0.9409)\t(0.0591, 0.0591)\t\n",
            "0.0500\t(0.9937, 0.9937)\t(0.0063, 0.0063)\t\n",
            "0.1000\t(0.9971, 0.9971)\t(0.0029, 0.0029)\t\n",
            "0.2000\t(0.9982, 0.9982)\t(0.0018, 0.0018)\t\n",
            "\n",
            "F and G values for Test Sample 42:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4320, 0.4320)\t(0.5680, 0.5680)\t\n",
            "0.0100\t(0.2385, 0.2385)\t(0.7615, 0.7615)\t\n",
            "0.0500\t(0.0230, 0.0230)\t(0.9770, 0.9770)\t\n",
            "0.1000\t(0.0143, 0.0143)\t(0.9857, 0.9857)\t\n",
            "0.2000\t(0.0024, 0.0024)\t(0.9976, 0.9976)\t\n",
            "\n",
            "F and G values for Test Sample 43:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6606, 0.6606)\t(0.3394, 0.3394)\t\n",
            "0.0100\t(0.9041, 0.9041)\t(0.0959, 0.0959)\t\n",
            "0.0500\t(0.9878, 0.9878)\t(0.0122, 0.0122)\t\n",
            "0.1000\t(0.9938, 0.9938)\t(0.0062, 0.0062)\t\n",
            "0.2000\t(0.9968, 0.9968)\t(0.0032, 0.0032)\t\n",
            "\n",
            "F and G values for Test Sample 44:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7048, 0.7048)\t(0.2952, 0.2952)\t\n",
            "0.0100\t(0.9302, 0.9302)\t(0.0698, 0.0698)\t\n",
            "0.0500\t(0.9915, 0.9915)\t(0.0085, 0.0085)\t\n",
            "0.1000\t(0.9957, 0.9957)\t(0.0043, 0.0043)\t\n",
            "0.2000\t(0.9976, 0.9976)\t(0.0024, 0.0024)\t\n",
            "\n",
            "F and G values for Test Sample 45:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6235, 0.6235)\t(0.3765, 0.3765)\t\n",
            "0.0100\t(0.8887, 0.8887)\t(0.1113, 0.1113)\t\n",
            "0.0500\t(0.9676, 0.9676)\t(0.0324, 0.0324)\t\n",
            "0.1000\t(0.9798, 0.9798)\t(0.0202, 0.0202)\t\n",
            "0.2000\t(0.9878, 0.9878)\t(0.0122, 0.0122)\t\n",
            "\n",
            "F and G values for Test Sample 46:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6404, 0.6404)\t(0.3596, 0.3596)\t\n",
            "0.0100\t(0.8748, 0.8748)\t(0.1252, 0.1252)\t\n",
            "0.0500\t(0.9571, 0.9571)\t(0.0429, 0.0429)\t\n",
            "0.1000\t(0.9670, 0.9670)\t(0.0330, 0.0330)\t\n",
            "0.2000\t(0.9516, 0.9516)\t(0.0484, 0.0484)\t\n",
            "\n",
            "F and G values for Test Sample 47:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5858, 0.5858)\t(0.4142, 0.4142)\t\n",
            "0.0100\t(0.9214, 0.9214)\t(0.0786, 0.0786)\t\n",
            "0.0500\t(0.9919, 0.9919)\t(0.0081, 0.0081)\t\n",
            "0.1000\t(0.9898, 0.9898)\t(0.0102, 0.0102)\t\n",
            "0.2000\t(0.9976, 0.9976)\t(0.0024, 0.0024)\t\n",
            "\n",
            "F and G values for Test Sample 48:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7479, 0.7479)\t(0.2521, 0.2521)\t\n",
            "0.0100\t(0.9275, 0.9275)\t(0.0725, 0.0725)\t\n",
            "0.0500\t(0.9905, 0.9905)\t(0.0095, 0.0095)\t\n",
            "0.1000\t(0.9964, 0.9964)\t(0.0036, 0.0036)\t\n",
            "0.2000\t(0.9975, 0.9975)\t(0.0025, 0.0025)\t\n",
            "\n",
            "F and G values for Test Sample 49:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6137, 0.6137)\t(0.3863, 0.3863)\t\n",
            "0.0100\t(0.8678, 0.8678)\t(0.1322, 0.1322)\t\n",
            "0.0500\t(0.9619, 0.9619)\t(0.0381, 0.0381)\t\n",
            "0.1000\t(0.9788, 0.9788)\t(0.0212, 0.0212)\t\n",
            "0.2000\t(0.9803, 0.9803)\t(0.0197, 0.0197)\t\n",
            "\n",
            "F and G values for Test Sample 50:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6596, 0.6596)\t(0.3404, 0.3404)\t\n",
            "0.0100\t(0.9015, 0.9015)\t(0.0985, 0.0985)\t\n",
            "0.0500\t(0.9889, 0.9889)\t(0.0111, 0.0111)\t\n",
            "0.1000\t(0.9954, 0.9954)\t(0.0046, 0.0046)\t\n",
            "0.2000\t(0.9976, 0.9976)\t(0.0024, 0.0024)\t\n",
            "\n",
            "F and G values for Test Sample 51:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4926, 0.4926)\t(0.5074, 0.5074)\t\n",
            "0.0100\t(0.1712, 0.1712)\t(0.8288, 0.8288)\t\n",
            "0.0500\t(0.0146, 0.0146)\t(0.9854, 0.9854)\t\n",
            "0.1000\t(0.0114, 0.0114)\t(0.9886, 0.9886)\t\n",
            "0.2000\t(0.0016, 0.0016)\t(0.9984, 0.9984)\t\n",
            "\n",
            "F and G values for Test Sample 52:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3563, 0.3563)\t(0.6437, 0.6437)\t\n",
            "0.0100\t(0.1220, 0.1220)\t(0.8780, 0.8780)\t\n",
            "0.0500\t(0.0121, 0.0121)\t(0.9879, 0.9879)\t\n",
            "0.1000\t(0.0100, 0.0100)\t(0.9900, 0.9900)\t\n",
            "0.2000\t(0.0013, 0.0013)\t(0.9987, 0.9987)\t\n",
            "\n",
            "F and G values for Test Sample 53:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5700, 0.5700)\t(0.4300, 0.4300)\t\n",
            "0.0100\t(0.5501, 0.5501)\t(0.4499, 0.4499)\t\n",
            "0.0500\t(0.6311, 0.6311)\t(0.3689, 0.3689)\t\n",
            "0.1000\t(0.6802, 0.6802)\t(0.3198, 0.3198)\t\n",
            "0.2000\t(0.7899, 0.7899)\t(0.2101, 0.2101)\t\n",
            "\n",
            "F and G values for Test Sample 54:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5570, 0.5570)\t(0.4430, 0.4430)\t\n",
            "0.0100\t(0.7612, 0.7612)\t(0.2388, 0.2388)\t\n",
            "0.0500\t(0.9725, 0.9725)\t(0.0275, 0.0275)\t\n",
            "0.1000\t(0.9793, 0.9793)\t(0.0207, 0.0207)\t\n",
            "0.2000\t(0.9861, 0.9861)\t(0.0139, 0.0139)\t\n",
            "\n",
            "F and G values for Test Sample 55:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6180, 0.6180)\t(0.3820, 0.3820)\t\n",
            "0.0100\t(0.9108, 0.9108)\t(0.0892, 0.0892)\t\n",
            "0.0500\t(0.9934, 0.9934)\t(0.0066, 0.0066)\t\n",
            "0.1000\t(0.9970, 0.9970)\t(0.0030, 0.0030)\t\n",
            "0.2000\t(0.9983, 0.9983)\t(0.0017, 0.0017)\t\n",
            "\n",
            "F and G values for Test Sample 56:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6905, 0.6905)\t(0.3095, 0.3095)\t\n",
            "0.0100\t(0.8858, 0.8858)\t(0.1142, 0.1142)\t\n",
            "0.0500\t(0.9808, 0.9808)\t(0.0192, 0.0192)\t\n",
            "0.1000\t(0.9915, 0.9915)\t(0.0085, 0.0085)\t\n",
            "0.2000\t(0.9939, 0.9939)\t(0.0061, 0.0061)\t\n",
            "\n",
            "F and G values for Test Sample 57:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6821, 0.6821)\t(0.3179, 0.3179)\t\n",
            "0.0100\t(0.9307, 0.9307)\t(0.0693, 0.0693)\t\n",
            "0.0500\t(0.9939, 0.9939)\t(0.0061, 0.0061)\t\n",
            "0.1000\t(0.9971, 0.9971)\t(0.0029, 0.0029)\t\n",
            "0.2000\t(0.9983, 0.9983)\t(0.0017, 0.0017)\t\n",
            "\n",
            "F and G values for Test Sample 58:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3230, 0.3230)\t(0.6770, 0.6770)\t\n",
            "0.0100\t(0.1117, 0.1117)\t(0.8883, 0.8883)\t\n",
            "0.0500\t(0.0115, 0.0115)\t(0.9885, 0.9885)\t\n",
            "0.1000\t(0.0099, 0.0099)\t(0.9901, 0.9901)\t\n",
            "0.2000\t(0.0012, 0.0012)\t(0.9988, 0.9988)\t\n",
            "\n",
            "F and G values for Test Sample 59:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5233, 0.5233)\t(0.4767, 0.4767)\t\n",
            "0.0100\t(0.3799, 0.3799)\t(0.6201, 0.6201)\t\n",
            "0.0500\t(0.2118, 0.2118)\t(0.7882, 0.7882)\t\n",
            "0.1000\t(0.1037, 0.1037)\t(0.8963, 0.8963)\t\n",
            "0.2000\t(0.0630, 0.0630)\t(0.9370, 0.9370)\t\n",
            "\n",
            "F and G values for Test Sample 60:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7079, 0.7079)\t(0.2921, 0.2921)\t\n",
            "0.0100\t(0.9360, 0.9360)\t(0.0640, 0.0640)\t\n",
            "0.0500\t(0.9938, 0.9938)\t(0.0062, 0.0062)\t\n",
            "0.1000\t(0.9968, 0.9968)\t(0.0032, 0.0032)\t\n",
            "0.2000\t(0.9982, 0.9982)\t(0.0018, 0.0018)\t\n",
            "\n",
            "F and G values for Test Sample 61:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6779, 0.6779)\t(0.3221, 0.3221)\t\n",
            "0.0100\t(0.9086, 0.9086)\t(0.0914, 0.0914)\t\n",
            "0.0500\t(0.9836, 0.9836)\t(0.0164, 0.0164)\t\n",
            "0.1000\t(0.9924, 0.9924)\t(0.0076, 0.0076)\t\n",
            "0.2000\t(0.9959, 0.9959)\t(0.0041, 0.0041)\t\n",
            "\n",
            "F and G values for Test Sample 62:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3590, 0.3590)\t(0.6410, 0.6410)\t\n",
            "0.0100\t(0.1163, 0.1163)\t(0.8837, 0.8837)\t\n",
            "0.0500\t(0.0119, 0.0119)\t(0.9881, 0.9881)\t\n",
            "0.1000\t(0.0100, 0.0100)\t(0.9900, 0.9900)\t\n",
            "0.2000\t(0.0014, 0.0014)\t(0.9986, 0.9986)\t\n",
            "\n",
            "F and G values for Test Sample 63:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3602, 0.3602)\t(0.6398, 0.6398)\t\n",
            "0.0100\t(0.1168, 0.1168)\t(0.8832, 0.8832)\t\n",
            "0.0500\t(0.0117, 0.0117)\t(0.9883, 0.9883)\t\n",
            "0.1000\t(0.0100, 0.0100)\t(0.9900, 0.9900)\t\n",
            "0.2000\t(0.0013, 0.0013)\t(0.9987, 0.9987)\t\n",
            "\n",
            "F and G values for Test Sample 64:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6762, 0.6762)\t(0.3238, 0.3238)\t\n",
            "0.0100\t(0.8740, 0.8740)\t(0.1260, 0.1260)\t\n",
            "0.0500\t(0.9196, 0.9196)\t(0.0804, 0.0804)\t\n",
            "0.1000\t(0.9684, 0.9684)\t(0.0316, 0.0316)\t\n",
            "0.2000\t(0.9810, 0.9810)\t(0.0190, 0.0190)\t\n",
            "\n",
            "F and G values for Test Sample 65:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7093, 0.7093)\t(0.2907, 0.2907)\t\n",
            "0.0100\t(0.9379, 0.9379)\t(0.0621, 0.0621)\t\n",
            "0.0500\t(0.9921, 0.9921)\t(0.0079, 0.0079)\t\n",
            "0.1000\t(0.9963, 0.9963)\t(0.0037, 0.0037)\t\n",
            "0.2000\t(0.9976, 0.9976)\t(0.0024, 0.0024)\t\n",
            "\n",
            "F and G values for Test Sample 66:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6176, 0.6176)\t(0.3824, 0.3824)\t\n",
            "0.0100\t(0.8133, 0.8133)\t(0.1867, 0.1867)\t\n",
            "0.0500\t(0.9055, 0.9055)\t(0.0945, 0.0945)\t\n",
            "0.1000\t(0.9368, 0.9368)\t(0.0632, 0.0632)\t\n",
            "0.2000\t(0.9508, 0.9508)\t(0.0492, 0.0492)\t\n",
            "\n",
            "F and G values for Test Sample 67:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3607, 0.3607)\t(0.6393, 0.6393)\t\n",
            "0.0100\t(0.1321, 0.1321)\t(0.8679, 0.8679)\t\n",
            "0.0500\t(0.0141, 0.0141)\t(0.9859, 0.9859)\t\n",
            "0.1000\t(0.0106, 0.0106)\t(0.9894, 0.9894)\t\n",
            "0.2000\t(0.0015, 0.0015)\t(0.9985, 0.9985)\t\n",
            "\n",
            "F and G values for Test Sample 68:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3080, 0.3080)\t(0.6920, 0.6920)\t\n",
            "0.0100\t(0.1242, 0.1242)\t(0.8758, 0.8758)\t\n",
            "0.0500\t(0.0154, 0.0154)\t(0.9846, 0.9846)\t\n",
            "0.1000\t(0.0099, 0.0099)\t(0.9901, 0.9901)\t\n",
            "0.2000\t(0.0015, 0.0015)\t(0.9985, 0.9985)\t\n",
            "\n",
            "F and G values for Test Sample 69:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7049, 0.7049)\t(0.2951, 0.2951)\t\n",
            "0.0100\t(0.9343, 0.9343)\t(0.0657, 0.0657)\t\n",
            "0.0500\t(0.9913, 0.9913)\t(0.0087, 0.0087)\t\n",
            "0.1000\t(0.9958, 0.9958)\t(0.0042, 0.0042)\t\n",
            "0.2000\t(0.9975, 0.9975)\t(0.0025, 0.0025)\t\n",
            "\n",
            "F and G values for Test Sample 70:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5983, 0.5983)\t(0.4017, 0.4017)\t\n",
            "0.0100\t(0.8289, 0.8289)\t(0.1711, 0.1711)\t\n",
            "0.0500\t(0.9640, 0.9640)\t(0.0360, 0.0360)\t\n",
            "0.1000\t(0.9826, 0.9826)\t(0.0174, 0.0174)\t\n",
            "0.2000\t(0.9896, 0.9896)\t(0.0104, 0.0104)\t\n",
            "\n",
            "F and G values for Test Sample 71:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4282, 0.4282)\t(0.5718, 0.5718)\t\n",
            "0.0100\t(0.2482, 0.2482)\t(0.7518, 0.7518)\t\n",
            "0.0500\t(0.0287, 0.0287)\t(0.9713, 0.9713)\t\n",
            "0.1000\t(0.0190, 0.0190)\t(0.9810, 0.9810)\t\n",
            "0.2000\t(0.0036, 0.0036)\t(0.9964, 0.9964)\t\n",
            "\n",
            "F and G values for Test Sample 72:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4231, 0.4231)\t(0.5769, 0.5769)\t\n",
            "0.0100\t(0.1799, 0.1799)\t(0.8201, 0.8201)\t\n",
            "0.0500\t(0.0213, 0.0213)\t(0.9787, 0.9787)\t\n",
            "0.1000\t(0.0158, 0.0158)\t(0.9842, 0.9842)\t\n",
            "0.2000\t(0.0026, 0.0026)\t(0.9974, 0.9974)\t\n",
            "\n",
            "F and G values for Test Sample 73:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6974, 0.6974)\t(0.3026, 0.3026)\t\n",
            "0.0100\t(0.9077, 0.9077)\t(0.0923, 0.0923)\t\n",
            "0.0500\t(0.9852, 0.9852)\t(0.0148, 0.0148)\t\n",
            "0.1000\t(0.9949, 0.9949)\t(0.0051, 0.0051)\t\n",
            "0.2000\t(0.9971, 0.9971)\t(0.0029, 0.0029)\t\n",
            "\n",
            "F and G values for Test Sample 74:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4109, 0.4109)\t(0.5891, 0.5891)\t\n",
            "0.0100\t(0.1696, 0.1696)\t(0.8304, 0.8304)\t\n",
            "0.0500\t(0.0229, 0.0229)\t(0.9771, 0.9771)\t\n",
            "0.1000\t(0.0171, 0.0171)\t(0.9829, 0.9829)\t\n",
            "0.2000\t(0.0039, 0.0039)\t(0.9961, 0.9961)\t\n",
            "\n",
            "F and G values for Test Sample 75:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7260, 0.7260)\t(0.2740, 0.2740)\t\n",
            "0.0100\t(0.9405, 0.9405)\t(0.0595, 0.0595)\t\n",
            "0.0500\t(0.9943, 0.9943)\t(0.0057, 0.0057)\t\n",
            "0.1000\t(0.9973, 0.9973)\t(0.0027, 0.0027)\t\n",
            "0.2000\t(0.9982, 0.9982)\t(0.0018, 0.0018)\t\n",
            "\n",
            "F and G values for Test Sample 76:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6267, 0.6267)\t(0.3733, 0.3733)\t\n",
            "0.0100\t(0.8581, 0.8581)\t(0.1419, 0.1419)\t\n",
            "0.0500\t(0.9785, 0.9785)\t(0.0215, 0.0215)\t\n",
            "0.1000\t(0.9885, 0.9885)\t(0.0115, 0.0115)\t\n",
            "0.2000\t(0.9941, 0.9941)\t(0.0059, 0.0059)\t\n",
            "\n",
            "F and G values for Test Sample 77:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6940, 0.6940)\t(0.3060, 0.3060)\t\n",
            "0.0100\t(0.8861, 0.8861)\t(0.1139, 0.1139)\t\n",
            "0.0500\t(0.9638, 0.9638)\t(0.0362, 0.0362)\t\n",
            "0.1000\t(0.9842, 0.9842)\t(0.0158, 0.0158)\t\n",
            "0.2000\t(0.9914, 0.9914)\t(0.0086, 0.0086)\t\n",
            "\n",
            "F and G values for Test Sample 78:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5514, 0.5514)\t(0.4486, 0.4486)\t\n",
            "0.0100\t(0.5654, 0.5654)\t(0.4346, 0.4346)\t\n",
            "0.0500\t(0.4723, 0.4723)\t(0.5277, 0.5277)\t\n",
            "0.1000\t(0.4202, 0.4202)\t(0.5798, 0.5798)\t\n",
            "0.2000\t(0.3810, 0.3810)\t(0.6190, 0.6190)\t\n",
            "\n",
            "F and G values for Test Sample 79:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7288, 0.7288)\t(0.2712, 0.2712)\t\n",
            "0.0100\t(0.9466, 0.9466)\t(0.0534, 0.0534)\t\n",
            "0.0500\t(0.9943, 0.9943)\t(0.0057, 0.0057)\t\n",
            "0.1000\t(0.9972, 0.9972)\t(0.0028, 0.0028)\t\n",
            "0.2000\t(0.9984, 0.9984)\t(0.0016, 0.0016)\t\n",
            "\n",
            "F and G values for Test Sample 80:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5899, 0.5899)\t(0.4101, 0.4101)\t\n",
            "0.0100\t(0.8593, 0.8593)\t(0.1407, 0.1407)\t\n",
            "0.0500\t(0.9809, 0.9809)\t(0.0191, 0.0191)\t\n",
            "0.1000\t(0.9884, 0.9884)\t(0.0116, 0.0116)\t\n",
            "0.2000\t(0.9938, 0.9938)\t(0.0062, 0.0062)\t\n",
            "\n",
            "F and G values for Test Sample 81:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4553, 0.4553)\t(0.5447, 0.5447)\t\n",
            "0.0100\t(0.2225, 0.2225)\t(0.7775, 0.7775)\t\n",
            "0.0500\t(0.0219, 0.0219)\t(0.9781, 0.9781)\t\n",
            "0.1000\t(0.0161, 0.0161)\t(0.9839, 0.9839)\t\n",
            "0.2000\t(0.0030, 0.0030)\t(0.9970, 0.9970)\t\n",
            "\n",
            "F and G values for Test Sample 82:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7172, 0.7172)\t(0.2828, 0.2828)\t\n",
            "0.0100\t(0.9473, 0.9473)\t(0.0527, 0.0527)\t\n",
            "0.0500\t(0.9943, 0.9943)\t(0.0057, 0.0057)\t\n",
            "0.1000\t(0.9971, 0.9971)\t(0.0029, 0.0029)\t\n",
            "0.2000\t(0.9983, 0.9983)\t(0.0017, 0.0017)\t\n",
            "\n",
            "F and G values for Test Sample 83:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5344, 0.5344)\t(0.4656, 0.4656)\t\n",
            "0.0100\t(0.4956, 0.4956)\t(0.5044, 0.5044)\t\n",
            "0.0500\t(0.2646, 0.2646)\t(0.7354, 0.7354)\t\n",
            "0.1000\t(0.1827, 0.1827)\t(0.8173, 0.8173)\t\n",
            "0.2000\t(0.1390, 0.1390)\t(0.8610, 0.8610)\t\n",
            "\n",
            "F and G values for Test Sample 84:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3576, 0.3576)\t(0.6424, 0.6424)\t\n",
            "0.0100\t(0.1166, 0.1166)\t(0.8834, 0.8834)\t\n",
            "0.0500\t(0.0121, 0.0121)\t(0.9879, 0.9879)\t\n",
            "0.1000\t(0.0099, 0.0099)\t(0.9901, 0.9901)\t\n",
            "0.2000\t(0.0013, 0.0013)\t(0.9987, 0.9987)\t\n",
            "\n",
            "F and G values for Test Sample 85:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5384, 0.5384)\t(0.4616, 0.4616)\t\n",
            "0.0100\t(0.3079, 0.3079)\t(0.6921, 0.6921)\t\n",
            "0.0500\t(0.0279, 0.0279)\t(0.9721, 0.9721)\t\n",
            "0.1000\t(0.0178, 0.0178)\t(0.9822, 0.9822)\t\n",
            "0.2000\t(0.0034, 0.0034)\t(0.9966, 0.9966)\t\n",
            "\n",
            "F and G values for Test Sample 86:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3396, 0.3396)\t(0.6604, 0.6604)\t\n",
            "0.0100\t(0.1503, 0.1503)\t(0.8497, 0.8497)\t\n",
            "0.0500\t(0.0240, 0.0240)\t(0.9760, 0.9760)\t\n",
            "0.1000\t(0.0141, 0.0141)\t(0.9859, 0.9859)\t\n",
            "0.2000\t(0.0032, 0.0032)\t(0.9968, 0.9968)\t\n",
            "\n",
            "F and G values for Test Sample 87:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3557, 0.3557)\t(0.6443, 0.6443)\t\n",
            "0.0100\t(0.1645, 0.1645)\t(0.8355, 0.8355)\t\n",
            "0.0500\t(0.0192, 0.0192)\t(0.9808, 0.9808)\t\n",
            "0.1000\t(0.0141, 0.0141)\t(0.9859, 0.9859)\t\n",
            "0.2000\t(0.0029, 0.0029)\t(0.9971, 0.9971)\t\n",
            "\n",
            "F and G values for Test Sample 88:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3711, 0.3711)\t(0.6289, 0.6289)\t\n",
            "0.0100\t(0.1431, 0.1431)\t(0.8569, 0.8569)\t\n",
            "0.0500\t(0.0162, 0.0162)\t(0.9838, 0.9838)\t\n",
            "0.1000\t(0.0131, 0.0131)\t(0.9869, 0.9869)\t\n",
            "0.2000\t(0.0026, 0.0026)\t(0.9974, 0.9974)\t\n",
            "\n",
            "F and G values for Test Sample 89:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6802, 0.6802)\t(0.3198, 0.3198)\t\n",
            "0.0100\t(0.9027, 0.9027)\t(0.0973, 0.0973)\t\n",
            "0.0500\t(0.9892, 0.9892)\t(0.0108, 0.0108)\t\n",
            "0.1000\t(0.9959, 0.9959)\t(0.0041, 0.0041)\t\n",
            "0.2000\t(0.9972, 0.9972)\t(0.0028, 0.0028)\t\n",
            "\n",
            "F and G values for Test Sample 90:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6797, 0.6797)\t(0.3203, 0.3203)\t\n",
            "0.0100\t(0.8961, 0.8961)\t(0.1039, 0.1039)\t\n",
            "0.0500\t(0.9877, 0.9877)\t(0.0123, 0.0123)\t\n",
            "0.1000\t(0.9927, 0.9927)\t(0.0073, 0.0073)\t\n",
            "0.2000\t(0.9966, 0.9966)\t(0.0034, 0.0034)\t\n",
            "\n",
            "F and G values for Test Sample 91:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6390, 0.6390)\t(0.3610, 0.3610)\t\n",
            "0.0100\t(0.9126, 0.9126)\t(0.0874, 0.0874)\t\n",
            "0.0500\t(0.9861, 0.9861)\t(0.0139, 0.0139)\t\n",
            "0.1000\t(0.9930, 0.9930)\t(0.0070, 0.0070)\t\n",
            "0.2000\t(0.9959, 0.9959)\t(0.0041, 0.0041)\t\n",
            "\n",
            "F and G values for Test Sample 92:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6145, 0.6145)\t(0.3855, 0.3855)\t\n",
            "0.0100\t(0.6897, 0.6897)\t(0.3103, 0.3103)\t\n",
            "0.0500\t(0.6489, 0.6489)\t(0.3511, 0.3511)\t\n",
            "0.1000\t(0.5944, 0.5944)\t(0.4056, 0.4056)\t\n",
            "0.2000\t(0.4956, 0.4956)\t(0.5044, 0.5044)\t\n",
            "\n",
            "F and G values for Test Sample 93:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6380, 0.6380)\t(0.3620, 0.3620)\t\n",
            "0.0100\t(0.7762, 0.7762)\t(0.2238, 0.2238)\t\n",
            "0.0500\t(0.9308, 0.9308)\t(0.0692, 0.0692)\t\n",
            "0.1000\t(0.9783, 0.9783)\t(0.0217, 0.0217)\t\n",
            "0.2000\t(0.9903, 0.9903)\t(0.0097, 0.0097)\t\n",
            "\n",
            "F and G values for Test Sample 94:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7458, 0.7458)\t(0.2542, 0.2542)\t\n",
            "0.0100\t(0.9173, 0.9173)\t(0.0827, 0.0827)\t\n",
            "0.0500\t(0.9831, 0.9831)\t(0.0169, 0.0169)\t\n",
            "0.1000\t(0.9956, 0.9956)\t(0.0044, 0.0044)\t\n",
            "0.2000\t(0.9974, 0.9974)\t(0.0026, 0.0026)\t\n",
            "\n",
            "F and G values for Test Sample 95:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6813, 0.6813)\t(0.3187, 0.3187)\t\n",
            "0.0100\t(0.9344, 0.9344)\t(0.0656, 0.0656)\t\n",
            "0.0500\t(0.9902, 0.9902)\t(0.0098, 0.0098)\t\n",
            "0.1000\t(0.9963, 0.9963)\t(0.0037, 0.0037)\t\n",
            "0.2000\t(0.9975, 0.9975)\t(0.0025, 0.0025)\t\n",
            "\n",
            "F and G values for Test Sample 96:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6942, 0.6942)\t(0.3058, 0.3058)\t\n",
            "0.0100\t(0.9426, 0.9426)\t(0.0574, 0.0574)\t\n",
            "0.0500\t(0.9939, 0.9939)\t(0.0061, 0.0061)\t\n",
            "0.1000\t(0.9968, 0.9968)\t(0.0032, 0.0032)\t\n",
            "0.2000\t(0.9982, 0.9982)\t(0.0018, 0.0018)\t\n",
            "\n",
            "F and G values for Test Sample 97:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4069, 0.4069)\t(0.5931, 0.5931)\t\n",
            "0.0100\t(0.1299, 0.1299)\t(0.8701, 0.8701)\t\n",
            "0.0500\t(0.0123, 0.0123)\t(0.9877, 0.9877)\t\n",
            "0.1000\t(0.0102, 0.0102)\t(0.9898, 0.9898)\t\n",
            "0.2000\t(0.0013, 0.0013)\t(0.9987, 0.9987)\t\n",
            "\n",
            "F and G values for Test Sample 98:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3583, 0.3583)\t(0.6417, 0.6417)\t\n",
            "0.0100\t(0.1404, 0.1404)\t(0.8596, 0.8596)\t\n",
            "0.0500\t(0.0165, 0.0165)\t(0.9835, 0.9835)\t\n",
            "0.1000\t(0.0103, 0.0103)\t(0.9897, 0.9897)\t\n",
            "0.2000\t(0.0013, 0.0013)\t(0.9987, 0.9987)\t\n",
            "\n",
            "F and G values for Test Sample 99:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7084, 0.7084)\t(0.2916, 0.2916)\t\n",
            "0.0100\t(0.9437, 0.9437)\t(0.0563, 0.0563)\t\n",
            "0.0500\t(0.9944, 0.9944)\t(0.0056, 0.0056)\t\n",
            "0.1000\t(0.9971, 0.9971)\t(0.0029, 0.0029)\t\n",
            "0.2000\t(0.9982, 0.9982)\t(0.0018, 0.0018)\t\n",
            "\n",
            "F and G values for Test Sample 100:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4567, 0.4567)\t(0.5433, 0.5433)\t\n",
            "0.0100\t(0.1881, 0.1881)\t(0.8119, 0.8119)\t\n",
            "0.0500\t(0.0163, 0.0163)\t(0.9837, 0.9837)\t\n",
            "0.1000\t(0.0124, 0.0124)\t(0.9876, 0.9876)\t\n",
            "0.2000\t(0.0017, 0.0017)\t(0.9983, 0.9983)\t\n",
            "\n",
            "F and G values for Test Sample 101:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5420, 0.5420)\t(0.4580, 0.4580)\t\n",
            "0.0100\t(0.2811, 0.2811)\t(0.7189, 0.7189)\t\n",
            "0.0500\t(0.0304, 0.0304)\t(0.9696, 0.9696)\t\n",
            "0.1000\t(0.0174, 0.0174)\t(0.9826, 0.9826)\t\n",
            "0.2000\t(0.0031, 0.0031)\t(0.9969, 0.9969)\t\n",
            "\n",
            "F and G values for Test Sample 102:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7346, 0.7346)\t(0.2654, 0.2654)\t\n",
            "0.0100\t(0.9528, 0.9528)\t(0.0472, 0.0472)\t\n",
            "0.0500\t(0.9951, 0.9951)\t(0.0049, 0.0049)\t\n",
            "0.1000\t(0.9975, 0.9975)\t(0.0025, 0.0025)\t\n",
            "0.2000\t(0.9985, 0.9985)\t(0.0015, 0.0015)\t\n",
            "\n",
            "F and G values for Test Sample 103:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3698, 0.3698)\t(0.6302, 0.6302)\t\n",
            "0.0100\t(0.1337, 0.1337)\t(0.8663, 0.8663)\t\n",
            "0.0500\t(0.0128, 0.0128)\t(0.9872, 0.9872)\t\n",
            "0.1000\t(0.0106, 0.0106)\t(0.9894, 0.9894)\t\n",
            "0.2000\t(0.0018, 0.0018)\t(0.9982, 0.9982)\t\n",
            "\n",
            "F and G values for Test Sample 104:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4225, 0.4225)\t(0.5775, 0.5775)\t\n",
            "0.0100\t(0.1502, 0.1502)\t(0.8498, 0.8498)\t\n",
            "0.0500\t(0.0153, 0.0153)\t(0.9847, 0.9847)\t\n",
            "0.1000\t(0.0121, 0.0121)\t(0.9879, 0.9879)\t\n",
            "0.2000\t(0.0019, 0.0019)\t(0.9981, 0.9981)\t\n",
            "\n",
            "F and G values for Test Sample 105:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.7026, 0.7026)\t(0.2974, 0.2974)\t\n",
            "0.0100\t(0.9050, 0.9050)\t(0.0950, 0.0950)\t\n",
            "0.0500\t(0.9833, 0.9833)\t(0.0167, 0.0167)\t\n",
            "0.1000\t(0.9926, 0.9926)\t(0.0074, 0.0074)\t\n",
            "0.2000\t(0.9944, 0.9944)\t(0.0056, 0.0056)\t\n",
            "\n",
            "F and G values for Test Sample 106:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6859, 0.6859)\t(0.3141, 0.3141)\t\n",
            "0.0100\t(0.8854, 0.8854)\t(0.1146, 0.1146)\t\n",
            "0.0500\t(0.9696, 0.9696)\t(0.0304, 0.0304)\t\n",
            "0.1000\t(0.9822, 0.9822)\t(0.0178, 0.0178)\t\n",
            "0.2000\t(0.9883, 0.9883)\t(0.0117, 0.0117)\t\n",
            "\n",
            "F and G values for Test Sample 107:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6448, 0.6448)\t(0.3552, 0.3552)\t\n",
            "0.0100\t(0.8908, 0.8908)\t(0.1092, 0.1092)\t\n",
            "0.0500\t(0.9829, 0.9829)\t(0.0171, 0.0171)\t\n",
            "0.1000\t(0.9916, 0.9916)\t(0.0084, 0.0084)\t\n",
            "0.2000\t(0.9959, 0.9959)\t(0.0041, 0.0041)\t\n",
            "\n",
            "F and G values for Test Sample 108:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.3933, 0.3933)\t(0.6067, 0.6067)\t\n",
            "0.0100\t(0.1131, 0.1131)\t(0.8869, 0.8869)\t\n",
            "0.0500\t(0.0115, 0.0115)\t(0.9885, 0.9885)\t\n",
            "0.1000\t(0.0099, 0.0099)\t(0.9901, 0.9901)\t\n",
            "0.2000\t(0.0014, 0.0014)\t(0.9986, 0.9986)\t\n",
            "\n",
            "F and G values for Test Sample 109:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6022, 0.6022)\t(0.3978, 0.3978)\t\n",
            "0.0100\t(0.7164, 0.7164)\t(0.2836, 0.2836)\t\n",
            "0.0500\t(0.8862, 0.8862)\t(0.1138, 0.1138)\t\n",
            "0.1000\t(0.9241, 0.9241)\t(0.0759, 0.0759)\t\n",
            "0.2000\t(0.9440, 0.9440)\t(0.0560, 0.0560)\t\n",
            "\n",
            "F and G values for Test Sample 110:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6748, 0.6748)\t(0.3252, 0.3252)\t\n",
            "0.0100\t(0.8617, 0.8617)\t(0.1383, 0.1383)\t\n",
            "0.0500\t(0.9577, 0.9577)\t(0.0423, 0.0423)\t\n",
            "0.1000\t(0.9777, 0.9777)\t(0.0223, 0.0223)\t\n",
            "0.2000\t(0.9852, 0.9852)\t(0.0148, 0.0148)\t\n",
            "\n",
            "F and G values for Test Sample 111:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5238, 0.5238)\t(0.4762, 0.4762)\t\n",
            "0.0100\t(0.2265, 0.2265)\t(0.7735, 0.7735)\t\n",
            "0.0500\t(0.0210, 0.0210)\t(0.9790, 0.9790)\t\n",
            "0.1000\t(0.0153, 0.0153)\t(0.9847, 0.9847)\t\n",
            "0.2000\t(0.0030, 0.0030)\t(0.9970, 0.9970)\t\n",
            "\n",
            "F and G values for Test Sample 112:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.6814, 0.6814)\t(0.3186, 0.3186)\t\n",
            "0.0100\t(0.9223, 0.9223)\t(0.0777, 0.0777)\t\n",
            "0.0500\t(0.9913, 0.9913)\t(0.0087, 0.0087)\t\n",
            "0.1000\t(0.9955, 0.9955)\t(0.0045, 0.0045)\t\n",
            "0.2000\t(0.9975, 0.9975)\t(0.0025, 0.0025)\t\n",
            "\n",
            "F and G values for Test Sample 113:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.5598, 0.5598)\t(0.4402, 0.4402)\t\n",
            "0.0100\t(0.6297, 0.6297)\t(0.3703, 0.3703)\t\n",
            "0.0500\t(0.4810, 0.4810)\t(0.5190, 0.5190)\t\n",
            "0.1000\t(0.6081, 0.6081)\t(0.3919, 0.3919)\t\n",
            "0.2000\t(0.6964, 0.6964)\t(0.3036, 0.3036)\t\n",
            "\n",
            "F and G values for Test Sample 114:\n",
            "Learning Rate\tClass 1 (F, G)\tClass 2 (F, G)\tClass 3 (F, G)\n",
            "0.0010\t(0.4142, 0.4142)\t(0.5858, 0.5858)\t\n",
            "0.0100\t(0.1226, 0.1226)\t(0.8774, 0.8774)\t\n",
            "0.0500\t(0.0110, 0.0110)\t(0.9890, 0.9890)\t\n",
            "0.1000\t(0.0099, 0.0099)\t(0.9901, 0.9901)\t\n",
            "0.2000\t(0.0012, 0.0012)\t(0.9988, 0.9988)\t\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate min and max probabilities\n",
        "alpha = 0.5  # Example value, adjust as needed\n",
        "beta=0.5\n",
        "\n",
        "# Initialize matrices to store F and G values\n",
        "F_values = np.zeros((5, X_test.shape[0], output_size))\n",
        "G_values = np.zeros((5, X_test.shape[0], output_size))\n",
        "\n",
        "F_G_values = np.zeros((X_test.shape[0], 5, 2, output_size))\n",
        "\n",
        "\n",
        "for lr_idx in range(5):\n",
        "    for sample_idx in range(X_test.shape[0]):\n",
        "        min_probs = np.min(class_probabilities[lr_idx, :, sample_idx, :], axis=0)\n",
        "        max_probs = np.max(class_probabilities[lr_idx, :, sample_idx, :], axis=0)\n",
        "\n",
        "        # Calculate F and G values\n",
        "        F_values[lr_idx, sample_idx, :] = (1 - alpha) * min_probs + alpha * max_probs\n",
        "        G_values[lr_idx, sample_idx, :] = (1 - beta) * min_probs + beta * max_probs \n",
        "        #G_values[lr_idx, sample_idx, :] = F_values[lr_idx, sample_idx, :]  # F and G are the same in this context\n",
        "\n",
        "        # Store F and G values, indexing correctly into F_G_values\n",
        "        F_G_values[sample_idx, lr_idx, 0, :] = F_values[lr_idx, sample_idx, :]\n",
        "        F_G_values[sample_idx, lr_idx, 1, :] = G_values[lr_idx, sample_idx,:]\n",
        "\n",
        "\n",
        "# Print F and G values\n",
        "for sample_idx in range(X_test.shape[0]):\n",
        "    print(f\"F and G values for Test Sample {sample_idx + 1}:\")\n",
        "    print(\"Learning Rate\\tClass 1 (F, G)\\tClass 2 (F, G)\\tClass 3 (F, G)\")\n",
        "    for lr_idx in range(5):\n",
        "        print(f\"{learning_rates[lr_idx]:.4f}\", end=\"\\t\")\n",
        "        for class_idx in range(output_size):\n",
        "            print(f\"({F_values[lr_idx, sample_idx, class_idx]:.4f}, {G_values[lr_idx, sample_idx, class_idx]:.4f})\", end=\"\\t\")\n",
        "        print()  # Newline for each learning rate\n",
        "    print()  # Newline for each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXMkWgCqIK5o",
        "outputId": "69ec2f85-e03d-4dc1-9239-c5b96ee26be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 1:\n",
            "Class 1: (F, G) = (0.8394, 0.8507)\n",
            "Class 2: (F, G) = (0.1062, 0.1493)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 2:\n",
            "Class 1: (F, G) = (0.0262, 0.1275)\n",
            "Class 2: (F, G) = (0.8493, 0.8725)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 3:\n",
            "Class 1: (F, G) = (0.0425, 0.1488)\n",
            "Class 2: (F, G) = (0.8307, 0.8512)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 4:\n",
            "Class 1: (F, G) = (0.8873, 0.9018)\n",
            "Class 2: (F, G) = (0.0193, 0.0982)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 5:\n",
            "Class 1: (F, G) = (0.9083, 0.9184)\n",
            "Class 2: (F, G) = (0.0145, 0.0816)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 6:\n",
            "Class 1: (F, G) = (0.0223, 0.0930)\n",
            "Class 2: (F, G) = (0.8973, 0.9070)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 7:\n",
            "Class 1: (F, G) = (0.0228, 0.1002)\n",
            "Class 2: (F, G) = (0.8874, 0.8998)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 8:\n",
            "Class 1: (F, G) = (0.0915, 0.1809)\n",
            "Class 2: (F, G) = (0.7944, 0.8191)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 9:\n",
            "Class 1: (F, G) = (0.6064, 0.6156)\n",
            "Class 2: (F, G) = (0.3704, 0.3844)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 10:\n",
            "Class 1: (F, G) = (0.9080, 0.9171)\n",
            "Class 2: (F, G) = (0.0194, 0.0829)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 11:\n",
            "Class 1: (F, G) = (0.8781, 0.8863)\n",
            "Class 2: (F, G) = (0.0728, 0.1137)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 12:\n",
            "Class 1: (F, G) = (0.0671, 0.1765)\n",
            "Class 2: (F, G) = (0.7915, 0.8235)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 13:\n",
            "Class 1: (F, G) = (0.9010, 0.9098)\n",
            "Class 2: (F, G) = (0.0320, 0.0902)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 14:\n",
            "Class 1: (F, G) = (0.1818, 0.2257)\n",
            "Class 2: (F, G) = (0.7601, 0.7743)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 15:\n",
            "Class 1: (F, G) = (0.9065, 0.9160)\n",
            "Class 2: (F, G) = (0.0199, 0.0840)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 16:\n",
            "Class 1: (F, G) = (0.0341, 0.1034)\n",
            "Class 2: (F, G) = (0.8857, 0.8966)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 17:\n",
            "Class 1: (F, G) = (0.9074, 0.9165)\n",
            "Class 2: (F, G) = (0.0213, 0.0835)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 18:\n",
            "Class 1: (F, G) = (0.9340, 0.9390)\n",
            "Class 2: (F, G) = (0.0123, 0.0610)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 19:\n",
            "Class 1: (F, G) = (0.9287, 0.9350)\n",
            "Class 2: (F, G) = (0.0118, 0.0650)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 20:\n",
            "Class 1: (F, G) = (0.0235, 0.1020)\n",
            "Class 2: (F, G) = (0.8855, 0.8980)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 21:\n",
            "Class 1: (F, G) = (0.8133, 0.8264)\n",
            "Class 2: (F, G) = (0.1172, 0.1736)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 22:\n",
            "Class 1: (F, G) = (0.8964, 0.9060)\n",
            "Class 2: (F, G) = (0.0334, 0.0940)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 23:\n",
            "Class 1: (F, G) = (0.0226, 0.0945)\n",
            "Class 2: (F, G) = (0.8953, 0.9055)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 24:\n",
            "Class 1: (F, G) = (0.9270, 0.9334)\n",
            "Class 2: (F, G) = (0.0133, 0.0666)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 25:\n",
            "Class 1: (F, G) = (0.9136, 0.9212)\n",
            "Class 2: (F, G) = (0.0192, 0.0788)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 26:\n",
            "Class 1: (F, G) = (0.8707, 0.8866)\n",
            "Class 2: (F, G) = (0.0250, 0.1134)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 27:\n",
            "Class 1: (F, G) = (0.9026, 0.9125)\n",
            "Class 2: (F, G) = (0.0205, 0.0875)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 28:\n",
            "Class 1: (F, G) = (0.8856, 0.8986)\n",
            "Class 2: (F, G) = (0.0290, 0.1014)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 29:\n",
            "Class 1: (F, G) = (0.9090, 0.9174)\n",
            "Class 2: (F, G) = (0.0229, 0.0826)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 30:\n",
            "Class 1: (F, G) = (0.0246, 0.1055)\n",
            "Class 2: (F, G) = (0.8812, 0.8945)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 31:\n",
            "Class 1: (F, G) = (0.9138, 0.9217)\n",
            "Class 2: (F, G) = (0.0174, 0.0783)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 32:\n",
            "Class 1: (F, G) = (0.9221, 0.9291)\n",
            "Class 2: (F, G) = (0.0145, 0.0709)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 33:\n",
            "Class 1: (F, G) = (0.8972, 0.9076)\n",
            "Class 2: (F, G) = (0.0232, 0.0924)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 34:\n",
            "Class 1: (F, G) = (0.9059, 0.9142)\n",
            "Class 2: (F, G) = (0.0282, 0.0858)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 35:\n",
            "Class 1: (F, G) = (0.9197, 0.9275)\n",
            "Class 2: (F, G) = (0.0141, 0.0725)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 36:\n",
            "Class 1: (F, G) = (0.9114, 0.9205)\n",
            "Class 2: (F, G) = (0.0199, 0.0795)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 37:\n",
            "Class 1: (F, G) = (0.1531, 0.2765)\n",
            "Class 2: (F, G) = (0.6779, 0.7235)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 38:\n",
            "Class 1: (F, G) = (0.9060, 0.9158)\n",
            "Class 2: (F, G) = (0.0194, 0.0842)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 39:\n",
            "Class 1: (F, G) = (0.0280, 0.1301)\n",
            "Class 2: (F, G) = (0.8470, 0.8699)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 40:\n",
            "Class 1: (F, G) = (0.8625, 0.8710)\n",
            "Class 2: (F, G) = (0.0806, 0.1290)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 41:\n",
            "Class 1: (F, G) = (0.9202, 0.9278)\n",
            "Class 2: (F, G) = (0.0142, 0.0722)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 42:\n",
            "Class 1: (F, G) = (0.0382, 0.1420)\n",
            "Class 2: (F, G) = (0.8390, 0.8580)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 43:\n",
            "Class 1: (F, G) = (0.8982, 0.9086)\n",
            "Class 2: (F, G) = (0.0238, 0.0914)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 44:\n",
            "Class 1: (F, G) = (0.9162, 0.9240)\n",
            "Class 2: (F, G) = (0.0178, 0.0760)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 45:\n",
            "Class 1: (F, G) = (0.8770, 0.8895)\n",
            "Class 2: (F, G) = (0.0507, 0.1105)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 46:\n",
            "Class 1: (F, G) = (0.8682, 0.8782)\n",
            "Class 2: (F, G) = (0.0791, 0.1218)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 47:\n",
            "Class 1: (F, G) = (0.8803, 0.8973)\n",
            "Class 2: (F, G) = (0.0230, 0.1027)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 48:\n",
            "Class 1: (F, G) = (0.9266, 0.9320)\n",
            "Class 2: (F, G) = (0.0174, 0.0680)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 49:\n",
            "Class 1: (F, G) = (0.8676, 0.8805)\n",
            "Class 2: (F, G) = (0.0605, 0.1195)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 50:\n",
            "Class 1: (F, G) = (0.8980, 0.9086)\n",
            "Class 2: (F, G) = (0.0211, 0.0914)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 51:\n",
            "Class 1: (F, G) = (0.0294, 0.1383)\n",
            "Class 2: (F, G) = (0.8363, 0.8617)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 52:\n",
            "Class 1: (F, G) = (0.0232, 0.1003)\n",
            "Class 2: (F, G) = (0.8880, 0.8997)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 53:\n",
            "Class 1: (F, G) = (0.6388, 0.6443)\n",
            "Class 2: (F, G) = (0.3437, 0.3557)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 54:\n",
            "Class 1: (F, G) = (0.8318, 0.8512)\n",
            "Class 2: (F, G) = (0.0609, 0.1488)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 55:\n",
            "Class 1: (F, G) = (0.8894, 0.9035)\n",
            "Class 2: (F, G) = (0.0162, 0.0965)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 56:\n",
            "Class 1: (F, G) = (0.9002, 0.9085)\n",
            "Class 2: (F, G) = (0.0323, 0.0915)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 57:\n",
            "Class 1: (F, G) = (0.9112, 0.9204)\n",
            "Class 2: (F, G) = (0.0146, 0.0796)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 58:\n",
            "Class 1: (F, G) = (0.0220, 0.0915)\n",
            "Class 2: (F, G) = (0.8992, 0.9085)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 59:\n",
            "Class 1: (F, G) = (0.1940, 0.2563)\n",
            "Class 2: (F, G) = (0.7216, 0.7437)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 60:\n",
            "Class 1: (F, G) = (0.9189, 0.9265)\n",
            "Class 2: (F, G) = (0.0146, 0.0735)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 61:\n",
            "Class 1: (F, G) = (0.9025, 0.9117)\n",
            "Class 2: (F, G) = (0.0273, 0.0883)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 62:\n",
            "Class 1: (F, G) = (0.0233, 0.0997)\n",
            "Class 2: (F, G) = (0.8884, 0.9003)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 63:\n",
            "Class 1: (F, G) = (0.0229, 0.1000)\n",
            "Class 2: (F, G) = (0.8880, 0.9000)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 64:\n",
            "Class 1: (F, G) = (0.8761, 0.8838)\n",
            "Class 2: (F, G) = (0.0723, 0.1162)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 65:\n",
            "Class 1: (F, G) = (0.9191, 0.9266)\n",
            "Class 2: (F, G) = (0.0167, 0.0734)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 66:\n",
            "Class 1: (F, G) = (0.8347, 0.8448)\n",
            "Class 2: (F, G) = (0.1159, 0.1552)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 67:\n",
            "Class 1: (F, G) = (0.0253, 0.1038)\n",
            "Class 2: (F, G) = (0.8842, 0.8962)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 68:\n",
            "Class 1: (F, G) = (0.0244, 0.0918)\n",
            "Class 2: (F, G) = (0.8998, 0.9082)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 69:\n",
            "Class 1: (F, G) = (0.9170, 0.9248)\n",
            "Class 2: (F, G) = (0.0178, 0.0752)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 70:\n",
            "Class 1: (F, G) = (0.8580, 0.8727)\n",
            "Class 2: (F, G) = (0.0537, 0.1273)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 71:\n",
            "Class 1: (F, G) = (0.0460, 0.1455)\n",
            "Class 2: (F, G) = (0.8359, 0.8545)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 72:\n",
            "Class 1: (F, G) = (0.0367, 0.1286)\n",
            "Class 2: (F, G) = (0.8541, 0.8714)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 73:\n",
            "Class 1: (F, G) = (0.9084, 0.9165)\n",
            "Class 2: (F, G) = (0.0228, 0.0835)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 74:\n",
            "Class 1: (F, G) = (0.0402, 0.1249)\n",
            "Class 2: (F, G) = (0.8591, 0.8751)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 75:\n",
            "Class 1: (F, G) = (0.9246, 0.9313)\n",
            "Class 2: (F, G) = (0.0135, 0.0687)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 76:\n",
            "Class 1: (F, G) = (0.8764, 0.8892)\n",
            "Class 2: (F, G) = (0.0378, 0.1108)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 77:\n",
            "Class 1: (F, G) = (0.8963, 0.9039)\n",
            "Class 2: (F, G) = (0.0444, 0.0961)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 78:\n",
            "Class 1: (F, G) = (0.4726, 0.4781)\n",
            "Class 2: (F, G) = (0.5170, 0.5219)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 79:\n",
            "Class 1: (F, G) = (0.9265, 0.9330)\n",
            "Class 2: (F, G) = (0.0130, 0.0670)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 80:\n",
            "Class 1: (F, G) = (0.8665, 0.8824)\n",
            "Class 2: (F, G) = (0.0380, 0.1176)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 81:\n",
            "Class 1: (F, G) = (0.0405, 0.1438)\n",
            "Class 2: (F, G) = (0.8352, 0.8562)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 82:\n",
            "Class 1: (F, G) = (0.9237, 0.9308)\n",
            "Class 2: (F, G) = (0.0133, 0.0692)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 83:\n",
            "Class 1: (F, G) = (0.2819, 0.3233)\n",
            "Class 2: (F, G) = (0.6561, 0.6767)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 84:\n",
            "Class 1: (F, G) = (0.0229, 0.0995)\n",
            "Class 2: (F, G) = (0.8887, 0.9005)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 85:\n",
            "Class 1: (F, G) = (0.0488, 0.1791)\n",
            "Class 2: (F, G) = (0.7881, 0.8209)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 86:\n",
            "Class 1: (F, G) = (0.0354, 0.1063)\n",
            "Class 2: (F, G) = (0.8835, 0.8937)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 87:\n",
            "Class 1: (F, G) = (0.0342, 0.1113)\n",
            "Class 2: (F, G) = (0.8771, 0.8887)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 88:\n",
            "Class 1: (F, G) = (0.0312, 0.1092)\n",
            "Class 2: (F, G) = (0.8780, 0.8908)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 89:\n",
            "Class 1: (F, G) = (0.9038, 0.9130)\n",
            "Class 2: (F, G) = (0.0208, 0.0870)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 90:\n",
            "Class 1: (F, G) = (0.9014, 0.9106)\n",
            "Class 2: (F, G) = (0.0252, 0.0894)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 91:\n",
            "Class 1: (F, G) = (0.8932, 0.9053)\n",
            "Class 2: (F, G) = (0.0263, 0.0947)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 92:\n",
            "Class 1: (F, G) = (0.6050, 0.6086)\n",
            "Class 2: (F, G) = (0.3862, 0.3914)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 93:\n",
            "Class 1: (F, G) = (0.8511, 0.8627)\n",
            "Class 2: (F, G) = (0.0652, 0.1373)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 94:\n",
            "Class 1: (F, G) = (0.9224, 0.9278)\n",
            "Class 2: (F, G) = (0.0210, 0.0722)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 95:\n",
            "Class 1: (F, G) = (0.9107, 0.9199)\n",
            "Class 2: (F, G) = (0.0181, 0.0801)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 96:\n",
            "Class 1: (F, G) = (0.9166, 0.9251)\n",
            "Class 2: (F, G) = (0.0144, 0.0749)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 97:\n",
            "Class 1: (F, G) = (0.0243, 0.1121)\n",
            "Class 2: (F, G) = (0.8719, 0.8879)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 98:\n",
            "Class 1: (F, G) = (0.0258, 0.1054)\n",
            "Class 2: (F, G) = (0.8828, 0.8946)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 99:\n",
            "Class 1: (F, G) = (0.9207, 0.9283)\n",
            "Class 2: (F, G) = (0.0136, 0.0717)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 100:\n",
            "Class 1: (F, G) = (0.0310, 0.1350)\n",
            "Class 2: (F, G) = (0.8438, 0.8650)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 101:\n",
            "Class 1: (F, G) = (0.0477, 0.1748)\n",
            "Class 2: (F, G) = (0.7926, 0.8252)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 102:\n",
            "Class 1: (F, G) = (0.9295, 0.9357)\n",
            "Class 2: (F, G) = (0.0119, 0.0643)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 103:\n",
            "Class 1: (F, G) = (0.0261, 0.1057)\n",
            "Class 2: (F, G) = (0.8815, 0.8943)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 104:\n",
            "Class 1: (F, G) = (0.0296, 0.1204)\n",
            "Class 2: (F, G) = (0.8622, 0.8796)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 105:\n",
            "Class 1: (F, G) = (0.9080, 0.9156)\n",
            "Class 2: (F, G) = (0.0287, 0.0844)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 106:\n",
            "Class 1: (F, G) = (0.8942, 0.9023)\n",
            "Class 2: (F, G) = (0.0470, 0.0977)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 107:\n",
            "Class 1: (F, G) = (0.8897, 0.9012)\n",
            "Class 2: (F, G) = (0.0296, 0.0988)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 108:\n",
            "Class 1: (F, G) = (0.0234, 0.1058)\n",
            "Class 2: (F, G) = (0.8794, 0.8942)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 109:\n",
            "Class 1: (F, G) = (0.8028, 0.8146)\n",
            "Class 2: (F, G) = (0.1404, 0.1854)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 110:\n",
            "Class 1: (F, G) = (0.8829, 0.8914)\n",
            "Class 2: (F, G) = (0.0575, 0.1086)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 111:\n",
            "Class 1: (F, G) = (0.0410, 0.1579)\n",
            "Class 2: (F, G) = (0.8125, 0.8421)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 112:\n",
            "Class 1: (F, G) = (0.9084, 0.9176)\n",
            "Class 2: (F, G) = (0.0189, 0.0824)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 113:\n",
            "Class 1: (F, G) = (0.5905, 0.5950)\n",
            "Class 2: (F, G) = (0.3986, 0.4050)\n",
            "\n",
            "Geometric Mean of F values and Arithmetic Mean of G values for Test Sample 114:\n",
            "Class 1: (F, G) = (0.0233, 0.1118)\n",
            "Class 2: (F, G) = (0.8715, 0.8882)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Calculate geometric mean of F values and arithmetic mean of G values across learning rates for each class\n",
        "geometric_mean_F_values = np.exp(np.mean(np.log(F_G_values[:, :, 0, :]), axis=1))\n",
        "arithmetic_mean_G_values = np.mean(F_G_values[:, :, 1, :], axis=1)\n",
        "\n",
        "# Print the results in the desired format\n",
        "for test_sample_idx in range(X_test.shape[0]):\n",
        "    print(f\"\\nGeometric Mean of F values and Arithmetic Mean of G values for Test Sample {test_sample_idx + 1}:\")\n",
        "\n",
        "    for class_idx in range(output_size):\n",
        "        geom_F = geometric_mean_F_values[test_sample_idx, class_idx]\n",
        "        avg_G = arithmetic_mean_G_values[test_sample_idx, class_idx]\n",
        "        print(f\"Class {class_idx + 1}: (F, G) = ({geom_F:.4f}, {avg_G:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfFWUKnBeWj8",
        "outputId": "2a28d7c2-6f91-40cf-867f-ea54c2ec15d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Metrics for the test set:\n",
            "Accuracy: 0.9825\n",
            "Precision: 0.9825\n",
            "Recall: 0.9825\n",
            "F1 Score: 0.9825\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "y_pred = []\n",
        "\n",
        "for sample_idx in range(X_test.shape[0]):\n",
        "    sample = X_test[sample_idx].reshape(1, -1)\n",
        "\n",
        "    # Forward propagation with the last trained weights (for simplicity)\n",
        "    a1, a2 = forward_propagation(sample, W1, b1, W2, b2)\n",
        "\n",
        "    y_pred.append(np.argmax(a2))\n",
        "\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate and print accuracy, precision, recall, and F1 score\n",
        "report = classification_report(y_true, y_pred, output_dict=True)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = report['weighted avg']['precision']\n",
        "recall = report['weighted avg']['recall']\n",
        "f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "print(\"\\nMetrics for the test set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx5Q-NtJjXj7",
        "outputId": "f174965f-b889-4137-a352-a12b8dced83e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 1:\n",
            "Class 1: [F, G] = [0.8394, 0.8507], Combined = 0.7056\n",
            "Class 2: [F, G] = [0.1062, 0.1493], Combined = 0.5104\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 2:\n",
            "Class 1: [F, G] = [0.0262, 0.1275], Combined = 0.4686\n",
            "Class 2: [F, G] = [0.8493, 0.8725], Combined = 0.7036\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 3:\n",
            "Class 1: [F, G] = [0.0425, 0.1488], Combined = 0.4707\n",
            "Class 2: [F, G] = [0.8307, 0.8512], Combined = 0.7000\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 4:\n",
            "Class 1: [F, G] = [0.8873, 0.9018], Combined = 0.7164\n",
            "Class 2: [F, G] = [0.0193, 0.0982], Combined = 0.4753\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 5:\n",
            "Class 1: [F, G] = [0.9083, 0.9184], Combined = 0.7233\n",
            "Class 2: [F, G] = [0.0145, 0.0816], Combined = 0.4785\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 6:\n",
            "Class 1: [F, G] = [0.0223, 0.0930], Combined = 0.4791\n",
            "Class 2: [F, G] = [0.8973, 0.9070], Combined = 0.7207\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 7:\n",
            "Class 1: [F, G] = [0.0228, 0.1002], Combined = 0.4767\n",
            "Class 2: [F, G] = [0.8874, 0.8998], Combined = 0.7172\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 8:\n",
            "Class 1: [F, G] = [0.0915, 0.1809], Combined = 0.4894\n",
            "Class 2: [F, G] = [0.7944, 0.8191], Combined = 0.6893\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 9:\n",
            "Class 1: [F, G] = [0.6064, 0.6156], Combined = 0.6482\n",
            "Class 2: [F, G] = [0.3704, 0.3844], Combined = 0.5873\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 10:\n",
            "Class 1: [F, G] = [0.9080, 0.9171], Combined = 0.7236\n",
            "Class 2: [F, G] = [0.0194, 0.0829], Combined = 0.4810\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 11:\n",
            "Class 1: [F, G] = [0.8781, 0.8863], Combined = 0.7165\n",
            "Class 2: [F, G] = [0.0728, 0.1137], Combined = 0.5029\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 12:\n",
            "Class 1: [F, G] = [0.0671, 0.1765], Combined = 0.4758\n",
            "Class 2: [F, G] = [0.7915, 0.8235], Combined = 0.6859\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 13:\n",
            "Class 1: [F, G] = [0.9010, 0.9098], Combined = 0.7220\n",
            "Class 2: [F, G] = [0.0320, 0.0902], Combined = 0.4862\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 14:\n",
            "Class 1: [F, G] = [0.1818, 0.2257], Combined = 0.5290\n",
            "Class 2: [F, G] = [0.7601, 0.7743], Combined = 0.6847\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 15:\n",
            "Class 1: [F, G] = [0.9065, 0.9160], Combined = 0.7231\n",
            "Class 2: [F, G] = [0.0199, 0.0840], Combined = 0.4810\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 16:\n",
            "Class 1: [F, G] = [0.0341, 0.1034], Combined = 0.4825\n",
            "Class 2: [F, G] = [0.8857, 0.8966], Combined = 0.7174\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 17:\n",
            "Class 1: [F, G] = [0.9074, 0.9165], Combined = 0.7234\n",
            "Class 2: [F, G] = [0.0213, 0.0835], Combined = 0.4820\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 18:\n",
            "Class 1: [F, G] = [0.9340, 0.9390], Combined = 0.7316\n",
            "Class 2: [F, G] = [0.0123, 0.0610], Combined = 0.4848\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 19:\n",
            "Class 1: [F, G] = [0.9287, 0.9350], Combined = 0.7298\n",
            "Class 2: [F, G] = [0.0118, 0.0650], Combined = 0.4830\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 20:\n",
            "Class 1: [F, G] = [0.0235, 0.1020], Combined = 0.4765\n",
            "Class 2: [F, G] = [0.8855, 0.8980], Combined = 0.7167\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 21:\n",
            "Class 1: [F, G] = [0.8133, 0.8264], Combined = 0.6984\n",
            "Class 2: [F, G] = [0.1172, 0.1736], Combined = 0.5081\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 22:\n",
            "Class 1: [F, G] = [0.8964, 0.9060], Combined = 0.7205\n",
            "Class 2: [F, G] = [0.0334, 0.0940], Combined = 0.4856\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 23:\n",
            "Class 1: [F, G] = [0.0226, 0.0945], Combined = 0.4787\n",
            "Class 2: [F, G] = [0.8953, 0.9055], Combined = 0.7200\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 24:\n",
            "Class 1: [F, G] = [0.9270, 0.9334], Combined = 0.7294\n",
            "Class 2: [F, G] = [0.0133, 0.0666], Combined = 0.4833\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 25:\n",
            "Class 1: [F, G] = [0.9136, 0.9212], Combined = 0.7256\n",
            "Class 2: [F, G] = [0.0192, 0.0788], Combined = 0.4825\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 26:\n",
            "Class 1: [F, G] = [0.8707, 0.8866], Combined = 0.7117\n",
            "Class 2: [F, G] = [0.0250, 0.1134], Combined = 0.4731\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 27:\n",
            "Class 1: [F, G] = [0.9026, 0.9125], Combined = 0.7220\n",
            "Class 2: [F, G] = [0.0205, 0.0875], Combined = 0.4800\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 28:\n",
            "Class 1: [F, G] = [0.8856, 0.8986], Combined = 0.7165\n",
            "Class 2: [F, G] = [0.0290, 0.1014], Combined = 0.4801\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 29:\n",
            "Class 1: [F, G] = [0.9090, 0.9174], Combined = 0.7241\n",
            "Class 2: [F, G] = [0.0229, 0.0826], Combined = 0.4833\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 30:\n",
            "Class 1: [F, G] = [0.0246, 0.1055], Combined = 0.4758\n",
            "Class 2: [F, G] = [0.8812, 0.8945], Combined = 0.7153\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 31:\n",
            "Class 1: [F, G] = [0.9138, 0.9217], Combined = 0.7255\n",
            "Class 2: [F, G] = [0.0174, 0.0783], Combined = 0.4815\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 32:\n",
            "Class 1: [F, G] = [0.9221, 0.9291], Combined = 0.7279\n",
            "Class 2: [F, G] = [0.0145, 0.0709], Combined = 0.4825\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 33:\n",
            "Class 1: [F, G] = [0.8972, 0.9076], Combined = 0.7204\n",
            "Class 2: [F, G] = [0.0232, 0.0924], Combined = 0.4798\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 34:\n",
            "Class 1: [F, G] = [0.9059, 0.9142], Combined = 0.7233\n",
            "Class 2: [F, G] = [0.0282, 0.0858], Combined = 0.4855\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 35:\n",
            "Class 1: [F, G] = [0.9197, 0.9275], Combined = 0.7270\n",
            "Class 2: [F, G] = [0.0141, 0.0725], Combined = 0.4816\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 36:\n",
            "Class 1: [F, G] = [0.9114, 0.9205], Combined = 0.7245\n",
            "Class 2: [F, G] = [0.0199, 0.0795], Combined = 0.4826\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 37:\n",
            "Class 1: [F, G] = [0.1531, 0.2765], Combined = 0.4920\n",
            "Class 2: [F, G] = [0.6779, 0.7235], Combined = 0.6524\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 38:\n",
            "Class 1: [F, G] = [0.9060, 0.9158], Combined = 0.7228\n",
            "Class 2: [F, G] = [0.0194, 0.0842], Combined = 0.4806\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 39:\n",
            "Class 1: [F, G] = [0.0280, 0.1301], Combined = 0.4687\n",
            "Class 2: [F, G] = [0.8470, 0.8699], Combined = 0.7032\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 40:\n",
            "Class 1: [F, G] = [0.8625, 0.8710], Combined = 0.7125\n",
            "Class 2: [F, G] = [0.0806, 0.1290], Combined = 0.5020\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 41:\n",
            "Class 1: [F, G] = [0.9202, 0.9278], Combined = 0.7272\n",
            "Class 2: [F, G] = [0.0142, 0.0722], Combined = 0.4818\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 42:\n",
            "Class 1: [F, G] = [0.0382, 0.1420], Combined = 0.4706\n",
            "Class 2: [F, G] = [0.8390, 0.8580], Combined = 0.7026\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 43:\n",
            "Class 1: [F, G] = [0.8982, 0.9086], Combined = 0.7206\n",
            "Class 2: [F, G] = [0.0238, 0.0914], Combined = 0.4806\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 44:\n",
            "Class 1: [F, G] = [0.9162, 0.9240], Combined = 0.7262\n",
            "Class 2: [F, G] = [0.0178, 0.0760], Combined = 0.4826\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 45:\n",
            "Class 1: [F, G] = [0.8770, 0.8895], Combined = 0.7146\n",
            "Class 2: [F, G] = [0.0507, 0.1105], Combined = 0.4902\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 46:\n",
            "Class 1: [F, G] = [0.8682, 0.8782], Combined = 0.7133\n",
            "Class 2: [F, G] = [0.0791, 0.1218], Combined = 0.5037\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 47:\n",
            "Class 1: [F, G] = [0.8803, 0.8973], Combined = 0.7137\n",
            "Class 2: [F, G] = [0.0230, 0.1027], Combined = 0.4758\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 48:\n",
            "Class 1: [F, G] = [0.9266, 0.9320], Combined = 0.7296\n",
            "Class 2: [F, G] = [0.0174, 0.0680], Combined = 0.4854\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 49:\n",
            "Class 1: [F, G] = [0.8676, 0.8805], Combined = 0.7121\n",
            "Class 2: [F, G] = [0.0605, 0.1195], Combined = 0.4930\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 50:\n",
            "Class 1: [F, G] = [0.8980, 0.9086], Combined = 0.7205\n",
            "Class 2: [F, G] = [0.0211, 0.0914], Combined = 0.4789\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 51:\n",
            "Class 1: [F, G] = [0.0294, 0.1383], Combined = 0.4665\n",
            "Class 2: [F, G] = [0.8363, 0.8617], Combined = 0.6995\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 52:\n",
            "Class 1: [F, G] = [0.0232, 0.1003], Combined = 0.4769\n",
            "Class 2: [F, G] = [0.8880, 0.8997], Combined = 0.7176\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 53:\n",
            "Class 1: [F, G] = [0.6388, 0.6443], Combined = 0.6576\n",
            "Class 2: [F, G] = [0.3437, 0.3557], Combined = 0.5814\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 54:\n",
            "Class 1: [F, G] = [0.8318, 0.8512], Combined = 0.7007\n",
            "Class 2: [F, G] = [0.0609, 0.1488], Combined = 0.4822\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 55:\n",
            "Class 1: [F, G] = [0.8894, 0.9035], Combined = 0.7171\n",
            "Class 2: [F, G] = [0.0162, 0.0965], Combined = 0.4739\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 56:\n",
            "Class 1: [F, G] = [0.9002, 0.9085], Combined = 0.7219\n",
            "Class 2: [F, G] = [0.0323, 0.0915], Combined = 0.4859\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 57:\n",
            "Class 1: [F, G] = [0.9112, 0.9204], Combined = 0.7243\n",
            "Class 2: [F, G] = [0.0146, 0.0796], Combined = 0.4793\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 58:\n",
            "Class 1: [F, G] = [0.0220, 0.0915], Combined = 0.4794\n",
            "Class 2: [F, G] = [0.8992, 0.9085], Combined = 0.7213\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 59:\n",
            "Class 1: [F, G] = [0.1940, 0.2563], Combined = 0.5251\n",
            "Class 2: [F, G] = [0.7216, 0.7437], Combined = 0.6721\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 60:\n",
            "Class 1: [F, G] = [0.9189, 0.9265], Combined = 0.7269\n",
            "Class 2: [F, G] = [0.0146, 0.0735], Combined = 0.4816\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 61:\n",
            "Class 1: [F, G] = [0.9025, 0.9117], Combined = 0.7222\n",
            "Class 2: [F, G] = [0.0273, 0.0883], Combined = 0.4840\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 62:\n",
            "Class 1: [F, G] = [0.0233, 0.0997], Combined = 0.4771\n",
            "Class 2: [F, G] = [0.8884, 0.9003], Combined = 0.7176\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 63:\n",
            "Class 1: [F, G] = [0.0229, 0.1000], Combined = 0.4768\n",
            "Class 2: [F, G] = [0.8880, 0.9000], Combined = 0.7175\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 64:\n",
            "Class 1: [F, G] = [0.8761, 0.8838], Combined = 0.7162\n",
            "Class 2: [F, G] = [0.0723, 0.1162], Combined = 0.5016\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 65:\n",
            "Class 1: [F, G] = [0.9191, 0.9266], Combined = 0.7270\n",
            "Class 2: [F, G] = [0.0167, 0.0734], Combined = 0.4829\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 66:\n",
            "Class 1: [F, G] = [0.8347, 0.8448], Combined = 0.7049\n",
            "Class 2: [F, G] = [0.1159, 0.1552], Combined = 0.5143\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 67:\n",
            "Class 1: [F, G] = [0.0253, 0.1038], Combined = 0.4769\n",
            "Class 2: [F, G] = [0.8842, 0.8962], Combined = 0.7165\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 68:\n",
            "Class 1: [F, G] = [0.0244, 0.0918], Combined = 0.4808\n",
            "Class 2: [F, G] = [0.8998, 0.9082], Combined = 0.7218\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 69:\n",
            "Class 1: [F, G] = [0.9170, 0.9248], Combined = 0.7264\n",
            "Class 2: [F, G] = [0.0178, 0.0752], Combined = 0.4829\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 70:\n",
            "Class 1: [F, G] = [0.8580, 0.8727], Combined = 0.7090\n",
            "Class 2: [F, G] = [0.0537, 0.1273], Combined = 0.4858\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 71:\n",
            "Class 1: [F, G] = [0.0460, 0.1455], Combined = 0.4742\n",
            "Class 2: [F, G] = [0.8359, 0.8545], Combined = 0.7020\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 72:\n",
            "Class 1: [F, G] = [0.0367, 0.1286], Combined = 0.4747\n",
            "Class 2: [F, G] = [0.8541, 0.8714], Combined = 0.7070\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 73:\n",
            "Class 1: [F, G] = [0.9084, 0.9165], Combined = 0.7241\n",
            "Class 2: [F, G] = [0.0228, 0.0835], Combined = 0.4829\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 74:\n",
            "Class 1: [F, G] = [0.0402, 0.1249], Combined = 0.4783\n",
            "Class 2: [F, G] = [0.8591, 0.8751], Combined = 0.7088\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 75:\n",
            "Class 1: [F, G] = [0.9246, 0.9313], Combined = 0.7287\n",
            "Class 2: [F, G] = [0.0135, 0.0687], Combined = 0.4827\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 76:\n",
            "Class 1: [F, G] = [0.8764, 0.8892], Combined = 0.7143\n",
            "Class 2: [F, G] = [0.0378, 0.1108], Combined = 0.4821\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 77:\n",
            "Class 1: [F, G] = [0.8963, 0.9039], Combined = 0.7212\n",
            "Class 2: [F, G] = [0.0444, 0.0961], Combined = 0.4917\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 78:\n",
            "Class 1: [F, G] = [0.4726, 0.4781], Combined = 0.6161\n",
            "Class 2: [F, G] = [0.5170, 0.5219], Combined = 0.6274\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 79:\n",
            "Class 1: [F, G] = [0.9265, 0.9330], Combined = 0.7292\n",
            "Class 2: [F, G] = [0.0130, 0.0670], Combined = 0.4830\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 80:\n",
            "Class 1: [F, G] = [0.8665, 0.8824], Combined = 0.7106\n",
            "Class 2: [F, G] = [0.0380, 0.1176], Combined = 0.4797\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 81:\n",
            "Class 1: [F, G] = [0.0405, 0.1438], Combined = 0.4714\n",
            "Class 2: [F, G] = [0.8352, 0.8562], Combined = 0.7009\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 82:\n",
            "Class 1: [F, G] = [0.9237, 0.9308], Combined = 0.7282\n",
            "Class 2: [F, G] = [0.0133, 0.0692], Combined = 0.4824\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 83:\n",
            "Class 1: [F, G] = [0.2819, 0.3233], Combined = 0.5550\n",
            "Class 2: [F, G] = [0.6561, 0.6767], Combined = 0.6563\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 84:\n",
            "Class 1: [F, G] = [0.0229, 0.0995], Combined = 0.4770\n",
            "Class 2: [F, G] = [0.8887, 0.9005], Combined = 0.7178\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 85:\n",
            "Class 1: [F, G] = [0.0488, 0.1791], Combined = 0.4634\n",
            "Class 2: [F, G] = [0.7881, 0.8209], Combined = 0.6847\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 86:\n",
            "Class 1: [F, G] = [0.0354, 0.1063], Combined = 0.4823\n",
            "Class 2: [F, G] = [0.8835, 0.8937], Combined = 0.7170\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 87:\n",
            "Class 1: [F, G] = [0.0342, 0.1113], Combined = 0.4796\n",
            "Class 2: [F, G] = [0.8771, 0.8887], Combined = 0.7149\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 88:\n",
            "Class 1: [F, G] = [0.0312, 0.1092], Combined = 0.4786\n",
            "Class 2: [F, G] = [0.8780, 0.8908], Combined = 0.7147\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 89:\n",
            "Class 1: [F, G] = [0.9038, 0.9130], Combined = 0.7225\n",
            "Class 2: [F, G] = [0.0208, 0.0870], Combined = 0.4804\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 90:\n",
            "Class 1: [F, G] = [0.9014, 0.9106], Combined = 0.7219\n",
            "Class 2: [F, G] = [0.0252, 0.0894], Combined = 0.4822\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 91:\n",
            "Class 1: [F, G] = [0.8932, 0.9053], Combined = 0.7188\n",
            "Class 2: [F, G] = [0.0263, 0.0947], Combined = 0.4809\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 92:\n",
            "Class 1: [F, G] = [0.6050, 0.6086], Combined = 0.6499\n",
            "Class 2: [F, G] = [0.3862, 0.3914], Combined = 0.5946\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 93:\n",
            "Class 1: [F, G] = [0.8511, 0.8627], Combined = 0.7084\n",
            "Class 2: [F, G] = [0.0652, 0.1373], Combined = 0.4893\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 94:\n",
            "Class 1: [F, G] = [0.9224, 0.9278], Combined = 0.7286\n",
            "Class 2: [F, G] = [0.0210, 0.0722], Combined = 0.4861\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 95:\n",
            "Class 1: [F, G] = [0.9107, 0.9199], Combined = 0.7242\n",
            "Class 2: [F, G] = [0.0181, 0.0801], Combined = 0.4813\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 96:\n",
            "Class 1: [F, G] = [0.9166, 0.9251], Combined = 0.7260\n",
            "Class 2: [F, G] = [0.0144, 0.0749], Combined = 0.4809\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 97:\n",
            "Class 1: [F, G] = [0.0243, 0.1121], Combined = 0.4732\n",
            "Class 2: [F, G] = [0.8719, 0.8879], Combined = 0.7120\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 98:\n",
            "Class 1: [F, G] = [0.0258, 0.1054], Combined = 0.4766\n",
            "Class 2: [F, G] = [0.8828, 0.8946], Combined = 0.7163\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 99:\n",
            "Class 1: [F, G] = [0.9207, 0.9283], Combined = 0.7273\n",
            "Class 2: [F, G] = [0.0136, 0.0717], Combined = 0.4816\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 100:\n",
            "Class 1: [F, G] = [0.0310, 0.1350], Combined = 0.4688\n",
            "Class 2: [F, G] = [0.8438, 0.8650], Combined = 0.7030\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 101:\n",
            "Class 1: [F, G] = [0.0477, 0.1748], Combined = 0.4643\n",
            "Class 2: [F, G] = [0.7926, 0.8252], Combined = 0.6859\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 102:\n",
            "Class 1: [F, G] = [0.9295, 0.9357], Combined = 0.7300\n",
            "Class 2: [F, G] = [0.0119, 0.0643], Combined = 0.4833\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 103:\n",
            "Class 1: [F, G] = [0.0261, 0.1057], Combined = 0.4767\n",
            "Class 2: [F, G] = [0.8815, 0.8943], Combined = 0.7156\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 104:\n",
            "Class 1: [F, G] = [0.0296, 0.1204], Combined = 0.4734\n",
            "Class 2: [F, G] = [0.8622, 0.8796], Combined = 0.7090\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 105:\n",
            "Class 1: [F, G] = [0.9080, 0.9156], Combined = 0.7241\n",
            "Class 2: [F, G] = [0.0287, 0.0844], Combined = 0.4863\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 106:\n",
            "Class 1: [F, G] = [0.8942, 0.9023], Combined = 0.7205\n",
            "Class 2: [F, G] = [0.0470, 0.0977], Combined = 0.4927\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 107:\n",
            "Class 1: [F, G] = [0.8897, 0.9012], Combined = 0.7181\n",
            "Class 2: [F, G] = [0.0296, 0.0988], Combined = 0.4815\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 108:\n",
            "Class 1: [F, G] = [0.0234, 0.1058], Combined = 0.4749\n",
            "Class 2: [F, G] = [0.8794, 0.8942], Combined = 0.7143\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 109:\n",
            "Class 1: [F, G] = [0.8028, 0.8146], Combined = 0.6963\n",
            "Class 2: [F, G] = [0.1404, 0.1854], Combined = 0.5182\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 110:\n",
            "Class 1: [F, G] = [0.8829, 0.8914], Combined = 0.7175\n",
            "Class 2: [F, G] = [0.0575, 0.1086], Combined = 0.4952\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 111:\n",
            "Class 1: [F, G] = [0.0410, 0.1579], Combined = 0.4664\n",
            "Class 2: [F, G] = [0.8125, 0.8421], Combined = 0.6920\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 112:\n",
            "Class 1: [F, G] = [0.9084, 0.9176], Combined = 0.7237\n",
            "Class 2: [F, G] = [0.0189, 0.0824], Combined = 0.4809\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 113:\n",
            "Class 1: [F, G] = [0.5905, 0.5950], Combined = 0.6459\n",
            "Class 2: [F, G] = [0.3986, 0.4050], Combined = 0.5973\n",
            "\n",
            "Geometric Mean, Average, and Combined for Test Sample 114:\n",
            "Class 1: [F, G] = [0.0233, 0.1118], Combined = 0.4727\n",
            "Class 2: [F, G] = [0.8715, 0.8882], Combined = 0.7116\n"
          ]
        }
      ],
      "source": [
        "# Parameters for the formula\n",
        "w = 0.5\n",
        "gama=0.5\n",
        "P = 1\n",
        "\n",
        "# Calculate the combined values\n",
        "#combined_values = w * ((geometric_mean_F_values + arithmetic_mean_G_values) / 2) + (1 - w) * (1 - (arithmetic_mean_G_values - geometric_mean_F_values))**P\n",
        "combined_values = w * (((1 - gama)*geometric_mean_F_values + gama*arithmetic_mean_G_values) / 2) + (1 - w) * (1 - (arithmetic_mean_G_values - geometric_mean_F_values))**P\n",
        "# Print geometric mean F and average G values along with the combined values for each sample\n",
        "for sample_idx in range(X_test.shape[0]):\n",
        "    print(f\"\\nGeometric Mean, Average, and Combined for Test Sample {sample_idx + 1}:\")\n",
        "    for class_idx in range(output_size):\n",
        "        geom_F = geometric_mean_F_values[sample_idx, class_idx]\n",
        "        avg_G = arithmetic_mean_G_values[sample_idx, class_idx]\n",
        "        combined_value = combined_values[sample_idx, class_idx]\n",
        "        print(f\"Class {class_idx + 1}: [F, G] = [{geom_F:.4f}, {avg_G:.4f}], Combined = {combined_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvMHn3MEcY0I",
        "outputId": "08750896-ca99-45cd-8370-5cb301dfd508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Class Probabilities for Test Sample 1:\n",
            "Class 1 Probability: 0.7056\n",
            "Class 2 Probability: 0.5104\n",
            "\n",
            "Class Probabilities for Test Sample 2:\n",
            "Class 1 Probability: 0.4686\n",
            "Class 2 Probability: 0.7036\n",
            "\n",
            "Class Probabilities for Test Sample 3:\n",
            "Class 1 Probability: 0.4707\n",
            "Class 2 Probability: 0.7000\n",
            "\n",
            "Class Probabilities for Test Sample 4:\n",
            "Class 1 Probability: 0.7164\n",
            "Class 2 Probability: 0.4753\n",
            "\n",
            "Class Probabilities for Test Sample 5:\n",
            "Class 1 Probability: 0.7233\n",
            "Class 2 Probability: 0.4785\n",
            "\n",
            "Class Probabilities for Test Sample 6:\n",
            "Class 1 Probability: 0.4791\n",
            "Class 2 Probability: 0.7207\n",
            "\n",
            "Class Probabilities for Test Sample 7:\n",
            "Class 1 Probability: 0.4767\n",
            "Class 2 Probability: 0.7172\n",
            "\n",
            "Class Probabilities for Test Sample 8:\n",
            "Class 1 Probability: 0.4894\n",
            "Class 2 Probability: 0.6893\n",
            "\n",
            "Class Probabilities for Test Sample 9:\n",
            "Class 1 Probability: 0.6482\n",
            "Class 2 Probability: 0.5873\n",
            "\n",
            "Class Probabilities for Test Sample 10:\n",
            "Class 1 Probability: 0.7236\n",
            "Class 2 Probability: 0.4810\n",
            "\n",
            "Class Probabilities for Test Sample 11:\n",
            "Class 1 Probability: 0.7165\n",
            "Class 2 Probability: 0.5029\n",
            "\n",
            "Class Probabilities for Test Sample 12:\n",
            "Class 1 Probability: 0.4758\n",
            "Class 2 Probability: 0.6859\n",
            "\n",
            "Class Probabilities for Test Sample 13:\n",
            "Class 1 Probability: 0.7220\n",
            "Class 2 Probability: 0.4862\n",
            "\n",
            "Class Probabilities for Test Sample 14:\n",
            "Class 1 Probability: 0.5290\n",
            "Class 2 Probability: 0.6847\n",
            "\n",
            "Class Probabilities for Test Sample 15:\n",
            "Class 1 Probability: 0.7231\n",
            "Class 2 Probability: 0.4810\n",
            "\n",
            "Class Probabilities for Test Sample 16:\n",
            "Class 1 Probability: 0.4825\n",
            "Class 2 Probability: 0.7174\n",
            "\n",
            "Class Probabilities for Test Sample 17:\n",
            "Class 1 Probability: 0.7234\n",
            "Class 2 Probability: 0.4820\n",
            "\n",
            "Class Probabilities for Test Sample 18:\n",
            "Class 1 Probability: 0.7316\n",
            "Class 2 Probability: 0.4848\n",
            "\n",
            "Class Probabilities for Test Sample 19:\n",
            "Class 1 Probability: 0.7298\n",
            "Class 2 Probability: 0.4830\n",
            "\n",
            "Class Probabilities for Test Sample 20:\n",
            "Class 1 Probability: 0.4765\n",
            "Class 2 Probability: 0.7167\n",
            "\n",
            "Class Probabilities for Test Sample 21:\n",
            "Class 1 Probability: 0.6984\n",
            "Class 2 Probability: 0.5081\n",
            "\n",
            "Class Probabilities for Test Sample 22:\n",
            "Class 1 Probability: 0.7205\n",
            "Class 2 Probability: 0.4856\n",
            "\n",
            "Class Probabilities for Test Sample 23:\n",
            "Class 1 Probability: 0.4787\n",
            "Class 2 Probability: 0.7200\n",
            "\n",
            "Class Probabilities for Test Sample 24:\n",
            "Class 1 Probability: 0.7294\n",
            "Class 2 Probability: 0.4833\n",
            "\n",
            "Class Probabilities for Test Sample 25:\n",
            "Class 1 Probability: 0.7256\n",
            "Class 2 Probability: 0.4825\n",
            "\n",
            "Class Probabilities for Test Sample 26:\n",
            "Class 1 Probability: 0.7117\n",
            "Class 2 Probability: 0.4731\n",
            "\n",
            "Class Probabilities for Test Sample 27:\n",
            "Class 1 Probability: 0.7220\n",
            "Class 2 Probability: 0.4800\n",
            "\n",
            "Class Probabilities for Test Sample 28:\n",
            "Class 1 Probability: 0.7165\n",
            "Class 2 Probability: 0.4801\n",
            "\n",
            "Class Probabilities for Test Sample 29:\n",
            "Class 1 Probability: 0.7241\n",
            "Class 2 Probability: 0.4833\n",
            "\n",
            "Class Probabilities for Test Sample 30:\n",
            "Class 1 Probability: 0.4758\n",
            "Class 2 Probability: 0.7153\n",
            "\n",
            "Class Probabilities for Test Sample 31:\n",
            "Class 1 Probability: 0.7255\n",
            "Class 2 Probability: 0.4815\n",
            "\n",
            "Class Probabilities for Test Sample 32:\n",
            "Class 1 Probability: 0.7279\n",
            "Class 2 Probability: 0.4825\n",
            "\n",
            "Class Probabilities for Test Sample 33:\n",
            "Class 1 Probability: 0.7204\n",
            "Class 2 Probability: 0.4798\n",
            "\n",
            "Class Probabilities for Test Sample 34:\n",
            "Class 1 Probability: 0.7233\n",
            "Class 2 Probability: 0.4855\n",
            "\n",
            "Class Probabilities for Test Sample 35:\n",
            "Class 1 Probability: 0.7270\n",
            "Class 2 Probability: 0.4816\n",
            "\n",
            "Class Probabilities for Test Sample 36:\n",
            "Class 1 Probability: 0.7245\n",
            "Class 2 Probability: 0.4826\n",
            "\n",
            "Class Probabilities for Test Sample 37:\n",
            "Class 1 Probability: 0.4920\n",
            "Class 2 Probability: 0.6524\n",
            "\n",
            "Class Probabilities for Test Sample 38:\n",
            "Class 1 Probability: 0.7228\n",
            "Class 2 Probability: 0.4806\n",
            "\n",
            "Class Probabilities for Test Sample 39:\n",
            "Class 1 Probability: 0.4687\n",
            "Class 2 Probability: 0.7032\n",
            "\n",
            "Class Probabilities for Test Sample 40:\n",
            "Class 1 Probability: 0.7125\n",
            "Class 2 Probability: 0.5020\n",
            "\n",
            "Class Probabilities for Test Sample 41:\n",
            "Class 1 Probability: 0.7272\n",
            "Class 2 Probability: 0.4818\n",
            "\n",
            "Class Probabilities for Test Sample 42:\n",
            "Class 1 Probability: 0.4706\n",
            "Class 2 Probability: 0.7026\n",
            "\n",
            "Class Probabilities for Test Sample 43:\n",
            "Class 1 Probability: 0.7206\n",
            "Class 2 Probability: 0.4806\n",
            "\n",
            "Class Probabilities for Test Sample 44:\n",
            "Class 1 Probability: 0.7262\n",
            "Class 2 Probability: 0.4826\n",
            "\n",
            "Class Probabilities for Test Sample 45:\n",
            "Class 1 Probability: 0.7146\n",
            "Class 2 Probability: 0.4902\n",
            "\n",
            "Class Probabilities for Test Sample 46:\n",
            "Class 1 Probability: 0.7133\n",
            "Class 2 Probability: 0.5037\n",
            "\n",
            "Class Probabilities for Test Sample 47:\n",
            "Class 1 Probability: 0.7137\n",
            "Class 2 Probability: 0.4758\n",
            "\n",
            "Class Probabilities for Test Sample 48:\n",
            "Class 1 Probability: 0.7296\n",
            "Class 2 Probability: 0.4854\n",
            "\n",
            "Class Probabilities for Test Sample 49:\n",
            "Class 1 Probability: 0.7121\n",
            "Class 2 Probability: 0.4930\n",
            "\n",
            "Class Probabilities for Test Sample 50:\n",
            "Class 1 Probability: 0.7205\n",
            "Class 2 Probability: 0.4789\n",
            "\n",
            "Class Probabilities for Test Sample 51:\n",
            "Class 1 Probability: 0.4665\n",
            "Class 2 Probability: 0.6995\n",
            "\n",
            "Class Probabilities for Test Sample 52:\n",
            "Class 1 Probability: 0.4769\n",
            "Class 2 Probability: 0.7176\n",
            "\n",
            "Class Probabilities for Test Sample 53:\n",
            "Class 1 Probability: 0.6576\n",
            "Class 2 Probability: 0.5814\n",
            "\n",
            "Class Probabilities for Test Sample 54:\n",
            "Class 1 Probability: 0.7007\n",
            "Class 2 Probability: 0.4822\n",
            "\n",
            "Class Probabilities for Test Sample 55:\n",
            "Class 1 Probability: 0.7171\n",
            "Class 2 Probability: 0.4739\n",
            "\n",
            "Class Probabilities for Test Sample 56:\n",
            "Class 1 Probability: 0.7219\n",
            "Class 2 Probability: 0.4859\n",
            "\n",
            "Class Probabilities for Test Sample 57:\n",
            "Class 1 Probability: 0.7243\n",
            "Class 2 Probability: 0.4793\n",
            "\n",
            "Class Probabilities for Test Sample 58:\n",
            "Class 1 Probability: 0.4794\n",
            "Class 2 Probability: 0.7213\n",
            "\n",
            "Class Probabilities for Test Sample 59:\n",
            "Class 1 Probability: 0.5251\n",
            "Class 2 Probability: 0.6721\n",
            "\n",
            "Class Probabilities for Test Sample 60:\n",
            "Class 1 Probability: 0.7269\n",
            "Class 2 Probability: 0.4816\n",
            "\n",
            "Class Probabilities for Test Sample 61:\n",
            "Class 1 Probability: 0.7222\n",
            "Class 2 Probability: 0.4840\n",
            "\n",
            "Class Probabilities for Test Sample 62:\n",
            "Class 1 Probability: 0.4771\n",
            "Class 2 Probability: 0.7176\n",
            "\n",
            "Class Probabilities for Test Sample 63:\n",
            "Class 1 Probability: 0.4768\n",
            "Class 2 Probability: 0.7175\n",
            "\n",
            "Class Probabilities for Test Sample 64:\n",
            "Class 1 Probability: 0.7162\n",
            "Class 2 Probability: 0.5016\n",
            "\n",
            "Class Probabilities for Test Sample 65:\n",
            "Class 1 Probability: 0.7270\n",
            "Class 2 Probability: 0.4829\n",
            "\n",
            "Class Probabilities for Test Sample 66:\n",
            "Class 1 Probability: 0.7049\n",
            "Class 2 Probability: 0.5143\n",
            "\n",
            "Class Probabilities for Test Sample 67:\n",
            "Class 1 Probability: 0.4769\n",
            "Class 2 Probability: 0.7165\n",
            "\n",
            "Class Probabilities for Test Sample 68:\n",
            "Class 1 Probability: 0.4808\n",
            "Class 2 Probability: 0.7218\n",
            "\n",
            "Class Probabilities for Test Sample 69:\n",
            "Class 1 Probability: 0.7264\n",
            "Class 2 Probability: 0.4829\n",
            "\n",
            "Class Probabilities for Test Sample 70:\n",
            "Class 1 Probability: 0.7090\n",
            "Class 2 Probability: 0.4858\n",
            "\n",
            "Class Probabilities for Test Sample 71:\n",
            "Class 1 Probability: 0.4742\n",
            "Class 2 Probability: 0.7020\n",
            "\n",
            "Class Probabilities for Test Sample 72:\n",
            "Class 1 Probability: 0.4747\n",
            "Class 2 Probability: 0.7070\n",
            "\n",
            "Class Probabilities for Test Sample 73:\n",
            "Class 1 Probability: 0.7241\n",
            "Class 2 Probability: 0.4829\n",
            "\n",
            "Class Probabilities for Test Sample 74:\n",
            "Class 1 Probability: 0.4783\n",
            "Class 2 Probability: 0.7088\n",
            "\n",
            "Class Probabilities for Test Sample 75:\n",
            "Class 1 Probability: 0.7287\n",
            "Class 2 Probability: 0.4827\n",
            "\n",
            "Class Probabilities for Test Sample 76:\n",
            "Class 1 Probability: 0.7143\n",
            "Class 2 Probability: 0.4821\n",
            "\n",
            "Class Probabilities for Test Sample 77:\n",
            "Class 1 Probability: 0.7212\n",
            "Class 2 Probability: 0.4917\n",
            "\n",
            "Class Probabilities for Test Sample 78:\n",
            "Class 1 Probability: 0.6161\n",
            "Class 2 Probability: 0.6274\n",
            "\n",
            "Class Probabilities for Test Sample 79:\n",
            "Class 1 Probability: 0.7292\n",
            "Class 2 Probability: 0.4830\n",
            "\n",
            "Class Probabilities for Test Sample 80:\n",
            "Class 1 Probability: 0.7106\n",
            "Class 2 Probability: 0.4797\n",
            "\n",
            "Class Probabilities for Test Sample 81:\n",
            "Class 1 Probability: 0.4714\n",
            "Class 2 Probability: 0.7009\n",
            "\n",
            "Class Probabilities for Test Sample 82:\n",
            "Class 1 Probability: 0.7282\n",
            "Class 2 Probability: 0.4824\n",
            "\n",
            "Class Probabilities for Test Sample 83:\n",
            "Class 1 Probability: 0.5550\n",
            "Class 2 Probability: 0.6563\n",
            "\n",
            "Class Probabilities for Test Sample 84:\n",
            "Class 1 Probability: 0.4770\n",
            "Class 2 Probability: 0.7178\n",
            "\n",
            "Class Probabilities for Test Sample 85:\n",
            "Class 1 Probability: 0.4634\n",
            "Class 2 Probability: 0.6847\n",
            "\n",
            "Class Probabilities for Test Sample 86:\n",
            "Class 1 Probability: 0.4823\n",
            "Class 2 Probability: 0.7170\n",
            "\n",
            "Class Probabilities for Test Sample 87:\n",
            "Class 1 Probability: 0.4796\n",
            "Class 2 Probability: 0.7149\n",
            "\n",
            "Class Probabilities for Test Sample 88:\n",
            "Class 1 Probability: 0.4786\n",
            "Class 2 Probability: 0.7147\n",
            "\n",
            "Class Probabilities for Test Sample 89:\n",
            "Class 1 Probability: 0.7225\n",
            "Class 2 Probability: 0.4804\n",
            "\n",
            "Class Probabilities for Test Sample 90:\n",
            "Class 1 Probability: 0.7219\n",
            "Class 2 Probability: 0.4822\n",
            "\n",
            "Class Probabilities for Test Sample 91:\n",
            "Class 1 Probability: 0.7188\n",
            "Class 2 Probability: 0.4809\n",
            "\n",
            "Class Probabilities for Test Sample 92:\n",
            "Class 1 Probability: 0.6499\n",
            "Class 2 Probability: 0.5946\n",
            "\n",
            "Class Probabilities for Test Sample 93:\n",
            "Class 1 Probability: 0.7084\n",
            "Class 2 Probability: 0.4893\n",
            "\n",
            "Class Probabilities for Test Sample 94:\n",
            "Class 1 Probability: 0.7286\n",
            "Class 2 Probability: 0.4861\n",
            "\n",
            "Class Probabilities for Test Sample 95:\n",
            "Class 1 Probability: 0.7242\n",
            "Class 2 Probability: 0.4813\n",
            "\n",
            "Class Probabilities for Test Sample 96:\n",
            "Class 1 Probability: 0.7260\n",
            "Class 2 Probability: 0.4809\n",
            "\n",
            "Class Probabilities for Test Sample 97:\n",
            "Class 1 Probability: 0.4732\n",
            "Class 2 Probability: 0.7120\n",
            "\n",
            "Class Probabilities for Test Sample 98:\n",
            "Class 1 Probability: 0.4766\n",
            "Class 2 Probability: 0.7163\n",
            "\n",
            "Class Probabilities for Test Sample 99:\n",
            "Class 1 Probability: 0.7273\n",
            "Class 2 Probability: 0.4816\n",
            "\n",
            "Class Probabilities for Test Sample 100:\n",
            "Class 1 Probability: 0.4688\n",
            "Class 2 Probability: 0.7030\n",
            "\n",
            "Class Probabilities for Test Sample 101:\n",
            "Class 1 Probability: 0.4643\n",
            "Class 2 Probability: 0.6859\n",
            "\n",
            "Class Probabilities for Test Sample 102:\n",
            "Class 1 Probability: 0.7300\n",
            "Class 2 Probability: 0.4833\n",
            "\n",
            "Class Probabilities for Test Sample 103:\n",
            "Class 1 Probability: 0.4767\n",
            "Class 2 Probability: 0.7156\n",
            "\n",
            "Class Probabilities for Test Sample 104:\n",
            "Class 1 Probability: 0.4734\n",
            "Class 2 Probability: 0.7090\n",
            "\n",
            "Class Probabilities for Test Sample 105:\n",
            "Class 1 Probability: 0.7241\n",
            "Class 2 Probability: 0.4863\n",
            "\n",
            "Class Probabilities for Test Sample 106:\n",
            "Class 1 Probability: 0.7205\n",
            "Class 2 Probability: 0.4927\n",
            "\n",
            "Class Probabilities for Test Sample 107:\n",
            "Class 1 Probability: 0.7181\n",
            "Class 2 Probability: 0.4815\n",
            "\n",
            "Class Probabilities for Test Sample 108:\n",
            "Class 1 Probability: 0.4749\n",
            "Class 2 Probability: 0.7143\n",
            "\n",
            "Class Probabilities for Test Sample 109:\n",
            "Class 1 Probability: 0.6963\n",
            "Class 2 Probability: 0.5182\n",
            "\n",
            "Class Probabilities for Test Sample 110:\n",
            "Class 1 Probability: 0.7175\n",
            "Class 2 Probability: 0.4952\n",
            "\n",
            "Class Probabilities for Test Sample 111:\n",
            "Class 1 Probability: 0.4664\n",
            "Class 2 Probability: 0.6920\n",
            "\n",
            "Class Probabilities for Test Sample 112:\n",
            "Class 1 Probability: 0.7237\n",
            "Class 2 Probability: 0.4809\n",
            "\n",
            "Class Probabilities for Test Sample 113:\n",
            "Class 1 Probability: 0.6459\n",
            "Class 2 Probability: 0.5973\n",
            "\n",
            "Class Probabilities for Test Sample 114:\n",
            "Class 1 Probability: 0.4727\n",
            "Class 2 Probability: 0.7116\n"
          ]
        }
      ],
      "source": [
        "# Print class probabilities (combined values) for each test sample\n",
        "for sample_idx in range(X_test.shape[0]):\n",
        "    print(f\"\\nClass Probabilities for Test Sample {sample_idx + 1}:\")\n",
        "    for class_idx in range(output_size):\n",
        "        combined_value = combined_values[sample_idx, class_idx]\n",
        "        print(f\"Class {class_idx + 1} Probability: {combined_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9qMLxEGZIvb",
        "outputId": "fd767ed8-07b8-4280-bc56-49d075fb7c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9912\n",
            "Precision: 0.9913\n",
            "Recall: 0.9912\n",
            "F1 Score: 0.9912\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Convert class probabilities to predicted class labels\n",
        "predicted_labels = np.argmax(combined_values, axis=1)\n",
        "\n",
        "# Get the true labels from the test set\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9912\n",
            "Precision: 0.9913\n",
            "Recall: 0.9912\n",
            "F1 Score: 0.9912\n",
            "RMSE: 0.4039\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Convert class probabilities to predicted class labels\n",
        "predicted_labels = np.argmax(combined_values, axis=1)\n",
        "\n",
        "# Get the true labels from the test set\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Calculate RMSE (using probabilities or one-hot encoded labels)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, combined_values))\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Plot the metrics\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'RMSE']\n",
        "values = [accuracy, precision, recall, f1, rmse]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, values, color=['blue', 'green', 'orange', 'purple', 'red'])\n",
        "plt.title('Classification Metrics and RMSE')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)  # Set y-axis limits for better visualization (except RMSE if it's high)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "True Certainty (TC): The model correctly classifies a sample with high confidence.\n",
        "\n",
        "True Uncertainty (TU): The model misclassifies a sample but does so with low confidence.\n",
        "\n",
        "False Certainty (FC): The model misclassifies a sample with high confidence.\n",
        "\n",
        "False Uncertainty (FU): The model correctly classifies a sample but does so with low confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.20103785 0.47705663 0.51534032 0.26730238 0.15387938 0.18649324\n",
            " 0.24801895 0.55321364 0.29095763 0.13535816 0.10230138 0.67517628\n",
            " 0.12852379 0.34522014 0.14306142 0.21382883 0.13211747 0.\n",
            " 0.03595302 0.25708792 0.31362603 0.15266898 0.19840996 0.04000842\n",
            " 0.09899309 0.34389602 0.16387386 0.23889032 0.11479173 0.27958575\n",
            " 0.10595056 0.0661161  0.18701644 0.11334305 0.0830705  0.11401041\n",
            " 1.         0.1487171  0.48288553 0.16352411 0.07947233 0.48064761\n",
            " 0.17917552 0.08976195 0.20755796 0.14035244 0.30161038 0.0250528\n",
            " 0.2257407  0.19009047 0.54261728 0.2419042  0.23516808 0.44405056\n",
            " 0.26499596 0.13088448 0.13451224 0.17634713 0.53403825 0.08551526\n",
            " 0.13832647 0.24014266 0.24365694 0.11507711 0.07684512 0.18469977\n",
            " 0.25647393 0.16117355 0.08556165 0.31089386 0.46959265 0.39935033\n",
            " 0.11725178 0.35593694 0.05366248 0.25874127 0.10670293 0.2343746\n",
            " 0.04417451 0.3230726  0.49960207 0.06145476 0.49119702 0.23910566\n",
            " 0.75322446 0.22034563 0.26443271 0.27262342 0.15394566 0.15198113\n",
            " 0.20317764 0.19897951 0.29835035 0.03551348 0.12489748 0.10193121\n",
            " 0.33964828 0.26176437 0.07899076 0.48433846 0.73327502 0.03085728\n",
            " 0.27082818 0.378919   0.09722707 0.11032548 0.20931032 0.29799091\n",
            " 0.27190536 0.13652368 0.64287844 0.13452302 0.21840207 0.34740533]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Threshold: 0.8\n",
            "True Certainty (TC): 112, True Uncertainty (TU): 0\n",
            "False Certainty (FC): 1, False Uncertainty (FU): 1\n",
            "Sum of TC + TU: 112\n",
            "Uncertainty Sensitivity (USen): 0.0000\n",
            "Uncertainty Specificity (USpe): 0.9912\n",
            "Uncertainty Precision (UPre): 0.0000\n",
            "Uncertainty Accuracy (UAcc): 0.9825\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to calculate uncertainty using entropy\n",
        "def calculate_uncertainty(probabilities):\n",
        "    return -np.sum(probabilities * np.log(probabilities + 1e-9), axis=1)  # Entropy\n",
        "\n",
        "# Calculate uncertainties for test samples\n",
        "uncertainties = calculate_uncertainty(combined_values)\n",
        "\n",
        "# Normalize uncertainties between 0 and 1\n",
        "def normalize_uncertainty(uncertainties):\n",
        "    min_uncertainty = np.min(uncertainties)\n",
        "    max_uncertainty = np.max(uncertainties)\n",
        "    return (uncertainties - min_uncertainty) / (max_uncertainty - min_uncertainty)\n",
        "\n",
        "normalized_uncertainties = normalize_uncertainty(uncertainties)\n",
        "\n",
        "print(normalized_uncertainties)\n",
        "\n",
        "# Function to find the threshold that maximizes TC + TU\n",
        "def find_threshold_max_tc_tu(combined_values, true_labels, normalized_uncertainties, thresholds=np.arange(0.1, 1.1, 0.1)):\n",
        "    best_threshold = 0\n",
        "    best_tc_tu_sum = -np.inf\n",
        "    best_tc, best_tu = 0, 0\n",
        "    best_fu, best_fc = 0, 0\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        TC, FU, FC, TU = 0, 0, 0, 0\n",
        "\n",
        "        for i in range(len(true_labels)):\n",
        "            pred_class = np.argmax(combined_values[i])\n",
        "            true_class = true_labels[i]\n",
        "            uncertainty = normalized_uncertainties[i]\n",
        "\n",
        "            if pred_class == true_class:\n",
        "                if uncertainty < threshold:\n",
        "                    TC += 1\n",
        "                else:\n",
        "                    FU += 1\n",
        "            else:\n",
        "                if uncertainty < threshold:\n",
        "                    FC += 1\n",
        "                else:\n",
        "                    TU += 1\n",
        "\n",
        "        tc_tu_sum = TC + TU\n",
        "\n",
        "        if tc_tu_sum > best_tc_tu_sum:\n",
        "            best_threshold = threshold\n",
        "            best_tc_tu_sum = tc_tu_sum\n",
        "            best_tc, best_tu = TC, TU\n",
        "            best_fu, best_fc = FU, FC\n",
        "\n",
        "    return best_threshold, best_tc, best_tu, best_fu, best_fc, best_tc_tu_sum\n",
        "\n",
        "# Function to plot TC, TU, FC, and FU vs. threshold\n",
        "def plot_tc_tu(combined_values, true_labels, normalized_uncertainties, thresholds=np.arange(0.1, 1.1, 0.1)):\n",
        "    tc_values = []\n",
        "    tu_values = []\n",
        "    fc_values = []\n",
        "    fu_values = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        TC, FU, FC, TU = 0, 0, 0, 0\n",
        "        for i in range(len(true_labels)):\n",
        "            pred_class = np.argmax(combined_values[i])\n",
        "            true_class = true_labels[i]\n",
        "            uncertainty = normalized_uncertainties[i]\n",
        "\n",
        "            if pred_class == true_class:\n",
        "                if uncertainty < threshold:\n",
        "                    TC += 1\n",
        "                else:\n",
        "                    FU += 1\n",
        "            else:\n",
        "                if uncertainty < threshold:\n",
        "                    FC += 1\n",
        "                else:\n",
        "                    TU += 1\n",
        "\n",
        "        tc_values.append(TC)\n",
        "        tu_values.append(TU)\n",
        "        fc_values.append(FC)\n",
        "        fu_values.append(FU)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(thresholds, tc_values, label=\"True Certainty (TC)\", marker='o')\n",
        "    plt.plot(thresholds, tu_values, label=\"True Uncertainty (TU)\", marker='o')\n",
        "    plt.plot(thresholds, fc_values, label=\"False Certainty (FC)\", marker='o')\n",
        "    plt.plot(thresholds, fu_values, label=\"False Uncertainty (FU)\", marker='o')\n",
        "    plt.xlabel('Uncertainty Threshold')\n",
        "    plt.ylabel('Counts')\n",
        "    plt.title('True/False Certainty and Uncertainty vs. Threshold')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot the confusion matrix\n",
        "def plot_confusion_matrix(best_tc, best_tu, best_fu, best_fc):\n",
        "    confusion_matrix_values = np.array([[best_tc, best_fu], [best_fc, best_tu]])\n",
        "    labels = np.array([[f\"True Certainty\\n{best_tc}\", f\"False Uncertainty\\n{best_fu}\"],\n",
        "                       [f\"False Certainty\\n{best_fc}\", f\"True Uncertainty\\n{best_tu}\"]])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(confusion_matrix_values, annot=labels, fmt='', cmap='Blues', cbar=False, annot_kws={\"size\": 16})\n",
        "    plt.title('Uncertainty Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot uncertainty metrics vs threshold\n",
        "def plot_uncertainty_metrics_vs_threshold(thresholds, usen_values, uspe_values, upre_values, uacc_values):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(thresholds, usen_values, label=\"Uncertainty Sensitivity (USen)\", marker='o')\n",
        "    plt.plot(thresholds, uspe_values, label=\"Uncertainty Specificity (USpe)\", marker='o')\n",
        "    plt.plot(thresholds, upre_values, label=\"Uncertainty Precision (UPre)\", marker='o')\n",
        "    plt.plot(thresholds, uacc_values, label=\"Uncertainty Accuracy (UAcc)\", marker='o')\n",
        "    plt.xlabel('Uncertainty Threshold')\n",
        "    plt.ylabel('Metrics')\n",
        "    plt.title('Uncertainty Metrics vs. Threshold')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot uncertainty metrics bar graph at best threshold\n",
        "def plot_uncertainty_metrics_bar_graph(USen, USpe, UPre, UAcc):\n",
        "    metrics = ['USen', 'USpe', 'UPre', 'UAcc']\n",
        "    values = [USen, USpe, UPre, UAcc]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple'])\n",
        "    plt.title(f\"Uncertainty Metrics at Best Threshold: {best_threshold}\")\n",
        "    plt.ylabel('Value')\n",
        "    plt.show()\n",
        "   \n",
        "\n",
        "# Function to plot bar graph of uncertainty metrics\n",
        "def plot_uncertainty_metrics_bar(thresholds, USen_values, USpe_values, UPre_values, UAcc_values):\n",
        "    x = np.arange(len(thresholds))  # label locations\n",
        "    width = 0.2  # width of the bars\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    plt.bar(x - 1.5*width, USen_values, width, label=\"USen\")\n",
        "    plt.bar(x - 0.5*width, USpe_values, width, label=\"USpe\")\n",
        "    plt.bar(x + 0.5*width, UPre_values, width, label=\"UPre\")\n",
        "    plt.bar(x + 1.5*width, UAcc_values, width, label=\"UAcc\")\n",
        "\n",
        "    plt.xlabel('Thresholds')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.title('Uncertainty Metrics vs. Threshold (Bar Plot)')\n",
        "    plt.xticks(x, thresholds)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming these are defined:\n",
        "# combined_values = np.array([[...], [...], ...])  # Model's predicted probabilities\n",
        "# true_labels = np.array([...])  # True class labels\n",
        "\n",
        "# Calculate uncertainties for test samples\n",
        "#uncertainties = calculate_uncertainty(combined_values)\n",
        "\n",
        "# Normalize the uncertainties\n",
        "#normalized_uncertainties = normalize_uncertainty(uncertainties)\n",
        "\n",
        "# Find the best threshold that maximizes TC + TU\n",
        "best_threshold, best_tc, best_tu, best_fu, best_fc, best_tc_tu_sum = find_threshold_max_tc_tu(combined_values, true_labels, normalized_uncertainties)\n",
        "\n",
        "# Calculate metrics at the best threshold\n",
        "USen = best_tu / (best_tu + best_fc) if (best_tu + best_fc) != 0 else 0\n",
        "USpe = best_tc / (best_tc + best_fu) if (best_tc + best_fu) != 0 else 0\n",
        "UPre = best_tu / (best_tu + best_fu) if (best_tu + best_fu) != 0 else 0\n",
        "UAcc = (best_tu + best_tc) / (best_tu + best_tc + best_fu + best_fc) if (best_tu + best_tc + best_fu + best_fc) != 0 else 0\n",
        "# Plot the bar graph for the metrics at the best threshold\n",
        "plot_uncertainty_metrics_bar_graph(USen, USpe, UPre, UAcc)\n",
        "# Output the results\n",
        "print(f\"Best Threshold: {best_threshold}\")\n",
        "print(f\"True Certainty (TC): {best_tc}, True Uncertainty (TU): {best_tu}\")\n",
        "print(f\"False Certainty (FC): {best_fc}, False Uncertainty (FU): {best_fu}\")\n",
        "print(f\"Sum of TC + TU: {best_tc_tu_sum}\")\n",
        "print(f\"Uncertainty Sensitivity (USen): {USen:.4f}\")\n",
        "print(f\"Uncertainty Specificity (USpe): {USpe:.4f}\")\n",
        "print(f\"Uncertainty Precision (UPre): {UPre:.4f}\")\n",
        "print(f\"Uncertainty Accuracy (UAcc): {UAcc:.4f}\")\n",
        "\n",
        "# Plot the confusion matrix at the best threshold\n",
        "plot_confusion_matrix(best_tc, best_tu, best_fu, best_fc)\n",
        "\n",
        "# Plot True/False Certainty and Uncertainty vs. Threshold\n",
        "plot_tc_tu(combined_values, true_labels, normalized_uncertainties)\n",
        "\n",
        "# Plot uncertainty metrics vs threshold\n",
        "thresholds = np.arange(0.1, 1.1, 0.1)\n",
        "USen_values = []\n",
        "USpe_values = []\n",
        "UPre_values = []\n",
        "UAcc_values = []\n",
        "\n",
        "\n",
        "for threshold in thresholds:\n",
        "    _, tc, tu, fu, fc, _ = find_threshold_max_tc_tu(combined_values, true_labels, normalized_uncertainties, [threshold])\n",
        "    \n",
        "    USen = tu / (tu + fc) if (tu + fc) != 0 else 0\n",
        "    USpe = tc / (tc + fu) if (tc + fu) != 0 else 0\n",
        "    UPre = tu / (tu + fu) if (tu + fu) != 0 else 0\n",
        "    UAcc = (tu + tc) / (tu + tc + fu + fc) if (tu + tc + fu + fc) != 0 else 0\n",
        "    \n",
        "    USen_values.append(USen)\n",
        "    USpe_values.append(USpe)\n",
        "    UPre_values.append(UPre)\n",
        "    UAcc_values.append(UAcc)\n",
        "\n",
        "plot_uncertainty_metrics_vs_threshold(thresholds, USen_values, USpe_values, UPre_values, UAcc_values)\n",
        "\n",
        "# Plot the bar graph for the metrics at the best threshold\n",
        "#plot_uncertainty_metrics_bar_graph(USen, USpe, UPre, UAcc)\n",
        "\n",
        "# Plot bar graph of uncertainty metrics vs. threshold\n",
        "plot_uncertainty_metrics_bar(thresholds,USen_values,USpe_values,UPre_values,UAcc_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected Calibration Error (ECE): 0.2797250993131981\n",
            "Overconfidence Error (OE): 0.0\n",
            "Underconfidence Error (UE): 0.2797250993131981\n",
            "Negative Log Likelihood (NLL): 0.3435706430880059\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "predicted_confidences = np.max(combined_values, axis=1)\n",
        "predicted_labels= np.argmax(combined_values, axis=1)\n",
        "\n",
        "def calculate_ece(true_labels ,predicted_labels,predicted_confidences, num_bins=15):\n",
        "    bin_boundaries = np.linspace(0.0, 1.0, num_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0.0\n",
        "    n = len(true_labels)\n",
        "\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        bin_indices = np.where((predicted_confidences > bin_lower) & (predicted_confidences <= bin_upper))[0]\n",
        "        if len(bin_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        bin_accuracy = np.mean(true_labels[bin_indices] == predicted_labels[bin_indices])\n",
        "        bin_confidence = np.mean(predicted_confidences[bin_indices])\n",
        "        bin_proportion = len(bin_indices) / n\n",
        "\n",
        "        # ECE calculation\n",
        "        ece += bin_proportion * np.abs(bin_accuracy - bin_confidence)\n",
        "\n",
        "    return ece\n",
        "\n",
        "def calculate_oe(true_labels, predicted_labels, predicted_confidences, num_bins=15):\n",
        "    bin_boundaries = np.linspace(0.0, 1.0, num_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    oe = 0.0\n",
        "    n = len(true_labels)\n",
        "\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        bin_indices = np.where((predicted_confidences > bin_lower) & (predicted_confidences <= bin_upper))[0]\n",
        "        if len(bin_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        bin_accuracy = np.mean(true_labels[bin_indices] == predicted_labels[bin_indices])\n",
        "        bin_confidence = np.mean(predicted_confidences[bin_indices])\n",
        "        bin_proportion = len(bin_indices) / n\n",
        "\n",
        "        # OE calculation (only consider cases where confidence > accuracy)\n",
        "        oe += bin_proportion * max(bin_confidence - bin_accuracy, 0)\n",
        "\n",
        "    return oe\n",
        "\n",
        "def calculate_ue(true_labels, predicted_labels, predicted_confidences, num_bins=15):\n",
        "    bin_boundaries = np.linspace(0.0, 1.0, num_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ue = 0.0\n",
        "    n = len(true_labels)\n",
        "\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        bin_indices = np.where((predicted_confidences > bin_lower) & (predicted_confidences <= bin_upper))[0]\n",
        "        if len(bin_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        bin_accuracy = np.mean(true_labels[bin_indices] == predicted_labels[bin_indices])\n",
        "        bin_confidence = np.mean(predicted_confidences[bin_indices])\n",
        "        bin_proportion = len(bin_indices) / n\n",
        "\n",
        "        # UE calculation (only consider cases where accuracy > confidence)\n",
        "        ue += bin_proportion * max(bin_accuracy - bin_confidence, 0)\n",
        "\n",
        "    return ue\n",
        "\n",
        "def negative_log_likelihood(true_labels, predicted_probabilities):\n",
        "\n",
        "    # Ensure predicted probabilities are in a safe range to avoid log(0)\n",
        "    epsilon = 1e-12\n",
        "    predicted_probabilities = np.clip(predicted_probabilities, epsilon, 1. - epsilon)\n",
        "\n",
        "    # Number of samples\n",
        "    n_samples = len(true_labels)\n",
        "\n",
        "    # Gather the predicted probabilities for the true classes\n",
        "    true_class_probabilities = predicted_probabilities[np.arange(n_samples), true_labels]\n",
        "\n",
        "    # Compute the negative log likelihood\n",
        "    nll = -np.mean(np.log(true_class_probabilities))\n",
        "\n",
        "    return nll\n",
        "\n",
        "# Example data\n",
        "#true_labels = np.array([0, 1, 0, 1, 1, 0, 1, 0])\n",
        "###predicted_labels = np.array([0, 1, 0, 0, 1, 1, 1, 0])\n",
        "##predicted_confidences = np.array([0.9, 0.8, 0.7, 0.6, 0.95, 0.55, 0.85, 0.5])\n",
        "#predicted_probabilities = np.array([[0.9, 0.1], [0.2, 0.8], [0.7, 0.3], [0.4, 0.6], [0.05, 0.95], [0.45, 0.55], [0.15, 0.85], [0.5, 0.5]])\n",
        "\n",
        "# Calculate metrics\n",
        "ece = calculate_ece(true_labels, predicted_labels, predicted_confidences)\n",
        "oe = calculate_oe(true_labels, predicted_labels, predicted_confidences)\n",
        "ue = calculate_ue(true_labels, predicted_labels, predicted_confidences)\n",
        "nll = negative_log_likelihood(true_labels, combined_values)\n",
        "\n",
        "# Print results\n",
        "print(f\"Expected Calibration Error (ECE): {ece}\")\n",
        "print(f\"Overconfidence Error (OE): {oe}\")\n",
        "print(f\"Underconfidence Error (UE): {ue}\")\n",
        "print(f\"Negative Log Likelihood (NLL): {nll}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
